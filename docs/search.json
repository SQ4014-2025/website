[
  {
    "objectID": "notes/notes_9.html",
    "href": "notes/notes_9.html",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "Objectives\nKey terms (bold on first use)\nUnderstanding prompt engineering\nAnatomy of a prompt\nCrafting the ultimate prompt\nFundamental prompt types\nAdvanced prompt types\nPrompt techniques for programmers\nWorked examples (week-aligned)\nPitfalls and anti-patterns\nSumming Up\n\n\nPrompt engineering is interface design for AI tools. You are not ‚Äúasking a question‚Äù ‚Äî you are specifying an input contract (context + constraints + success criteria) so the output is testable.\n\n\n\nThis week builds a practical toolkit for getting reliable work out of coding assistants.\n\nUnderstanding prompt engineering: why prompts fail, and what ‚Äúbetter prompts‚Äù look like\nAnatomy of a prompt: context, instructions, examples, and expected format\nCrafting the ultimate prompt: using the model to improve your own prompts\nFundamental prompt types: zero-shot, few-shot, open-ended, constrained, structured\nAdvanced prompt types: chain-of-thought, recursive prompting, context manipulation, refinement, output control\nPrompt techniques for programmers: how this becomes a daily workflow habit\n\n\n\n\n\nprompt: the input you send the model; in practice it‚Äôs a specification of what you want.\ncontext: background info the model needs to interpret the task (code, data shape, constraints).\nconstraints: boundaries you set (language, complexity, libraries, performance, security rules).\nfew-shot: showing a couple of examples so the model copies the pattern.\nstructured prompt: a prompt that forces the output into a predictable structure (JSON, table, sections).\noutput control: techniques that reduce randomness by restricting response shape.\n\n\n\n\nPrompt engineering is not ‚Äútricking‚Äù the model. It‚Äôs about reducing ambiguity.\n\n\nCoding tasks are brittle:\n\nsmall mistakes cause crashes\nmissing constraints produce insecure or inefficient code\na ‚Äúmostly correct‚Äù solution can still be useless\n\nWhen prompts fail, it‚Äôs usually one of these reasons:\n\nUnclear task (what action do you want?)\nMissing context (what codebase? what inputs?)\nNo success criteria (how do we judge ‚Äúgood‚Äù?)\nNo output format (what should the response look like?)\nNo constraints (libraries? style? performance?)\n\n\n\n\n\nA classic failure is:\n\nPrompt: ‚ÄúFix this code‚Äù\nOutput: generic advice, or wrong assumptions\n\nInstead, you want a prompt that provides:\n\nthe exact error\nthe code\nthe expected behaviour\nwhat you already tried\nhow you want the answer returned\n\n\n\n\n\nA good prompt is a communication bundle.\nThink of it as four parts:\n\nContext ‚Äî what the model needs to know\nInstructions ‚Äî what to do\nExamples ‚Äî patterns to follow\nFormat contract ‚Äî the exact structure of the response\n\n\n\n\nContext can be:\n\nthe code you‚Äôre working on\nthe data format / schema\nenvironment details (OS, Python version)\nconstraints: libraries allowed, complexity limits\n\nRule of thumb: if a human dev would ask for it before helping you, the model needs it too.\n\n\n\nGood instruction starts with a clear verb:\n\n‚Äúrefactor‚Äù\n‚Äúdebug‚Äù\n‚Äúgenerate tests‚Äù\n‚Äúexplain‚Äù\n‚Äúconvert‚Äù\n\nBad instruction is vague:\n\n‚Äúhelp me‚Äù\n‚Äúimprove this‚Äù\n\n\n\n\nFew-shot prompting is basically pattern copying.\nIf you want a specific style:\n\nshow 1‚Äì2 examples\nkeep them short\nkeep formatting consistent\n\n\n\n\nIf you don‚Äôt specify format, the model will:\n\nramble\nmix explanation and code\ninvent file structures\n\nSo you specify:\n\n‚Äúreturn JSON only‚Äù\n‚Äúreturn only Python code‚Äù\n‚Äúreturn a markdown checklist‚Äù\n\n\n\n\n\n‚úÖ Use an LLM to improve your prompt before using it for the real task.\nInstead of:\n\n‚ÄúWrite an API request tutorial‚Äù\n\nAsk the model:\n\n‚ÄúImprove this prompt so it produces the best possible tutorial.‚Äù\n\n\n\nUse this when you want a high-quality response.\nraw_prompt = \"Give me instructions on how to send an HTTP request to an API and handle errors.\"\n\nrefiner = f\"\"\"\nYou are an expert prompt engineer.\n\nImprove the prompt below by:\n- adding missing context questions\n- adding constraints\n- specifying output format\n\nPROMPT TO IMPROVE:\n{raw_prompt}\n\nReturn the improved prompt only.\n\"\"\"\n\nprint(refiner)\nWhat you‚Äôre doing:\n\nturning fuzzy intent into a structured request\nforcing the model to ask missing questions\n\n\n\n\n\nThis week introduces several ‚Äúprompt families‚Äù.\n\n\nZero-shot = no examples.\nExample:\n\n‚ÄúWrite a Python function to validate emails.‚Äù\n\nBest when:\n\nthe task is common\nthe output shape is simple\n\nRisk:\n\nthe model guesses assumptions\n\nzero_shot = \"Write a Python function that validates an email address.\"\nprint(zero_shot)\n\n\n\nFew-shot = provide examples.\nBest when:\n\nyou care about formatting\nyou want a consistent style\n\nfew_shot = \"\"\"\nYou are a Python developer.\n\nExample 1:\ndef double(x):\n    return x * 2\n\nExample 2:\ndef square(x):\n    return x ** 2\n\nNow write a function:\ndef cube(x):\n\"\"\"\nprint(few_shot)\n\n\n\nOpen-ended = exploration.\nBest for:\n\ncomparing tools\nbrainstorming designs\ngenerating options\n\nBad for:\n\nprecise code requirements\n\n\n\n\nConstrained prompts are strict.\nExamples:\n\n‚ÄúList exactly three built-in Python data structures.‚Äù\n‚ÄúReturn only code.‚Äù\n\nconstrained = \"List exactly three built-in Python data structures.\"\nprint(constrained)\n\n\n\nIterative prompting = conversation loop.\nYou:\n\nask\nevaluate\nrefine\n\nThis behaves like debugging.\nprompt_v1 = \"Explain why this test is failing.\"\nprompt_v2 = \"Explain why this pytest test is failing. Use bullet points and include the fixed code.\"\nprint(prompt_v1)\nprint(prompt_v2)\n\n\n\nStructured prompts enforce a schema.\nExample:\n\n‚ÄúReturn JSON with keys: files, changes, reasons‚Äù\n\nThis is huge for engineering teams.\nstructured = \"\"\"\nReturn JSON only.\n\nSchema:\n{\"files\": [{\"path\": \"...\", \"change\": \"...\", \"reason\": \"...\"}]}\n\nTask: Propose refactor changes for a small Python script.\n\"\"\"\nprint(structured)\n\n\n\n\nThese techniques are about increasing reliability.\n\n\nIn this week, chain-of-thought means:\n\nencourage step-by-step reasoning\nreduce ‚Äújumping to conclusions‚Äù\n\nUse carefully: the model may sound confident even when wrong.\n\n\n\nRecursive prompting = prompt ‚Üí output ‚Üí prompt again.\nExample workflow:\n\ngenerate draft\ncritique draft\nrevise draft\n\nThis is similar to:\n\ncode review loops\niterative refactoring\n\n\n\n\nContext manipulation = changing what the model sees.\nExamples:\n\ninclude only the relevant file (not the whole repo)\nprovide a minimal failing example\nredact secrets and irrelevant text\n\n\n\n\nInstruction refinement = tuning wording.\nSmall changes matter:\n\n‚Äúexplain‚Äù vs ‚Äúteach‚Äù\n‚Äúbrief‚Äù vs ‚Äúdetailed‚Äù\n‚Äúcode only‚Äù vs ‚Äúcode + rationale‚Äù\n\n\n\n\nOutput control = forcing response format.\nThis is key when you need:\n\ndocumentation\ncommit messages\nstructured change requests\n\n\n\n\n\n\n\nThink:\n\nprompt = function signature\ncontext = arguments\nconstraints = preconditions\noutput format = return type\n\n\n\n\nprompt = \"\"\"\nYou are a senior Python developer.\n\nTask: {task}\nContext: {context}\nConstraints: {constraints}\nOutput format: {format}\n\nReturn only the output format requested.\n\"\"\"\n\nfilled = prompt.format(\n    task=\"Write pytest tests for apply_discount(price, discount)\",\n    context=\"discount must be between 0 and 1; negative should raise ValueError\",\n    constraints=\"Use pytest; include edge cases; no extra commentary\",\n    format=\"Python code\"\n)\n\nprint(filled)\nWhy this works:\n\nclear role\nexplicit requirements\npredictable output\n\n\n\n\n\n\n\nBad prompt:\n\n‚ÄúFix this code‚Äù\n\nBetter prompt:\n\ninclude error\ninclude expected behaviour\nrequest specific output\n\n\n\n\nIf you want documentation:\n\nspecify audience\nspecify sections\nspecify formatting\n\n\n\n\nStrong prompts include:\n\nwhat to test\nedge cases\nexpected failures\n\n\n\n\n\n\n\nIt doesn‚Äôt.\nIf it can interpret your prompt two ways, it will choose one randomly.\n\n\n\nAI output must be validated:\n\nrun the code\nwrite tests\nreview logic\n\n\n\n\nToo much irrelevant context reduces quality.\nUse:\n\nminimal failing examples\nfocused snippets\n\n\n\n\n\n\nPrompts are specs, not questions\nContext prevents guessing\nConstraints reduce nonsense\nExamples control style\nOutput control makes responses testable\nIteration is normal ‚Äî treat it like debugging",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#objectives",
    "href": "notes/notes_9.html#objectives",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "This week builds a practical toolkit for getting reliable work out of coding assistants.\n\nUnderstanding prompt engineering: why prompts fail, and what ‚Äúbetter prompts‚Äù look like\nAnatomy of a prompt: context, instructions, examples, and expected format\nCrafting the ultimate prompt: using the model to improve your own prompts\nFundamental prompt types: zero-shot, few-shot, open-ended, constrained, structured\nAdvanced prompt types: chain-of-thought, recursive prompting, context manipulation, refinement, output control\nPrompt techniques for programmers: how this becomes a daily workflow habit",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#key-terms-bold-on-first-use",
    "href": "notes/notes_9.html#key-terms-bold-on-first-use",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "prompt: the input you send the model; in practice it‚Äôs a specification of what you want.\ncontext: background info the model needs to interpret the task (code, data shape, constraints).\nconstraints: boundaries you set (language, complexity, libraries, performance, security rules).\nfew-shot: showing a couple of examples so the model copies the pattern.\nstructured prompt: a prompt that forces the output into a predictable structure (JSON, table, sections).\noutput control: techniques that reduce randomness by restricting response shape.",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#understanding-prompt-engineering",
    "href": "notes/notes_9.html#understanding-prompt-engineering",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "Prompt engineering is not ‚Äútricking‚Äù the model. It‚Äôs about reducing ambiguity.\n\n\nCoding tasks are brittle:\n\nsmall mistakes cause crashes\nmissing constraints produce insecure or inefficient code\na ‚Äúmostly correct‚Äù solution can still be useless\n\nWhen prompts fail, it‚Äôs usually one of these reasons:\n\nUnclear task (what action do you want?)\nMissing context (what codebase? what inputs?)\nNo success criteria (how do we judge ‚Äúgood‚Äù?)\nNo output format (what should the response look like?)\nNo constraints (libraries? style? performance?)\n\n\n\n\n\nA classic failure is:\n\nPrompt: ‚ÄúFix this code‚Äù\nOutput: generic advice, or wrong assumptions\n\nInstead, you want a prompt that provides:\n\nthe exact error\nthe code\nthe expected behaviour\nwhat you already tried\nhow you want the answer returned",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#anatomy-of-a-prompt",
    "href": "notes/notes_9.html#anatomy-of-a-prompt",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "A good prompt is a communication bundle.\nThink of it as four parts:\n\nContext ‚Äî what the model needs to know\nInstructions ‚Äî what to do\nExamples ‚Äî patterns to follow\nFormat contract ‚Äî the exact structure of the response\n\n\n\n\nContext can be:\n\nthe code you‚Äôre working on\nthe data format / schema\nenvironment details (OS, Python version)\nconstraints: libraries allowed, complexity limits\n\nRule of thumb: if a human dev would ask for it before helping you, the model needs it too.\n\n\n\nGood instruction starts with a clear verb:\n\n‚Äúrefactor‚Äù\n‚Äúdebug‚Äù\n‚Äúgenerate tests‚Äù\n‚Äúexplain‚Äù\n‚Äúconvert‚Äù\n\nBad instruction is vague:\n\n‚Äúhelp me‚Äù\n‚Äúimprove this‚Äù\n\n\n\n\nFew-shot prompting is basically pattern copying.\nIf you want a specific style:\n\nshow 1‚Äì2 examples\nkeep them short\nkeep formatting consistent\n\n\n\n\nIf you don‚Äôt specify format, the model will:\n\nramble\nmix explanation and code\ninvent file structures\n\nSo you specify:\n\n‚Äúreturn JSON only‚Äù\n‚Äúreturn only Python code‚Äù\n‚Äúreturn a markdown checklist‚Äù",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#crafting-the-ultimate-prompt",
    "href": "notes/notes_9.html#crafting-the-ultimate-prompt",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "‚úÖ Use an LLM to improve your prompt before using it for the real task.\nInstead of:\n\n‚ÄúWrite an API request tutorial‚Äù\n\nAsk the model:\n\n‚ÄúImprove this prompt so it produces the best possible tutorial.‚Äù\n\n\n\nUse this when you want a high-quality response.\nraw_prompt = \"Give me instructions on how to send an HTTP request to an API and handle errors.\"\n\nrefiner = f\"\"\"\nYou are an expert prompt engineer.\n\nImprove the prompt below by:\n- adding missing context questions\n- adding constraints\n- specifying output format\n\nPROMPT TO IMPROVE:\n{raw_prompt}\n\nReturn the improved prompt only.\n\"\"\"\n\nprint(refiner)\nWhat you‚Äôre doing:\n\nturning fuzzy intent into a structured request\nforcing the model to ask missing questions",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#fundamental-prompt-types",
    "href": "notes/notes_9.html#fundamental-prompt-types",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "This week introduces several ‚Äúprompt families‚Äù.\n\n\nZero-shot = no examples.\nExample:\n\n‚ÄúWrite a Python function to validate emails.‚Äù\n\nBest when:\n\nthe task is common\nthe output shape is simple\n\nRisk:\n\nthe model guesses assumptions\n\nzero_shot = \"Write a Python function that validates an email address.\"\nprint(zero_shot)\n\n\n\nFew-shot = provide examples.\nBest when:\n\nyou care about formatting\nyou want a consistent style\n\nfew_shot = \"\"\"\nYou are a Python developer.\n\nExample 1:\ndef double(x):\n    return x * 2\n\nExample 2:\ndef square(x):\n    return x ** 2\n\nNow write a function:\ndef cube(x):\n\"\"\"\nprint(few_shot)\n\n\n\nOpen-ended = exploration.\nBest for:\n\ncomparing tools\nbrainstorming designs\ngenerating options\n\nBad for:\n\nprecise code requirements\n\n\n\n\nConstrained prompts are strict.\nExamples:\n\n‚ÄúList exactly three built-in Python data structures.‚Äù\n‚ÄúReturn only code.‚Äù\n\nconstrained = \"List exactly three built-in Python data structures.\"\nprint(constrained)\n\n\n\nIterative prompting = conversation loop.\nYou:\n\nask\nevaluate\nrefine\n\nThis behaves like debugging.\nprompt_v1 = \"Explain why this test is failing.\"\nprompt_v2 = \"Explain why this pytest test is failing. Use bullet points and include the fixed code.\"\nprint(prompt_v1)\nprint(prompt_v2)\n\n\n\nStructured prompts enforce a schema.\nExample:\n\n‚ÄúReturn JSON with keys: files, changes, reasons‚Äù\n\nThis is huge for engineering teams.\nstructured = \"\"\"\nReturn JSON only.\n\nSchema:\n{\"files\": [{\"path\": \"...\", \"change\": \"...\", \"reason\": \"...\"}]}\n\nTask: Propose refactor changes for a small Python script.\n\"\"\"\nprint(structured)",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#advanced-prompt-types",
    "href": "notes/notes_9.html#advanced-prompt-types",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "These techniques are about increasing reliability.\n\n\nIn this week, chain-of-thought means:\n\nencourage step-by-step reasoning\nreduce ‚Äújumping to conclusions‚Äù\n\nUse carefully: the model may sound confident even when wrong.\n\n\n\nRecursive prompting = prompt ‚Üí output ‚Üí prompt again.\nExample workflow:\n\ngenerate draft\ncritique draft\nrevise draft\n\nThis is similar to:\n\ncode review loops\niterative refactoring\n\n\n\n\nContext manipulation = changing what the model sees.\nExamples:\n\ninclude only the relevant file (not the whole repo)\nprovide a minimal failing example\nredact secrets and irrelevant text\n\n\n\n\nInstruction refinement = tuning wording.\nSmall changes matter:\n\n‚Äúexplain‚Äù vs ‚Äúteach‚Äù\n‚Äúbrief‚Äù vs ‚Äúdetailed‚Äù\n‚Äúcode only‚Äù vs ‚Äúcode + rationale‚Äù\n\n\n\n\nOutput control = forcing response format.\nThis is key when you need:\n\ndocumentation\ncommit messages\nstructured change requests",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#prompt-techniques-for-programmers",
    "href": "notes/notes_9.html#prompt-techniques-for-programmers",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "Think:\n\nprompt = function signature\ncontext = arguments\nconstraints = preconditions\noutput format = return type\n\n\n\n\nprompt = \"\"\"\nYou are a senior Python developer.\n\nTask: {task}\nContext: {context}\nConstraints: {constraints}\nOutput format: {format}\n\nReturn only the output format requested.\n\"\"\"\n\nfilled = prompt.format(\n    task=\"Write pytest tests for apply_discount(price, discount)\",\n    context=\"discount must be between 0 and 1; negative should raise ValueError\",\n    constraints=\"Use pytest; include edge cases; no extra commentary\",\n    format=\"Python code\"\n)\n\nprint(filled)\nWhy this works:\n\nclear role\nexplicit requirements\npredictable output",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#worked-examples-week-aligned",
    "href": "notes/notes_9.html#worked-examples-week-aligned",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "Bad prompt:\n\n‚ÄúFix this code‚Äù\n\nBetter prompt:\n\ninclude error\ninclude expected behaviour\nrequest specific output\n\n\n\n\nIf you want documentation:\n\nspecify audience\nspecify sections\nspecify formatting\n\n\n\n\nStrong prompts include:\n\nwhat to test\nedge cases\nexpected failures",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#pitfalls-and-anti-patterns",
    "href": "notes/notes_9.html#pitfalls-and-anti-patterns",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "It doesn‚Äôt.\nIf it can interpret your prompt two ways, it will choose one randomly.\n\n\n\nAI output must be validated:\n\nrun the code\nwrite tests\nreview logic\n\n\n\n\nToo much irrelevant context reduces quality.\nUse:\n\nminimal failing examples\nfocused snippets",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_9.html#summing-up",
    "href": "notes/notes_9.html#summing-up",
    "title": "Week 09 ‚Äî Prompt engineering",
    "section": "",
    "text": "Prompts are specs, not questions\nContext prevents guessing\nConstraints reduce nonsense\nExamples control style\nOutput control makes responses testable\nIteration is normal ‚Äî treat it like debugging",
    "crumbs": [
      "Notes",
      "Notes 09 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html",
    "href": "notes/notes_7.html",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "What this week is doing (big picture)\nKey terms (bolded the first time)\nFigures extracted from the PDF\nGetting our strategy from our AI tools\nWorked example: prompting for a UI plan (Quarto runnable code)\nFrom flows to pages: what screens does the user need?\nTranslating requirements into template structure\nWireframes: how to ask the AI for layout help\nResponsive design basics (what this week wants you to learn)\nJinja2 integration: UI that actually connects to backend data\nBuilding components: avoiding copy/paste templates\nTesting UI logic (lightweight sanity checks)\nCode example: simple Jinja2 loop for quiz list\nCode example: minimal responsive CSS skeleton\nSumming Up\nQuick checklist (week skills)\n\n\n‚ÄúInteresting people like Data Scientists and software developers can find UI design tough, and generative AI tools help tremendously.‚Äù\n\n\n\nThis week switches focus from backend logic to the part users actually touch: the user interface (UI). The aim is to use chat-based generative AI (especially ChatGPT) as a pair designer + pair developer to:\n\npropose a UI strategy\nwrite structured prompts that produce targeted UI guidance and code\nconvert wireframes and flows into Flask templates + components\nadd responsive HTML/CSS elements without being a frontend expert\n\nThis week is very workflow heavy: it is less ‚Äúwhat is a transformer?‚Äù and more ‚Äúhow do we get from backend routes to a usable application screen?‚Äù\n\n\n\nWe will use these week-specific ideas repeatedly:\n\nWireframe ‚Äî a sketch (boxes + labels) of what the UI should contain and roughly how it should be laid out.\nFlask templates ‚Äî HTML files rendered by Flask (often using Jinja2), letting you combine content + layout + Python variables.\nJinja2 ‚Äî the templating language Flask uses to inject variables/loops/conditionals into HTML.\nResponsive design ‚Äî designing pages so they work on different screen sizes (mobile, tablet, desktop).\nPrompt scaffolding ‚Äî structuring a prompt into sections (objective, constraints, inputs, outputs) so the AI returns usable results.\n\n\n\n\nA key idea early in this week: do strategy first, then code second. The point is that you can waste a lot of time prompting for code before you have agreed what the UI is supposed to do and feel like.\n\n\n\nIf you ask for ‚Äúa frontend‚Äù without specifics, the AI returns generic boilerplate.\nStrategy prompts help you decide: pages, navigation, user flow, components, and naming.\nYou get a UI that matches your backend data model instead of fighting it later.\n\n\n\n\nThis week demonstrates writing a prompt with explicit blocks:\n\nRole (who the AI should act as)\nObjective (what the UI needs to achieve)\nContext (what already exists: backend routes, DB, etc.)\nConstraints (Flask templates, mobile-friendly, accessible)\nDeliverables (pages list, wireframe descriptions, template structure, CSS suggestions)\n\n\n‚ÄúIf we say‚Äùgenerate a frontend for my application‚Äù without enough information, the guidance will be too vague.‚Äù\n\n\n\n\n\nBelow is a ‚Äúprompt builder‚Äù approach: you store prompt sections as strings and assemble them. This makes prompts easier to iterate and reuse.\n#| label: ui-prompt-scaffold\n#| eval: false\nrole = \"Act as a professional Python developer specialising in Flask and UX.\"\nobjective = \"Design a front end for a database-driven Flask quiz application.\"\ncontext = \"Backend exists with routes for login, quizzes, and results.\"\nconstraints = \"Use Flask templates (Jinja2). Keep it responsive and accessible.\"\ndeliverables = \"Return: (1) page list, (2) nav structure, (3) template folder structure, (4) CSS starter.\"\n\nprompt = \"\n\n\".join([role, objective, context, constraints, deliverables])\nprint(prompt)\n\n\n\n\nThis week frames UI work as a user journey: what does the user do first, next, and last?\nA practical approach is to list screens as verbs: Log in, Choose a quiz, Answer questions, See results.\nOnce screens exist, you map navigation: where can the user go from each screen?\nThis is where AI is helpful: it can suggest missing pages (e.g., error states, empty-state screens).\n\n\n\n\n\nThis week emphasises organising your project so templates don‚Äôt become a mess.\nIn Flask, a common pattern is a templates/ folder and a static/ folder.\nTemplates contain HTML/Jinja; static contains CSS, JavaScript, and images.\nAI can draft a sensible baseline structure, but you must keep it consistent.\nIf your templates are inconsistent (different nav bars per page), your UI will feel broken.\n\n\n\n\n\nA wireframe is not pretty ‚Äî it is a constraint tool.\nYou describe boxes: header, sidebar, main content, footer.\nThis week shows using AI to turn a description into HTML skeleton.\nWhen prompting: specify grid/layout system (Flexbox, CSS grid, Bootstrap) or you‚Äôll get random HTML.\nBe explicit about which elements repeat across pages (navbar, footer).\n\n\n\n\n\nThis week‚Äôs promise: you can achieve modern UI behaviour without deep frontend knowledge.\nResponsive design usually means: a single codebase that adapts to screen width.\nPractical knobs: max-width, min-width, breakpoints, and flexible grids.\nAI is good at generating starter CSS, but you need to test in the browser.\nCommon failure: the UI looks fine on desktop but unusable on mobile.\n\n\n\n\n\nStatic HTML is easy. The challenge is making pages dynamic.\nThis week expects you to connect templates to Flask route variables.\nIn Jinja2 you often loop over data: quizzes, questions, or results.\nAI can generate the loop structure quickly, but you must match variable names to your actual Python.\nAlways confirm: what does the route pass into the template?\n\n\n\n\n\nAs your UI grows, duplication becomes the enemy.\nIn Flask/Jinja, you can create a base template and extend it.\nThis week‚Äôs workflow benefits from: a base layout, then child pages.\nAI can scaffold a base.html with blocks (content, scripts).\nPitfall: students forget to keep blocks consistent and pages break silently.\n\n\n\n\n\nWeek 07 is not a testing week, but UI work still needs verification.\nAt minimum: confirm routes render, form submissions work, and nav links don‚Äôt 404.\nAI can help generate checklists for manual testing.\nIf you have time, write a tiny smoke test using Flask‚Äôs test client.\n\n\n\n\nThis matches this week‚Äôs theme of converting backend data into pages.\n{% extends \"base.html\" %}\n{% block content %}\n&lt;h1&gt;Available quizzes&lt;/h1&gt;\n&lt;ul&gt;\n  {% for quiz in quizzes %}\n    &lt;li&gt;&lt;a href=\"/quiz/{{ quiz.id }}\"&gt;{{ quiz.title }}&lt;/a&gt;&lt;/li&gt;\n  {% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\n\n\nbody { max-width: 960px; margin: 0 auto; padding: 1rem; }\nnav a { margin-right: 1rem; }\n@media (max-width: 600px) {\n  body { padding: 0.5rem; }\n  nav a { display: block; margin: 0.25rem 0; }\n}\n\n\n\n\nWeek 07 uses ChatGPT to move from ‚Äúbackend works‚Äù to ‚Äúusers can actually use it‚Äù.\nThe skill is not magic HTML generation: it‚Äôs prompt scaffolding + iteration.\nWireframes + flows keep AI output grounded.\nFlask templates + Jinja2 are the bridge between Python data and the UI.\nResponsive design is essential: test on multiple screen sizes early.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#what-this-week-is-doing-big-picture",
    "href": "notes/notes_7.html#what-this-week-is-doing-big-picture",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "This week switches focus from backend logic to the part users actually touch: the user interface (UI). The aim is to use chat-based generative AI (especially ChatGPT) as a pair designer + pair developer to:\n\npropose a UI strategy\nwrite structured prompts that produce targeted UI guidance and code\nconvert wireframes and flows into Flask templates + components\nadd responsive HTML/CSS elements without being a frontend expert\n\nThis week is very workflow heavy: it is less ‚Äúwhat is a transformer?‚Äù and more ‚Äúhow do we get from backend routes to a usable application screen?‚Äù",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#key-terms-bolded-the-first-time",
    "href": "notes/notes_7.html#key-terms-bolded-the-first-time",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "We will use these week-specific ideas repeatedly:\n\nWireframe ‚Äî a sketch (boxes + labels) of what the UI should contain and roughly how it should be laid out.\nFlask templates ‚Äî HTML files rendered by Flask (often using Jinja2), letting you combine content + layout + Python variables.\nJinja2 ‚Äî the templating language Flask uses to inject variables/loops/conditionals into HTML.\nResponsive design ‚Äî designing pages so they work on different screen sizes (mobile, tablet, desktop).\nPrompt scaffolding ‚Äî structuring a prompt into sections (objective, constraints, inputs, outputs) so the AI returns usable results.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#getting-our-strategy-from-our-ai-tools",
    "href": "notes/notes_7.html#getting-our-strategy-from-our-ai-tools",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "A key idea early in this week: do strategy first, then code second. The point is that you can waste a lot of time prompting for code before you have agreed what the UI is supposed to do and feel like.\n\n\n\nIf you ask for ‚Äúa frontend‚Äù without specifics, the AI returns generic boilerplate.\nStrategy prompts help you decide: pages, navigation, user flow, components, and naming.\nYou get a UI that matches your backend data model instead of fighting it later.\n\n\n\n\nThis week demonstrates writing a prompt with explicit blocks:\n\nRole (who the AI should act as)\nObjective (what the UI needs to achieve)\nContext (what already exists: backend routes, DB, etc.)\nConstraints (Flask templates, mobile-friendly, accessible)\nDeliverables (pages list, wireframe descriptions, template structure, CSS suggestions)\n\n\n‚ÄúIf we say‚Äùgenerate a frontend for my application‚Äù without enough information, the guidance will be too vague.‚Äù",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#worked-example-prompting-for-a-ui-plan-quarto-runnable-code",
    "href": "notes/notes_7.html#worked-example-prompting-for-a-ui-plan-quarto-runnable-code",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "Below is a ‚Äúprompt builder‚Äù approach: you store prompt sections as strings and assemble them. This makes prompts easier to iterate and reuse.\n#| label: ui-prompt-scaffold\n#| eval: false\nrole = \"Act as a professional Python developer specialising in Flask and UX.\"\nobjective = \"Design a front end for a database-driven Flask quiz application.\"\ncontext = \"Backend exists with routes for login, quizzes, and results.\"\nconstraints = \"Use Flask templates (Jinja2). Keep it responsive and accessible.\"\ndeliverables = \"Return: (1) page list, (2) nav structure, (3) template folder structure, (4) CSS starter.\"\n\nprompt = \"\n\n\".join([role, objective, context, constraints, deliverables])\nprint(prompt)",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#from-flows-to-pages-what-screens-does-the-user-need",
    "href": "notes/notes_7.html#from-flows-to-pages-what-screens-does-the-user-need",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "This week frames UI work as a user journey: what does the user do first, next, and last?\nA practical approach is to list screens as verbs: Log in, Choose a quiz, Answer questions, See results.\nOnce screens exist, you map navigation: where can the user go from each screen?\nThis is where AI is helpful: it can suggest missing pages (e.g., error states, empty-state screens).",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#translating-requirements-into-template-structure",
    "href": "notes/notes_7.html#translating-requirements-into-template-structure",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "This week emphasises organising your project so templates don‚Äôt become a mess.\nIn Flask, a common pattern is a templates/ folder and a static/ folder.\nTemplates contain HTML/Jinja; static contains CSS, JavaScript, and images.\nAI can draft a sensible baseline structure, but you must keep it consistent.\nIf your templates are inconsistent (different nav bars per page), your UI will feel broken.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#wireframes-how-to-ask-the-ai-for-layout-help",
    "href": "notes/notes_7.html#wireframes-how-to-ask-the-ai-for-layout-help",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "A wireframe is not pretty ‚Äî it is a constraint tool.\nYou describe boxes: header, sidebar, main content, footer.\nThis week shows using AI to turn a description into HTML skeleton.\nWhen prompting: specify grid/layout system (Flexbox, CSS grid, Bootstrap) or you‚Äôll get random HTML.\nBe explicit about which elements repeat across pages (navbar, footer).",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#responsive-design-basics-what-this-week-wants-you-to-learn",
    "href": "notes/notes_7.html#responsive-design-basics-what-this-week-wants-you-to-learn",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "This week‚Äôs promise: you can achieve modern UI behaviour without deep frontend knowledge.\nResponsive design usually means: a single codebase that adapts to screen width.\nPractical knobs: max-width, min-width, breakpoints, and flexible grids.\nAI is good at generating starter CSS, but you need to test in the browser.\nCommon failure: the UI looks fine on desktop but unusable on mobile.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#jinja2-integration-ui-that-actually-connects-to-backend-data",
    "href": "notes/notes_7.html#jinja2-integration-ui-that-actually-connects-to-backend-data",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "Static HTML is easy. The challenge is making pages dynamic.\nThis week expects you to connect templates to Flask route variables.\nIn Jinja2 you often loop over data: quizzes, questions, or results.\nAI can generate the loop structure quickly, but you must match variable names to your actual Python.\nAlways confirm: what does the route pass into the template?",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#building-components-avoiding-copy-paste-templates",
    "href": "notes/notes_7.html#building-components-avoiding-copy-paste-templates",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "As your UI grows, duplication becomes the enemy.\nIn Flask/Jinja, you can create a base template and extend it.\nThis week‚Äôs workflow benefits from: a base layout, then child pages.\nAI can scaffold a base.html with blocks (content, scripts).\nPitfall: students forget to keep blocks consistent and pages break silently.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#testing-ui-logic-lightweight-sanity-checks",
    "href": "notes/notes_7.html#testing-ui-logic-lightweight-sanity-checks",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "Week 07 is not a testing week, but UI work still needs verification.\nAt minimum: confirm routes render, form submissions work, and nav links don‚Äôt 404.\nAI can help generate checklists for manual testing.\nIf you have time, write a tiny smoke test using Flask‚Äôs test client.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#code-example-simple-jinja2-loop-for-quiz-list",
    "href": "notes/notes_7.html#code-example-simple-jinja2-loop-for-quiz-list",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "This matches this week‚Äôs theme of converting backend data into pages.\n{% extends \"base.html\" %}\n{% block content %}\n&lt;h1&gt;Available quizzes&lt;/h1&gt;\n&lt;ul&gt;\n  {% for quiz in quizzes %}\n    &lt;li&gt;&lt;a href=\"/quiz/{{ quiz.id }}\"&gt;{{ quiz.title }}&lt;/a&gt;&lt;/li&gt;\n  {% endfor %}\n&lt;/ul&gt;\n{% endblock %}",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#code-example-minimal-responsive-css-skeleton",
    "href": "notes/notes_7.html#code-example-minimal-responsive-css-skeleton",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "body { max-width: 960px; margin: 0 auto; padding: 1rem; }\nnav a { margin-right: 1rem; }\n@media (max-width: 600px) {\n  body { padding: 0.5rem; }\n  nav a { display: block; margin: 0.25rem 0; }\n}",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_7.html#summing-up",
    "href": "notes/notes_7.html#summing-up",
    "title": "Week 07 ‚Äî Building user interfaces with ChatGPT",
    "section": "",
    "text": "Week 07 uses ChatGPT to move from ‚Äúbackend works‚Äù to ‚Äúusers can actually use it‚Äù.\nThe skill is not magic HTML generation: it‚Äôs prompt scaffolding + iteration.\nWireframes + flows keep AI output grounded.\nFlask templates + Jinja2 are the bridge between Python data and the UI.\nResponsive design is essential: test on multiple screen sizes early.",
    "crumbs": [
      "Notes",
      "Notes 07 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html",
    "href": "notes/notes_5.html",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "Using Blackbox AI to generate base code\nApplication development with generative AI tools\nSetting up the development environment\nDeveloping core features\nCreating the database\nConnecting to our database\nCalling our database from the frontend\nRefactoring our Questions class\nModifying our entry point (App.py)\nPulling a set of questions\nCreating a test session in the database\nCreating code for the test session\nGenerating a question set\nVerifying our test session was created\nSumming Up\nFigures extracted from Week 05\n\n\n\n\nThis week builds the next slice of the HAM radio practice-exam app using Blackbox AI as your ‚Äúcode draft engine‚Äù.\nThis week‚Äôs theme is not ‚Äúlet AI build everything.‚Äù It‚Äôs co-development: you steer architecture; AI accelerates implementation.\n\n\n‚ÄúThe goal is to demonstrate effective cooperation between human expertise and AI.‚Äù\n\n\n\n\nBlackbox AI ‚Äî a coding assistant used here to generate base implementations quickly, especially for repetitive web-app scaffolding.\nPersistent session ‚Äî a user state that survives across requests (in Flask, commonly via signed cookies + server-side data decisions).\nSeparation of concerns ‚Äî keeping responsibilities split (routes vs data access vs domain logic), so fixes don‚Äôt ripple everywhere.\nQuestion set ‚Äî a reproducible list of question IDs that defines an exam attempt (so the test can be resumed/graded).\nTest session ‚Äî a database record that tracks a user‚Äôs progress through a question set (position, answers, score).\n\n\n\n\n\n\nThe author frames AI tools as a multiplier: you still need to decide what the system is and where things live (files/modules/classes).\nA practical mental model: treat the assistant like a very fast junior developer‚Äîgreat at drafting, weak at reading your mind.\nYou can accept AI-generated code as a starting point, but you should refactor it into your architecture.\nWhen the AI suggests putting everything into one file, you should resist. This week is about improving structure while moving fast.\n\n\n‚ÄúMaintain separation of concerns‚ÄîEven fast development benefits from clean architecture.‚Äù\n\n\n\n\nPrompts are interfaces. If your prompt is vague, the ‚ÄúAPI response‚Äù (generated code) will be unpredictable.\nIf your prompt is too rigid, you‚Äôll spend more time fighting the output than writing code yourself.\nThe sweet spot is a well-scoped prompt that includes context + constraints + expected behaviour.\n\n\n‚ÄúThis prompt will tell Blackbox what we want it to do and give specificity.‚Äù\n\n\n\n\n\n\nThis week assumes you already have a working Flask project from earlier weeks.\nYou‚Äôre now adding state: question sets and test sessions need to persist across pages and across time.\n\nMinimum local setup:\n\nCreate a virtual environment and install Flask.\nDecide where your database file lives (for a student project, a local SQLite file is fine).\nEnsure your project has a clear module boundary between routes and data access.\n\nA minimal runnable Flask skeleton:\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef index():\n    return {\"status\": \"ok\"}\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\n\n\nThis week‚Äôs development plan is incremental: add the database layer, wire it to the UI, then improve architecture.\nThis is where ‚Äúseparation of concerns‚Äù stops being an academic phrase and becomes a survival tool.\n\n\n\nThe author‚Äôs first instinct is direct:\n\n‚ÄúI start with the database.‚Äù\n\nWhy start here?\n\nWeb apps are stateful experiences built on stateless HTTP. If you don‚Äôt persist state, you‚Äôll rebuild it on every request (painful).\nA database is the simplest shared memory for user progress (even if you later swap SQLite for Postgres).\n\nAnother sentence in this week explains the concrete requirement:\n\n‚ÄúNow I just need to create a table that will store these IDs and some session data for our students.‚Äù\n\n\n\n\nquestion_sets: store a reproducible list of question IDs for an exam attempt.\ntest_sessions: store progress through a question set (index, answers, score).\n\nRunnable SQLite setup (creates tables):\nimport sqlite3\n\nDB_PATH = \"week05_demo.sqlite\"\n\nschema_sql = \"\"\"\nCREATE TABLE IF NOT EXISTS question_sets (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n  question_ids TEXT NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS test_sessions (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  question_set_id INTEGER NOT NULL,\n  current_index INTEGER DEFAULT 0,\n  score INTEGER DEFAULT 0,\n  created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n  FOREIGN KEY(question_set_id) REFERENCES question_sets(id)\n);\n\"\"\"\n\nwith sqlite3.connect(DB_PATH) as conn:\n    conn.executescript(schema_sql)\n\nprint(\"OK: tables created in\", DB_PATH)\n\n\n\n\nThis week uses SQLite for simplicity. The key engineering decision is where to put connection logic.\n\nGood: db.py (or database.py) owns connection creation, and other modules import helper functions.\nRisky: every route creates its own SQL strings and connections inline (hard to test, hard to debug).\n\nA minimal ‚Äúdb helper‚Äù module pattern:\n# db.py\nimport sqlite3\nfrom pathlib import Path\n\nDB_PATH = Path(\"week05_demo.sqlite\")\n\ndef get_conn():\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    return conn\n\n\n\n\nThe UI needs to request ‚Äúthe next question‚Äù and submit ‚Äúan answer‚Äù.\nThe backend needs to read/write the current test session (progress, scoring).\n\nA minimal pattern: use routes as thin controllers; keep SQL out of the route when possible.\n\n\n\n\nAs features grow, a ‚ÄúQuestions‚Äù class tends to become a dumping ground.\nRefactoring here is about separating: (1) question bank access, (2) question selection, (3) rendering/formatting.\n\n\n\n\n\nEntry points tend to accumulate imports, global objects, and configuration.\nThis week nudges you toward clarity: keep initialization in one place, but push functionality into modules.\n\n\n\n\n\nA question set is your ‚Äúfrozen exam‚Äù. Once created, you should be able to reproduce it exactly.\nThis prevents the classic bug where refreshing the page gives a different test.\n\n\n\n\n\nA test session is a record that points to a question set + stores progress.\nThis is also where session cookies + DB state meet: cookie identifies the browser; DB stores the durable progress.\n\nA student-friendly session cookie setup in Flask:\nfrom flask import Flask, session\n\napp = Flask(__name__)\napp.secret_key = \"dev-only-change-me\"  # in real apps, use env vars\n\n@app.get(\"/set\")\ndef set_value():\n    session[\"user_id\"] = 123\n    return {\"ok\": True}\n\n@app.get(\"/get\")\ndef get_value():\n    return {\"user_id\": session.get(\"user_id\")}\n\n\n\n\nThis week‚Äôs practical point: let AI draft the route handlers, but you must ensure they call your data layer cleanly.\nKeep the contract obvious: create_session(question_set_id) -&gt; session_id and get_session(session_id) -&gt; record.\n\n\n\n\n\nGenerating here means: selecting question IDs (not generating new questions).\nYou want deterministic reproducibility: store the IDs you chose.\n\n\n\n\n\nVerification is not optional. AI makes it easy to write code that ‚Äúlooks right‚Äù but fails at runtime.\nCheck: DB rows exist, foreign keys match, and the UI can resume where it left off.\n\nA tiny verification query (runnable):\nimport sqlite3\n\nDB_PATH = \"week05_demo.sqlite\"\nwith sqlite3.connect(DB_PATH) as conn:\n    qs = conn.execute(\"SELECT COUNT(*) FROM question_sets\").fetchone()[0]\n    ts = conn.execute(\"SELECT COUNT(*) FROM test_sessions\").fetchone()[0]\n\nprint({\"question_sets\": qs, \"test_sessions\": ts})\n\n\n\n\n\nThis week is a case study in using Blackbox AI for speed while still enforcing architecture.\nThe most important engineering idea is separation of concerns: routes orchestrate, domain logic decides, data layer persists.\nThe product outcome is a test experience that can persist progress across pages: question sets + test sessions.",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html#using-blackbox-ai-to-generate-base-code",
    "href": "notes/notes_5.html#using-blackbox-ai-to-generate-base-code",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "This week builds the next slice of the HAM radio practice-exam app using Blackbox AI as your ‚Äúcode draft engine‚Äù.\nThis week‚Äôs theme is not ‚Äúlet AI build everything.‚Äù It‚Äôs co-development: you steer architecture; AI accelerates implementation.\n\n\n‚ÄúThe goal is to demonstrate effective cooperation between human expertise and AI.‚Äù\n\n\n\n\nBlackbox AI ‚Äî a coding assistant used here to generate base implementations quickly, especially for repetitive web-app scaffolding.\nPersistent session ‚Äî a user state that survives across requests (in Flask, commonly via signed cookies + server-side data decisions).\nSeparation of concerns ‚Äî keeping responsibilities split (routes vs data access vs domain logic), so fixes don‚Äôt ripple everywhere.\nQuestion set ‚Äî a reproducible list of question IDs that defines an exam attempt (so the test can be resumed/graded).\nTest session ‚Äî a database record that tracks a user‚Äôs progress through a question set (position, answers, score).",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html#application-development-with-generative-ai-tools",
    "href": "notes/notes_5.html#application-development-with-generative-ai-tools",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "The author frames AI tools as a multiplier: you still need to decide what the system is and where things live (files/modules/classes).\nA practical mental model: treat the assistant like a very fast junior developer‚Äîgreat at drafting, weak at reading your mind.\nYou can accept AI-generated code as a starting point, but you should refactor it into your architecture.\nWhen the AI suggests putting everything into one file, you should resist. This week is about improving structure while moving fast.\n\n\n‚ÄúMaintain separation of concerns‚ÄîEven fast development benefits from clean architecture.‚Äù\n\n\n\n\nPrompts are interfaces. If your prompt is vague, the ‚ÄúAPI response‚Äù (generated code) will be unpredictable.\nIf your prompt is too rigid, you‚Äôll spend more time fighting the output than writing code yourself.\nThe sweet spot is a well-scoped prompt that includes context + constraints + expected behaviour.\n\n\n‚ÄúThis prompt will tell Blackbox what we want it to do and give specificity.‚Äù",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html#setting-up-the-development-environment",
    "href": "notes/notes_5.html#setting-up-the-development-environment",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "This week assumes you already have a working Flask project from earlier weeks.\nYou‚Äôre now adding state: question sets and test sessions need to persist across pages and across time.\n\nMinimum local setup:\n\nCreate a virtual environment and install Flask.\nDecide where your database file lives (for a student project, a local SQLite file is fine).\nEnsure your project has a clear module boundary between routes and data access.\n\nA minimal runnable Flask skeleton:\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef index():\n    return {\"status\": \"ok\"}\n\nif __name__ == \"__main__\":\n    app.run(debug=True)",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html#developing-core-features",
    "href": "notes/notes_5.html#developing-core-features",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "This week‚Äôs development plan is incremental: add the database layer, wire it to the UI, then improve architecture.\nThis is where ‚Äúseparation of concerns‚Äù stops being an academic phrase and becomes a survival tool.\n\n\n\nThe author‚Äôs first instinct is direct:\n\n‚ÄúI start with the database.‚Äù\n\nWhy start here?\n\nWeb apps are stateful experiences built on stateless HTTP. If you don‚Äôt persist state, you‚Äôll rebuild it on every request (painful).\nA database is the simplest shared memory for user progress (even if you later swap SQLite for Postgres).\n\nAnother sentence in this week explains the concrete requirement:\n\n‚ÄúNow I just need to create a table that will store these IDs and some session data for our students.‚Äù\n\n\n\n\nquestion_sets: store a reproducible list of question IDs for an exam attempt.\ntest_sessions: store progress through a question set (index, answers, score).\n\nRunnable SQLite setup (creates tables):\nimport sqlite3\n\nDB_PATH = \"week05_demo.sqlite\"\n\nschema_sql = \"\"\"\nCREATE TABLE IF NOT EXISTS question_sets (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n  question_ids TEXT NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS test_sessions (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n  question_set_id INTEGER NOT NULL,\n  current_index INTEGER DEFAULT 0,\n  score INTEGER DEFAULT 0,\n  created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n  FOREIGN KEY(question_set_id) REFERENCES question_sets(id)\n);\n\"\"\"\n\nwith sqlite3.connect(DB_PATH) as conn:\n    conn.executescript(schema_sql)\n\nprint(\"OK: tables created in\", DB_PATH)\n\n\n\n\nThis week uses SQLite for simplicity. The key engineering decision is where to put connection logic.\n\nGood: db.py (or database.py) owns connection creation, and other modules import helper functions.\nRisky: every route creates its own SQL strings and connections inline (hard to test, hard to debug).\n\nA minimal ‚Äúdb helper‚Äù module pattern:\n# db.py\nimport sqlite3\nfrom pathlib import Path\n\nDB_PATH = Path(\"week05_demo.sqlite\")\n\ndef get_conn():\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    return conn\n\n\n\n\nThe UI needs to request ‚Äúthe next question‚Äù and submit ‚Äúan answer‚Äù.\nThe backend needs to read/write the current test session (progress, scoring).\n\nA minimal pattern: use routes as thin controllers; keep SQL out of the route when possible.\n\n\n\n\nAs features grow, a ‚ÄúQuestions‚Äù class tends to become a dumping ground.\nRefactoring here is about separating: (1) question bank access, (2) question selection, (3) rendering/formatting.\n\n\n\n\n\nEntry points tend to accumulate imports, global objects, and configuration.\nThis week nudges you toward clarity: keep initialization in one place, but push functionality into modules.\n\n\n\n\n\nA question set is your ‚Äúfrozen exam‚Äù. Once created, you should be able to reproduce it exactly.\nThis prevents the classic bug where refreshing the page gives a different test.\n\n\n\n\n\nA test session is a record that points to a question set + stores progress.\nThis is also where session cookies + DB state meet: cookie identifies the browser; DB stores the durable progress.\n\nA student-friendly session cookie setup in Flask:\nfrom flask import Flask, session\n\napp = Flask(__name__)\napp.secret_key = \"dev-only-change-me\"  # in real apps, use env vars\n\n@app.get(\"/set\")\ndef set_value():\n    session[\"user_id\"] = 123\n    return {\"ok\": True}\n\n@app.get(\"/get\")\ndef get_value():\n    return {\"user_id\": session.get(\"user_id\")}\n\n\n\n\nThis week‚Äôs practical point: let AI draft the route handlers, but you must ensure they call your data layer cleanly.\nKeep the contract obvious: create_session(question_set_id) -&gt; session_id and get_session(session_id) -&gt; record.\n\n\n\n\n\nGenerating here means: selecting question IDs (not generating new questions).\nYou want deterministic reproducibility: store the IDs you chose.\n\n\n\n\n\nVerification is not optional. AI makes it easy to write code that ‚Äúlooks right‚Äù but fails at runtime.\nCheck: DB rows exist, foreign keys match, and the UI can resume where it left off.\n\nA tiny verification query (runnable):\nimport sqlite3\n\nDB_PATH = \"week05_demo.sqlite\"\nwith sqlite3.connect(DB_PATH) as conn:\n    qs = conn.execute(\"SELECT COUNT(*) FROM question_sets\").fetchone()[0]\n    ts = conn.execute(\"SELECT COUNT(*) FROM test_sessions\").fetchone()[0]\n\nprint({\"question_sets\": qs, \"test_sessions\": ts})",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_5.html#summing-up",
    "href": "notes/notes_5.html#summing-up",
    "title": "Week 05 ‚Äî Application development with Blackbox AI",
    "section": "",
    "text": "This week is a case study in using Blackbox AI for speed while still enforcing architecture.\nThe most important engineering idea is separation of concerns: routes orchestrate, domain logic decides, data layer persists.\nThe product outcome is a test experience that can persist progress across pages: question sets + test sessions.",
    "crumbs": [
      "Notes",
      "Notes 05 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html",
    "href": "notes/notes_3.html",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Week focus: turning vague intent into reliable, testable prompt instructions.\n\n\nFigures used in this week\nWhy prompting is a design skill\nPrompt anatomy\nFrom vague to specific\nConstraints, rubrics, and acceptance tests\nFew-shot examples\nOutput formats that don‚Äôt break\nIteration loop\nFailure modes + debugging prompts\nPrompt patterns you can reuse\nSumming Up\nAppendix A ‚Äî Worked walkthrough: building a prompt step by step\n\n\n\nPrompting is not ‚Äúasking nicely‚Äù. In the real world, you use prompts to specify behaviour under constraints:\n\nyou want repeatable outputs\nyou want outputs that are checkable\nyou want the model to use the right information\nyou want it to behave sensibly when information is missing\n\nA useful mindset: a prompt is closer to a function signature than a conversation.\n\n‚ÄúA prompt is a program in natural language.‚Äù\n\n\n\n\nExplain the task (what success looks like)\nConstrain the output (how the response must be shaped)\n\n\n\n\n\nA practical prompt usually contains these parts (not always in this order):\n\nRole: ‚ÄúYou are a ‚Ä¶‚Äù (useful when behaviour matters)\nGoal: what to produce\nContext: what the model should know or assume\nInputs: what you are providing (data, text, code, constraints)\nRules: explicit do/don‚Äôt constraints\nOutput format: JSON, Markdown, table, bullet list, etc\nQuality bar: rubric / checklist / tests\n\n\n\n\nRole prompting: framing the model‚Äôs perspective and responsibilities.\nConstraints: explicit limits that prevent unwanted behaviour or formats.\nFew-shot prompting: teaching by giving examples of desired input‚Üíoutput.\nRubric: a checklist the model should satisfy.\nOutput schema: a formal structure the output must follow (e.g., JSON).\n\n\n\n\nBad (vague):\nWrite a good explanation of SQL joins.\nBetter (engineered):\nYou are a first-year teaching assistant.\nExplain SQL joins to beginners using:\n- exactly 3 join types (INNER, LEFT, FULL)\n- one simple table example\n- one common mistake per join type\nOutput as markdown with headings and a final summary.\n\n\n\n\nMany prompts fail because they skip specification. You can convert vague intent into a usable prompt by answering:\n\nWhat is the deliverable? (an explanation / a plan / code / critique)\nWho is the audience?\nWhat constraints exist? (length, style, tools, time)\nWhat does success look like? (rubric / checklist)\nWhat should never happen? (anti-requirements)\n\n\n‚ÄúBe specific about what you want and what you don‚Äôt want.‚Äù\n\n\n\nTake the vague prompt:\nHelp me debug my Python.\nRewrite it into something testable:\nYou are a Python tutor. I will paste code and an error message.\n1) Explain what the error means in plain language\n2) Identify the exact line likely causing it\n3) Provide a fixed version of the code\n4) Add one print/debug statement that proves the fix works\nKeep changes minimal.\n\n\n\n\nIf you want outputs you can trust, you need a quality gate. This can be:\n\nA short rubric (‚Äúmust include X, avoid Y‚Äù)\nAn acceptance test (unit tests, schema validation)\nA format requirement (JSON schema, markdown headings)\n\n\n‚ÄúMake the output easy to verify.‚Äù\n\n\n\n# A tiny ‚Äúacceptance test‚Äù for AI-written text outputs\ndef contains_required_phrases(text: str, required: list[str]) -&gt; bool:\n    return all(phrase.lower() in text.lower() for phrase in required)\n\nsample = \"This answer covers INNER JOIN, LEFT JOIN and FULL JOIN.\"\nprint(contains_required_phrases(sample, [\"INNER JOIN\", \"LEFT JOIN\", \"FULL JOIN\"]))\n\n\n\n\nFew-shot prompting is when you show the model examples of the behaviour you want. This is especially powerful when you want consistent style or formatting.\n\n\n\nWhen outputs must match a ‚Äúhouse style‚Äù\nWhen the task is ambiguous (many valid interpretations)\nWhen you care about edge-cases\n\n\n‚ÄúExamples reduce ambiguity.‚Äù\n\n\n\n\nTask: Convert short notes into a structured study card.\nFormat:\n- Term\n- Definition\n- Example\n- Common mistake\n\nExample input:\n\"SQL LEFT JOIN keeps all rows from the left table\"\nExample output:\nTerm: LEFT JOIN\nDefinition: Returns all rows from left table plus matching right rows.\nExample: ...\nCommon mistake: Confusing with INNER JOIN.\n\nNow do the same for: \"FULL JOIN keeps all rows from both\"\n\n\n\n\nIn real workflows, AI output often becomes input to something else: - a script - a web app - a report - a database\nThat means formatting matters. If you don‚Äôt force structure, you‚Äôll get ‚Äúpretty‚Äù output that is unusable.\n\n‚ÄúAsk for the output format you need.‚Äù\n\n\n\nimport json\n\nraw = \"\"\"{\n  \"title\": \"Study card\",\n  \"points\": [\"one\", \"two\"]\n}\"\"\"\ndata = json.loads(raw)\nprint(data[\"title\"], data[\"points\"])\n\n\n\n\nPrompting is iterative. A common loop is:\n\nDraft prompt\nRun it on a few cases\nDiagnose failure modes\nAdd constraints / examples\nRe-run\n\n\n‚ÄúIterate on prompts like you iterate on code.‚Äù\n\n\n\nPROMPT_V1 = \"Summarise this article.\"\nPROMPT_V2 = \"Summarise this article in 5 bullets for first-year students.\"\nPROMPT_V3 = PROMPT_V2 + \" Include one limitation and one question.\"\n\nfor i, p in enumerate([PROMPT_V1, PROMPT_V2, PROMPT_V3], start=1):\n    print(f\"v{i}: {p}\")\n\n\n\n\nCommon failure modes in early prompting:\n\nOverly broad instructions ‚Üí generic output\nMissing constraints ‚Üí wrong format, wrong length\nUnclear audience ‚Üí tone mismatch\nNo examples ‚Üí inconsistent style\nNo rubric ‚Üí ‚Äúsounds good‚Äù but incomplete\n\n\n\nYou produced an answer that failed these requirements:\n- (list failures)\nExplain why it failed and produce a corrected version.\nThen provide a checklist to prevent the same failure next time.\n\n\n\n\nBelow are reusable patterns you can copy into real work.\n\n\nDo the task. Then verify your output against this checklist:\n- ...\nIf anything fails, fix and re-check.\n\n\n\nExplain this concept simply. Then ask me 5 questions, increasing difficulty.\nGive answers at the end.\n\n\n\nGive 3 options. For each, include: benefits, drawbacks, best use-case.\nFinish with a recommendation for a beginner.\n\n\n\n\n\nPrompting is best treated as specification and design.\nGood prompts include constraints, a format, and a quality gate.\nFew-shot examples reduce ambiguity and increase consistency.\nIteration is normal: test, debug, refine.\nOutputs should be structured so they can be verified and reused.\n\n\n\n\n\n\n\nWe begin with an intent that a human understands but a model can misinterpret.\nExample intent: Make this explanation better.\nThe phrase better is underspecified: better for who, in what way, with what constraints?\nIf you leave this vague, the model may change style, length, or even meaning.\n\n\n\n\n\nDecide who the output is for (audience).\nFor week 3, our audience is first-year university students.\nPurpose might be: quick revision notes, lecture handout, or tutorial support.\nAudience + purpose affect tone, vocabulary, and example choice.\n\n\n\n\n\nConstraints prevent ‚Äòcreative drift‚Äô.\nCommon constraints: word count, formatting, required sections, no jargon.\nConstraints also help the model prioritise: if space is limited, it must focus.\n\n\n\n\n\nA rubric is a checklist your output should pass.\nRubrics are powerful because they are explicit, testable, and reusable.\nExample rubric: must include 1 analogy, 1 code example, 3 bullet takeaways.\n\n\n\n\n\nIf the output must feed another tool, specify a strict format.\nExamples: JSON, Markdown headings, CSV, YAML front matter.\nIn Quarto notes, Markdown + headings is usually enough.\n\n\n\n\n\nIf you care about style, show the model the style.\nOne example pair can dramatically increase consistency.\nToo many examples can bloat the prompt, so start small.\n\n\n\n\n\nWhen output fails, don‚Äôt just re-run: diagnose.\nAsk: which instruction was ignored? Was it ambiguous? Was it missing?\nThen modify the prompt so the failure becomes impossible or unlikely.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#why-prompting-is-a-design-skill",
    "href": "notes/notes_3.html#why-prompting-is-a-design-skill",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Prompting is not ‚Äúasking nicely‚Äù. In the real world, you use prompts to specify behaviour under constraints:\n\nyou want repeatable outputs\nyou want outputs that are checkable\nyou want the model to use the right information\nyou want it to behave sensibly when information is missing\n\nA useful mindset: a prompt is closer to a function signature than a conversation.\n\n‚ÄúA prompt is a program in natural language.‚Äù\n\n\n\n\nExplain the task (what success looks like)\nConstrain the output (how the response must be shaped)",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#prompt-anatomy",
    "href": "notes/notes_3.html#prompt-anatomy",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "A practical prompt usually contains these parts (not always in this order):\n\nRole: ‚ÄúYou are a ‚Ä¶‚Äù (useful when behaviour matters)\nGoal: what to produce\nContext: what the model should know or assume\nInputs: what you are providing (data, text, code, constraints)\nRules: explicit do/don‚Äôt constraints\nOutput format: JSON, Markdown, table, bullet list, etc\nQuality bar: rubric / checklist / tests\n\n\n\n\nRole prompting: framing the model‚Äôs perspective and responsibilities.\nConstraints: explicit limits that prevent unwanted behaviour or formats.\nFew-shot prompting: teaching by giving examples of desired input‚Üíoutput.\nRubric: a checklist the model should satisfy.\nOutput schema: a formal structure the output must follow (e.g., JSON).\n\n\n\n\nBad (vague):\nWrite a good explanation of SQL joins.\nBetter (engineered):\nYou are a first-year teaching assistant.\nExplain SQL joins to beginners using:\n- exactly 3 join types (INNER, LEFT, FULL)\n- one simple table example\n- one common mistake per join type\nOutput as markdown with headings and a final summary.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#from-vague-to-specific",
    "href": "notes/notes_3.html#from-vague-to-specific",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Many prompts fail because they skip specification. You can convert vague intent into a usable prompt by answering:\n\nWhat is the deliverable? (an explanation / a plan / code / critique)\nWho is the audience?\nWhat constraints exist? (length, style, tools, time)\nWhat does success look like? (rubric / checklist)\nWhat should never happen? (anti-requirements)\n\n\n‚ÄúBe specific about what you want and what you don‚Äôt want.‚Äù\n\n\n\nTake the vague prompt:\nHelp me debug my Python.\nRewrite it into something testable:\nYou are a Python tutor. I will paste code and an error message.\n1) Explain what the error means in plain language\n2) Identify the exact line likely causing it\n3) Provide a fixed version of the code\n4) Add one print/debug statement that proves the fix works\nKeep changes minimal.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#constraints-rubrics-and-acceptance-tests",
    "href": "notes/notes_3.html#constraints-rubrics-and-acceptance-tests",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "If you want outputs you can trust, you need a quality gate. This can be:\n\nA short rubric (‚Äúmust include X, avoid Y‚Äù)\nAn acceptance test (unit tests, schema validation)\nA format requirement (JSON schema, markdown headings)\n\n\n‚ÄúMake the output easy to verify.‚Äù\n\n\n\n# A tiny ‚Äúacceptance test‚Äù for AI-written text outputs\ndef contains_required_phrases(text: str, required: list[str]) -&gt; bool:\n    return all(phrase.lower() in text.lower() for phrase in required)\n\nsample = \"This answer covers INNER JOIN, LEFT JOIN and FULL JOIN.\"\nprint(contains_required_phrases(sample, [\"INNER JOIN\", \"LEFT JOIN\", \"FULL JOIN\"]))",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#few-shot-examples",
    "href": "notes/notes_3.html#few-shot-examples",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Few-shot prompting is when you show the model examples of the behaviour you want. This is especially powerful when you want consistent style or formatting.\n\n\n\nWhen outputs must match a ‚Äúhouse style‚Äù\nWhen the task is ambiguous (many valid interpretations)\nWhen you care about edge-cases\n\n\n‚ÄúExamples reduce ambiguity.‚Äù\n\n\n\n\nTask: Convert short notes into a structured study card.\nFormat:\n- Term\n- Definition\n- Example\n- Common mistake\n\nExample input:\n\"SQL LEFT JOIN keeps all rows from the left table\"\nExample output:\nTerm: LEFT JOIN\nDefinition: Returns all rows from left table plus matching right rows.\nExample: ...\nCommon mistake: Confusing with INNER JOIN.\n\nNow do the same for: \"FULL JOIN keeps all rows from both\"",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#output-formats-that-dont-break",
    "href": "notes/notes_3.html#output-formats-that-dont-break",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "In real workflows, AI output often becomes input to something else: - a script - a web app - a report - a database\nThat means formatting matters. If you don‚Äôt force structure, you‚Äôll get ‚Äúpretty‚Äù output that is unusable.\n\n‚ÄúAsk for the output format you need.‚Äù\n\n\n\nimport json\n\nraw = \"\"\"{\n  \"title\": \"Study card\",\n  \"points\": [\"one\", \"two\"]\n}\"\"\"\ndata = json.loads(raw)\nprint(data[\"title\"], data[\"points\"])",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#iteration-loop",
    "href": "notes/notes_3.html#iteration-loop",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Prompting is iterative. A common loop is:\n\nDraft prompt\nRun it on a few cases\nDiagnose failure modes\nAdd constraints / examples\nRe-run\n\n\n‚ÄúIterate on prompts like you iterate on code.‚Äù\n\n\n\nPROMPT_V1 = \"Summarise this article.\"\nPROMPT_V2 = \"Summarise this article in 5 bullets for first-year students.\"\nPROMPT_V3 = PROMPT_V2 + \" Include one limitation and one question.\"\n\nfor i, p in enumerate([PROMPT_V1, PROMPT_V2, PROMPT_V3], start=1):\n    print(f\"v{i}: {p}\")",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#failure-modes-debugging-prompts",
    "href": "notes/notes_3.html#failure-modes-debugging-prompts",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Common failure modes in early prompting:\n\nOverly broad instructions ‚Üí generic output\nMissing constraints ‚Üí wrong format, wrong length\nUnclear audience ‚Üí tone mismatch\nNo examples ‚Üí inconsistent style\nNo rubric ‚Üí ‚Äúsounds good‚Äù but incomplete\n\n\n\nYou produced an answer that failed these requirements:\n- (list failures)\nExplain why it failed and produce a corrected version.\nThen provide a checklist to prevent the same failure next time.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#prompt-patterns-you-can-reuse",
    "href": "notes/notes_3.html#prompt-patterns-you-can-reuse",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Below are reusable patterns you can copy into real work.\n\n\nDo the task. Then verify your output against this checklist:\n- ...\nIf anything fails, fix and re-check.\n\n\n\nExplain this concept simply. Then ask me 5 questions, increasing difficulty.\nGive answers at the end.\n\n\n\nGive 3 options. For each, include: benefits, drawbacks, best use-case.\nFinish with a recommendation for a beginner.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#summing-up",
    "href": "notes/notes_3.html#summing-up",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "Prompting is best treated as specification and design.\nGood prompts include constraints, a format, and a quality gate.\nFew-shot examples reduce ambiguity and increase consistency.\nIteration is normal: test, debug, refine.\nOutputs should be structured so they can be verified and reused.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_3.html#appendix-a-worked-walkthrough-building-a-prompt-step-by-step",
    "href": "notes/notes_3.html#appendix-a-worked-walkthrough-building-a-prompt-step-by-step",
    "title": "Week 03 ‚Äî Designing Your First Prompt",
    "section": "",
    "text": "We begin with an intent that a human understands but a model can misinterpret.\nExample intent: Make this explanation better.\nThe phrase better is underspecified: better for who, in what way, with what constraints?\nIf you leave this vague, the model may change style, length, or even meaning.\n\n\n\n\n\nDecide who the output is for (audience).\nFor week 3, our audience is first-year university students.\nPurpose might be: quick revision notes, lecture handout, or tutorial support.\nAudience + purpose affect tone, vocabulary, and example choice.\n\n\n\n\n\nConstraints prevent ‚Äòcreative drift‚Äô.\nCommon constraints: word count, formatting, required sections, no jargon.\nConstraints also help the model prioritise: if space is limited, it must focus.\n\n\n\n\n\nA rubric is a checklist your output should pass.\nRubrics are powerful because they are explicit, testable, and reusable.\nExample rubric: must include 1 analogy, 1 code example, 3 bullet takeaways.\n\n\n\n\n\nIf the output must feed another tool, specify a strict format.\nExamples: JSON, Markdown headings, CSV, YAML front matter.\nIn Quarto notes, Markdown + headings is usually enough.\n\n\n\n\n\nIf you care about style, show the model the style.\nOne example pair can dramatically increase consistency.\nToo many examples can bloat the prompt, so start small.\n\n\n\n\n\nWhen output fails, don‚Äôt just re-run: diagnose.\nAsk: which instruction was ignored? Was it ambiguous? Was it missing?\nThen modify the prompt so the failure becomes impossible or unlikely.",
    "crumbs": [
      "Notes",
      "Notes 03 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html",
    "href": "notes/notes_10.html",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Objectives\nWhat is vibe coding?\nWhat is Cursor, and why is it different?\nProject-wide context and customization\nFirst concept: build something small, then improve it\nThe initial prompt to build our game\nCursor basics (chat vs edit vs autocomplete)\nGiving feedback (how to steer the model)\nAdding context (what to paste into chat)\nSelecting a mode (decision table)\nModel selection (speed vs quality)\nMAX mode (when you need the heavy hammer)\nResults from the first prompt: expect rough edges\nRunning our game for the first time (environment + dependencies)\nMaking changes to our game: small diffs win\nThe second prompt (iteration with structure)\nDebugging in a vibe-coding workflow\nFigure: example diff\nRapid-fire practical reminders (for students)\nExtra extracted figures (from this week)\n\n\n‚ÄúVibe coding‚Äù is a workflow, not a magic trick.\n\nThis week we‚Äôll explore vibe coding by building a small game (a simple working program you can run).\n\n\nWeek 10 moves from ‚Äúcoding with AI as a helper‚Äù to a conversational programming workflow where your editor + model become a co-author.\n\nHow Cursor differs from ‚Äújust ChatGPT in a browser‚Äù\nWhat vibe coding means in practice (and what it does not mean)\nHow to prompt for a whole working program and then iterate safely\nHow to use Cursor‚Äôs modes (chat, edit, autocomplete) deliberately\nHow to keep control: debugging, tests, and small edits instead of chaos\n\n\n\n\n\n‚Äúvibe coding‚Äù is basically: you steer by intent and feedback, not by writing every line from scratch.\nYou start by describing behavior (what the program should do) rather than implementation (how it does it).\nThe AI proposes code.\nYou run it, observe, and then refine the prompt (or apply edits) until it behaves.\n\n\n‚ÄúThe feedback loop is the ‚Äúvibe‚Äù.‚Äù\n\n\n\n\nLLMs are strong at first drafts and pattern-matching.\nEditors like Cursor provide context (your whole repo), so the model can be more precise.\nYou can move faster when you accept that drafts are disposable.\n\n\n\n\n\n‚Äúno thinking‚Äù.\n‚Äútrust the model blindly‚Äù.\n‚Äúskip debugging‚Äù.\n\n\n\n\n\nOver-scoping: asking for ‚Äúbuild the whole app‚Äù without constraints.\nUnder-specifying: vague goals produce vague programs.\nContext leakage: if your project contains confusing files, the model can copy the wrong patterns.\n\n\n\n\n\nCursor is an IDE designed to make AI part of the editing loop. The big difference is context + actions:\n\nIt sees your open files, selections, and (optionally) the entire project.\nIt can apply edits directly (not just suggest them).\nIt supports modes that match real programming tasks.\n\nIn other words, it‚Äôs not ‚Äúchatting about code‚Äù ‚Äî it‚Äôs editing code with chat.\n\n\n\nYou can ask for changes in-place (e.g., ‚Äúrefactor this function‚Äù).\nYou can request multi-file modifications (e.g., ‚Äúadd a new module + tests‚Äù).\nYou can iterate with smaller deltas, which makes reviewing easier.\n\n\n\n\n\nCursor can make too many edits if your prompt is broad.\nAlways review diffs like you would a teammate PR.\n\n\n\n\n\nCursor becomes powerful when it understands your project:\n\nFolder structure\nDependencies\nNaming conventions\n‚ÄúHow we do things here‚Äù\n\nThis is where good software engineering meets AI.\n\n\nEven with Cursor, models still have limited context windows. So the art is:\n\nOnly include what matters\nSummarize what‚Äôs irrelevant\nAvoid dumping huge logs without focus\n\nIf the AI keeps making the same mistake, your context is missing something. Fix it by adding:\n\nconstraints (‚Äúdo not change public API‚Äù)\nexamples (‚Äúhere is one correct pattern in this repo‚Äù)\na test (‚Äúthis must pass‚Äù)\n\n\n\n\n\nThis is a great productivity pattern because:\n\nSmall apps expose real engineering problems quickly: I/O, state, loops, UI feedback\nStudents can run + test without needing deployment\nIteration is visible\n\n\n\nYou want early success:\n\na window opens\nsomething responds\nthe loop runs\n\nThen you polish.\n\n\n\n\n\n‚ÄúDon‚Äôt be vague!‚Äù\n\nA strong initial prompt contains:\n\ngoal (what game?)\nplatform (Python? Pygame? terminal?)\nconstraints (keep it simple, one file)\nexpected behavior (controls, win condition)\noutput format (code only, plus run instructions)\n\n\n\nYou are an expert Python developer.\n\nBuild a simple Pong-style game in Python using pygame.\n\nConstraints:\n- Single file: game.py\n- Keep it beginner friendly (clear functions, comments)\n- WASD controls for left paddle, arrow keys for right paddle\n- Score displayed at top\n- Press Q to quit\n\nOutput:\n- Provide the complete code for game.py\n- Then provide exact run steps (pip + python)\n\n\n\n\nIt creates a format contract: the model knows what to output.\nIt defines inputs (controls), outputs (score), and stop condition (quit).\nIt sets the difficulty level (beginner friendly).\n\n\n\n\n\nCursor gives you multiple ways to use AI, and mixing them up causes pain.\n\n\nUse for:\n\ndesign discussion (‚Äúwhat approach should we take?‚Äù)\ndebugging (‚Äúhere‚Äôs the stack trace, what does it mean?‚Äù)\nplanning (‚Äúbreak this into steps‚Äù)\n\n\n\n\nUse for:\n\ntargeted rewrites (‚Äúsimplify this function‚Äù)\ntransformations (‚Äúconvert to dataclass‚Äù)\nlocal refactors (‚Äúrename variables consistently‚Äù)\n\n\n\n\nUse for:\n\nwriting boilerplate quickly\nfinishing predictable patterns\nstaying in flow\n\nTeaching tip: chat is best when you don‚Äôt know what to do, edit is best when you know what you want changed.\n\n\n\n\nFeedback is the steering wheel of vibe coding.\nGood feedback is:\n\nspecific (‚Äúthe paddle is moving too fast‚Äù)\ntestable (‚Äúball should bounce at the same angle‚Äù)\nconstrained (‚Äúonly change movement speed constants‚Äù)\n\nBad feedback is:\n\nemotional (‚Äúthis is broken‚Äù)\nvague (‚Äúmake it better‚Äù)\n\n\n\n\nWhat did I expect?\nWhat happened instead?\nWhere in the code does that behavior live?\nWhat is the smallest change that could fix it?\n\n\n\n\n\nThe temptation is to paste everything. Don‚Äôt.\nPaste only:\n\nthe failing function\nthe error message\nthe minimal reproduction steps\n\nExample:\nBug report:\n- When I press W, the paddle moves diagonally.\n\nRepro:\n1. Run game.py\n2. Press W for 2 seconds\n\nExpected:\n- Paddle moves up only.\n\nActual:\n- Paddle moves up + right.\n\nHere is the movement code (lines 41-68):\n[paste snippet]\nThis gives the model a ‚Äúdebug ticket‚Äù it can solve.\n\n\n\n\n\n\nTask\nBest Cursor mode\nWhy\n\n\n\n\n‚ÄúExplain this error‚Äù\nChat\nreasoning + teaching\n\n\n‚ÄúRewrite this function‚Äù\nEdit\ndirect transformation\n\n\n‚ÄúAdd docstrings‚Äù\nEdit\nrepetitive change\n\n\n‚ÄúFinish this for-loop‚Äù\nAutocomplete\npredictable\n\n\n‚ÄúDesign a module layout‚Äù\nChat\nbrainstorming\n\n\n\nNB:  Cursor is constantly being updated, the modes may have slightly different names or capabilities.\n\n\n\nDifferent models behave differently:\n\nsome are faster, but more shallow\nsome reason better, but slower\nsome follow formatting better\n\nYou should practice:\n\nuse the ‚Äúsmart‚Äù model for big architectural tasks\nuse the ‚Äúfast‚Äù model for repetitive refactors\n\n\n\n\nMAX mode is useful when:\n\nthe codebase is large\nthe change is cross-cutting\nyou need the model to ‚Äúhold more context‚Äù\n\nBut it can also lead to over-editing.\nRule: MAX mode is for design + sweeping edits, not for ‚Äúchange one line‚Äù.\n\n\n\n\n‚ÄúResults from the first prompt‚Äù\n\nYour first generated program will usually be:\n\nrunnable (if you gave good constraints)\nugly (variable names, structure)\nmissing polish (pause, restart, menus)\n\nThat‚Äôs normal.\nWhat matters is: we have something to iterate on.\n\n\n\n\n\npython -m venv .venv\n# mac/linux:\nsource .venv/bin/activate\n# windows:\n# .venv\\Scripts\\activate\n\npip install pygame\npython game.py\n\n\n\n\nWrong Python interpreter selected in the IDE\nMissing virtual environment activation\nPygame not installed in the active env\n\nTip: prove which python you are using:\npython -c \"import sys; print(sys.executable)\"\npython -c \"import pygame; print(pygame.__version__)\"\n\n\n\n\nVibe coding works best with small, reviewable changes.\nExamples of small changes:\n\nchange paddle speed constant\nadd a pause key\ncap frame rate\ntweak collision detection\n\nExamples of risky changes:\n\n‚Äúrewrite the whole game architecture‚Äù\n‚Äúswitch to a different framework‚Äù\n‚Äúadd multiplayer networking‚Äù\n\n\n\nChange request:\n\nIn game.py, reduce paddle speed by 25%.\nOnly change the speed constant(s).\nDo not refactor anything else.\nReturn the exact diff as a unified patch.\nThat kind of constraint reduces collateral damage.\n\n\n\n\nThis week shows that iteration isn‚Äôt random ‚Äî it‚Äôs structured prompting.\nA ‚Äúsecond prompt‚Äù is usually one of:\n\nbug fix\nfeature add\nrefactor for clarity\nadd tests / validation\n\nKey skill: describe the change in terms of inputs ‚Üí behavior ‚Üí output.\n\n\n\nWhen you rely on AI, you sometimes stop debugging. That‚Äôs dangerous.\nInstead:\n\nuse AI to accelerate debugging, not replace it\ndemand explanations\nadd prints / logs\nshrink the problem\n\n\n\nBad:\nFix my code\nGood:\nI'm getting this error when I run game.py:\n\nTraceback (most recent call last):\n  File \"game.py\", line 121, in &lt;module&gt;\n    main()\n  File \"game.py\", line 78, in main\n    ball.update()\nAttributeError: 'Ball' object has no attribute 'update'\n\nTask:\n1) Explain why this error happens\n2) Show the smallest fix\n3) Provide the corrected code for the Ball class only\n\n\n\n\n\nUse screenshots like this to talk about:\n\nframe rate (smooth motion)\ncollision boundaries\nscore placement\nreadability",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#objectives",
    "href": "notes/notes_10.html#objectives",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Week 10 moves from ‚Äúcoding with AI as a helper‚Äù to a conversational programming workflow where your editor + model become a co-author.\n\nHow Cursor differs from ‚Äújust ChatGPT in a browser‚Äù\nWhat vibe coding means in practice (and what it does not mean)\nHow to prompt for a whole working program and then iterate safely\nHow to use Cursor‚Äôs modes (chat, edit, autocomplete) deliberately\nHow to keep control: debugging, tests, and small edits instead of chaos",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#what-is-vibe-coding",
    "href": "notes/notes_10.html#what-is-vibe-coding",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "‚Äúvibe coding‚Äù is basically: you steer by intent and feedback, not by writing every line from scratch.\nYou start by describing behavior (what the program should do) rather than implementation (how it does it).\nThe AI proposes code.\nYou run it, observe, and then refine the prompt (or apply edits) until it behaves.\n\n\n‚ÄúThe feedback loop is the ‚Äúvibe‚Äù.‚Äù\n\n\n\n\nLLMs are strong at first drafts and pattern-matching.\nEditors like Cursor provide context (your whole repo), so the model can be more precise.\nYou can move faster when you accept that drafts are disposable.\n\n\n\n\n\n‚Äúno thinking‚Äù.\n‚Äútrust the model blindly‚Äù.\n‚Äúskip debugging‚Äù.\n\n\n\n\n\nOver-scoping: asking for ‚Äúbuild the whole app‚Äù without constraints.\nUnder-specifying: vague goals produce vague programs.\nContext leakage: if your project contains confusing files, the model can copy the wrong patterns.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#what-is-cursor-and-why-is-it-different",
    "href": "notes/notes_10.html#what-is-cursor-and-why-is-it-different",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Cursor is an IDE designed to make AI part of the editing loop. The big difference is context + actions:\n\nIt sees your open files, selections, and (optionally) the entire project.\nIt can apply edits directly (not just suggest them).\nIt supports modes that match real programming tasks.\n\nIn other words, it‚Äôs not ‚Äúchatting about code‚Äù ‚Äî it‚Äôs editing code with chat.\n\n\n\nYou can ask for changes in-place (e.g., ‚Äúrefactor this function‚Äù).\nYou can request multi-file modifications (e.g., ‚Äúadd a new module + tests‚Äù).\nYou can iterate with smaller deltas, which makes reviewing easier.\n\n\n\n\n\nCursor can make too many edits if your prompt is broad.\nAlways review diffs like you would a teammate PR.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#project-wide-context-and-customization",
    "href": "notes/notes_10.html#project-wide-context-and-customization",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Cursor becomes powerful when it understands your project:\n\nFolder structure\nDependencies\nNaming conventions\n‚ÄúHow we do things here‚Äù\n\nThis is where good software engineering meets AI.\n\n\nEven with Cursor, models still have limited context windows. So the art is:\n\nOnly include what matters\nSummarize what‚Äôs irrelevant\nAvoid dumping huge logs without focus\n\nIf the AI keeps making the same mistake, your context is missing something. Fix it by adding:\n\nconstraints (‚Äúdo not change public API‚Äù)\nexamples (‚Äúhere is one correct pattern in this repo‚Äù)\na test (‚Äúthis must pass‚Äù)",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#first-concept-build-something-small-then-improve-it",
    "href": "notes/notes_10.html#first-concept-build-something-small-then-improve-it",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "This is a great productivity pattern because:\n\nSmall apps expose real engineering problems quickly: I/O, state, loops, UI feedback\nStudents can run + test without needing deployment\nIteration is visible\n\n\n\nYou want early success:\n\na window opens\nsomething responds\nthe loop runs\n\nThen you polish.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#the-initial-prompt-to-build-our-game",
    "href": "notes/notes_10.html#the-initial-prompt-to-build-our-game",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "‚ÄúDon‚Äôt be vague!‚Äù\n\nA strong initial prompt contains:\n\ngoal (what game?)\nplatform (Python? Pygame? terminal?)\nconstraints (keep it simple, one file)\nexpected behavior (controls, win condition)\noutput format (code only, plus run instructions)\n\n\n\nYou are an expert Python developer.\n\nBuild a simple Pong-style game in Python using pygame.\n\nConstraints:\n- Single file: game.py\n- Keep it beginner friendly (clear functions, comments)\n- WASD controls for left paddle, arrow keys for right paddle\n- Score displayed at top\n- Press Q to quit\n\nOutput:\n- Provide the complete code for game.py\n- Then provide exact run steps (pip + python)\n\n\n\n\nIt creates a format contract: the model knows what to output.\nIt defines inputs (controls), outputs (score), and stop condition (quit).\nIt sets the difficulty level (beginner friendly).",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#cursor-basics-chat-vs-edit-vs-autocomplete",
    "href": "notes/notes_10.html#cursor-basics-chat-vs-edit-vs-autocomplete",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Cursor gives you multiple ways to use AI, and mixing them up causes pain.\n\n\nUse for:\n\ndesign discussion (‚Äúwhat approach should we take?‚Äù)\ndebugging (‚Äúhere‚Äôs the stack trace, what does it mean?‚Äù)\nplanning (‚Äúbreak this into steps‚Äù)\n\n\n\n\nUse for:\n\ntargeted rewrites (‚Äúsimplify this function‚Äù)\ntransformations (‚Äúconvert to dataclass‚Äù)\nlocal refactors (‚Äúrename variables consistently‚Äù)\n\n\n\n\nUse for:\n\nwriting boilerplate quickly\nfinishing predictable patterns\nstaying in flow\n\nTeaching tip: chat is best when you don‚Äôt know what to do, edit is best when you know what you want changed.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#giving-feedback-how-to-steer-the-model",
    "href": "notes/notes_10.html#giving-feedback-how-to-steer-the-model",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Feedback is the steering wheel of vibe coding.\nGood feedback is:\n\nspecific (‚Äúthe paddle is moving too fast‚Äù)\ntestable (‚Äúball should bounce at the same angle‚Äù)\nconstrained (‚Äúonly change movement speed constants‚Äù)\n\nBad feedback is:\n\nemotional (‚Äúthis is broken‚Äù)\nvague (‚Äúmake it better‚Äù)\n\n\n\n\nWhat did I expect?\nWhat happened instead?\nWhere in the code does that behavior live?\nWhat is the smallest change that could fix it?",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#adding-context-what-to-paste-into-chat",
    "href": "notes/notes_10.html#adding-context-what-to-paste-into-chat",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "The temptation is to paste everything. Don‚Äôt.\nPaste only:\n\nthe failing function\nthe error message\nthe minimal reproduction steps\n\nExample:\nBug report:\n- When I press W, the paddle moves diagonally.\n\nRepro:\n1. Run game.py\n2. Press W for 2 seconds\n\nExpected:\n- Paddle moves up only.\n\nActual:\n- Paddle moves up + right.\n\nHere is the movement code (lines 41-68):\n[paste snippet]\nThis gives the model a ‚Äúdebug ticket‚Äù it can solve.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#selecting-a-mode-decision-table",
    "href": "notes/notes_10.html#selecting-a-mode-decision-table",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Task\nBest Cursor mode\nWhy\n\n\n\n\n‚ÄúExplain this error‚Äù\nChat\nreasoning + teaching\n\n\n‚ÄúRewrite this function‚Äù\nEdit\ndirect transformation\n\n\n‚ÄúAdd docstrings‚Äù\nEdit\nrepetitive change\n\n\n‚ÄúFinish this for-loop‚Äù\nAutocomplete\npredictable\n\n\n‚ÄúDesign a module layout‚Äù\nChat\nbrainstorming\n\n\n\nNB:  Cursor is constantly being updated, the modes may have slightly different names or capabilities.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#model-selection-speed-vs-quality",
    "href": "notes/notes_10.html#model-selection-speed-vs-quality",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Different models behave differently:\n\nsome are faster, but more shallow\nsome reason better, but slower\nsome follow formatting better\n\nYou should practice:\n\nuse the ‚Äúsmart‚Äù model for big architectural tasks\nuse the ‚Äúfast‚Äù model for repetitive refactors",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#max-mode-when-you-need-the-heavy-hammer",
    "href": "notes/notes_10.html#max-mode-when-you-need-the-heavy-hammer",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "MAX mode is useful when:\n\nthe codebase is large\nthe change is cross-cutting\nyou need the model to ‚Äúhold more context‚Äù\n\nBut it can also lead to over-editing.\nRule: MAX mode is for design + sweeping edits, not for ‚Äúchange one line‚Äù.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#results-from-the-first-prompt-expect-rough-edges",
    "href": "notes/notes_10.html#results-from-the-first-prompt-expect-rough-edges",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "‚ÄúResults from the first prompt‚Äù\n\nYour first generated program will usually be:\n\nrunnable (if you gave good constraints)\nugly (variable names, structure)\nmissing polish (pause, restart, menus)\n\nThat‚Äôs normal.\nWhat matters is: we have something to iterate on.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#running-our-game-for-the-first-time-environment--dependencies",
    "href": "notes/notes_10.html#running-our-game-for-the-first-time-environment--dependencies",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "python -m venv .venv\n# mac/linux:\nsource .venv/bin/activate\n# windows:\n# .venv\\Scripts\\activate\n\npip install pygame\npython game.py\n\n\n\n\nWrong Python interpreter selected in the IDE\nMissing virtual environment activation\nPygame not installed in the active env\n\nTip: prove which python you are using:\npython -c \"import sys; print(sys.executable)\"\npython -c \"import pygame; print(pygame.__version__)\"",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#making-changes-to-our-game-small-diffs-win",
    "href": "notes/notes_10.html#making-changes-to-our-game-small-diffs-win",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Vibe coding works best with small, reviewable changes.\nExamples of small changes:\n\nchange paddle speed constant\nadd a pause key\ncap frame rate\ntweak collision detection\n\nExamples of risky changes:\n\n‚Äúrewrite the whole game architecture‚Äù\n‚Äúswitch to a different framework‚Äù\n‚Äúadd multiplayer networking‚Äù\n\n\n\nChange request:\n\nIn game.py, reduce paddle speed by 25%.\nOnly change the speed constant(s).\nDo not refactor anything else.\nReturn the exact diff as a unified patch.\nThat kind of constraint reduces collateral damage.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#the-second-prompt-iteration-with-structure",
    "href": "notes/notes_10.html#the-second-prompt-iteration-with-structure",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "This week shows that iteration isn‚Äôt random ‚Äî it‚Äôs structured prompting.\nA ‚Äúsecond prompt‚Äù is usually one of:\n\nbug fix\nfeature add\nrefactor for clarity\nadd tests / validation\n\nKey skill: describe the change in terms of inputs ‚Üí behavior ‚Üí output.",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#debugging-in-a-vibe-coding-workflow",
    "href": "notes/notes_10.html#debugging-in-a-vibe-coding-workflow",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "When you rely on AI, you sometimes stop debugging. That‚Äôs dangerous.\nInstead:\n\nuse AI to accelerate debugging, not replace it\ndemand explanations\nadd prints / logs\nshrink the problem\n\n\n\nBad:\nFix my code\nGood:\nI'm getting this error when I run game.py:\n\nTraceback (most recent call last):\n  File \"game.py\", line 121, in &lt;module&gt;\n    main()\n  File \"game.py\", line 78, in main\n    ball.update()\nAttributeError: 'Ball' object has no attribute 'update'\n\nTask:\n1) Explain why this error happens\n2) Show the smallest fix\n3) Provide the corrected code for the Ball class only",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "notes/notes_10.html#figure-example-diff",
    "href": "notes/notes_10.html#figure-example-diff",
    "title": "Week 10 ‚Äî Vibe coding with Cursor",
    "section": "",
    "text": "Use screenshots like this to talk about:\n\nframe rate (smooth motion)\ncollision boundaries\nscore placement\nreadability",
    "crumbs": [
      "Notes",
      "Notes 10 üìï"
    ]
  },
  {
    "objectID": "module-syllabus.html",
    "href": "module-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dr.¬†Ed Harris, Module Leader, Data Scientist, Statistician, and coffee enthusiast. NB Ed is away for the beginning of the Spring semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#your-instructors",
    "href": "module-syllabus.html#your-instructors",
    "title": "Syllabus",
    "section": "",
    "text": "Dr.¬†Ed Harris, Module Leader, Data Scientist, Statistician, and coffee enthusiast. NB Ed is away for the beginning of the Spring semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#learning-objectives",
    "href": "module-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\n1.¬†¬†¬† Develop and apply complex workflows using chain calls to LLMs, demonstrating an understanding of automated systems.\n2.¬†¬†¬† Construct and analyse systems where Python code interacts with both completions and new prompts, showcasing programming proficiency.\n3.¬†¬†¬† Design a customer service chatbot using learned techniques, reflecting an ability to integrate various aspects of data science technology.\n4.¬†¬†¬† Evaluate and apply prompt engineering skills in practical scenarios, including chat agent response systems and safety evaluations.\n5.¬†¬†¬† Synthesise and apply knowledge of vector databases in building applications like retrieval augmented generation (RAG) and multilingual search systems.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#meetings",
    "href": "module-syllabus.html#meetings",
    "title": "Syllabus",
    "section": "Meetings",
    "text": "Meetings\nMeetings will be held in person in Telford, Station Quarter. Spring 2025: Check your personal schedule for room, day, time.\nNotes and brief lectures will introduce the weekly topic and sometimes involve live coding demonstration of key concepts. Lectures and notes should be reviewed prior to attending each weekly session.\nFlipped classroom will be used exclusively for this module. This reverses traditional teaching by having students go through new material independently, through videos or readings, before class meetings. Then, class time is used for discussion, problem-solving, assessments, and applying ideas. This lets students work through content at their own pace and ensures that support from teachers and peers is available when tackling challenging concepts. The idea is to make learning more active, collaborative, and confidence-building. Also, it is usually more fun than traditional lectures!\nMaterial may be livestreamed or otherwise recorded for later viewing.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#online-resources",
    "href": "module-syllabus.html#online-resources",
    "title": "Syllabus",
    "section": "Online resources",
    "text": "Online resources\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the module website. This material is completely open and accessible to all without login or other barriers.\nHarper Adams module website (university enrolled students only, requires login)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#assessments",
    "href": "module-syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nAssessment 1\nProject The Real World AI module culminates in a unique summative assessment, consisting of a group project that is both designed and executed by the students supported with supervised class work and discussion, and student support and technical staff. This project is split into two main components: a pitch presentation and the final project creation. In the pitch presentation, reminiscent of a ‚ÄòDragon‚Äôs Den‚Äô scenario, groups of students are expected to conceive and present an innovative application of large language models (LLMs) and APIs. This presentation should detail the objectives, technology use (such as vector databases and OpenAI API), methodology, and potential impact of the proposed project. Following the pitch, students collaborate to turn their concept into a reality. This phase involves prompt engineering, crafting efficient data processing systems, and ultimately producing a functional prototype or application that aligns with their initial pitch. The project is assessed on its functionality, innovation, effective use of data science technologies, and overall execution.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#readings",
    "href": "module-syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\nMorgan, J. C. 2025. Coding with AI. 1st edition. Manning Publications. Shelter Island, NY.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#five-tips-for-success",
    "href": "module-syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this module depends very much on you and the effort you put into it. Like any learning, the burden of engaging with the material is on you. The module staff and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the course staff, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, let‚Äôs talk about it. If you hear something on the news that sounds related to what we discussed, share it and let‚Äôs discuss. If the reading is confusing, ask.\nDo some of the optional readings, read and share relevant Twitter/X posts, read and share relevant Hacker News articles. Consider joining or forming a reading group that meets once per week to discuss.\nDo the homework and the tutorials. The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon‚Äôt procrastinate. If something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, and eventually you won‚Äôt know where to begin asking questions. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, ask for help, and let us help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-syllabus.html#module-policies",
    "href": "module-syllabus.html#module-policies",
    "title": "Syllabus",
    "section": "Module policies",
    "text": "Module policies\nThe essence of all work that you submit to this course must be your own. Unless otherwise specified, collaboration on assessments (e.g., assignments, labs, problem sets, projects, quizzes, or tests) is not permitted except to the extent that you may ask classmates and others for help so long as that help does not reduce to another doing your work for you. Generally speaking, when asking for help, you may show your work to others, but you may not view theirs, so long as you and they respect this policy‚Äôs other constraints.\nReasonable\n\nCommunicating with classmates about assessments in English (or some other spoken language), and properly citing those discussions.\nDiscussing the course‚Äôs material with others in order to understand it better.\nHelping a classmate identify a bug in their code, as by viewing, compiling, or running their code after you have submitted that portion of the pset yourself.\nIncorporating a few lines of code that you find online or elsewhere into your own code, provided that those lines are not themselves solutions to assigned work and that you cite the lines‚Äô origins.\nSending or showing code that you‚Äôve written to someone, possibly a classmate, so that they might help you identify and fix a bug.\nSubmitting the same or similar work to this course that you have submitted previously to this course.\nTurning to the web or elsewhere for instruction beyond the course‚Äôs own, for references, and for solutions to technical difficulties, but not for outright solutions to assigned work.\nUsing AI-based software to ask questions, but not presenting its answers as your own.\nWhiteboarding solutions with others using diagrams or pseudocode but not actual code.\nWorking with (and even paying) a tutor to help you with the course, provided the tutor does not do your work for you.\n\nNot Reasonable\n\nAccessing a solution to some assessement prior to (re-)submitting your own.\nAccessing or attempting to access, without permission, an account not your own.\nAsking a classmate to see their solution to some assessment before submitting your own.\nFailing to cite (as with comments) the origins of code or techniques that you discover outside of the course‚Äôs own lessons and integrate into your own work, even while respecting this policy‚Äôs other constraints.\nGiving or showing to a classmate a solution to an assessment when it is they, and not you, who is struggling to solve it.\nPaying or offering to pay an individual for work that you may submit as (part of) your own.\nProviding or making available solutions to assessments to anyone, whether a past, present, or prospective future student.\nSearching for or soliciting outright solutions to assessments online or elsewhere.\nSplitting an assessment‚Äôs workload with another individual and combining your work.\nSubmitting (after possibly modifying) the work of another individual beyond the few lines allowed herein.\nSubmitting the same or similar work to this course that you have submitted or will submit to another course, unless explictly allowed.\nUsing AI-based software (including ChatGPT, GitHub Copilot, the new Bing, et al.) that suggests answers or lines of code.\nViewing another‚Äôs solution to an assessment and basing your own solution on it.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "module-project.html",
    "href": "module-project.html",
    "title": "Module Project",
    "section": "",
    "text": "Info coming‚Ä¶",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "module-project.html#final-project",
    "href": "module-project.html#final-project",
    "title": "Module Project",
    "section": "",
    "text": "Info coming‚Ä¶",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "module-project.html#getting-started",
    "href": "module-project.html#getting-started",
    "title": "Module Project",
    "section": "Getting Started",
    "text": "Getting Started",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "module-project.html#how-to-submit",
    "href": "module-project.html#how-to-submit",
    "title": "Module Project",
    "section": "How to Submit",
    "text": "How to Submit",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "module-faq.html",
    "href": "module-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Yes, of course you can and should! But‚Ä¶\nThe point of the flipped classroom is to maximise the time spent in class discussing and solving problems. If you don‚Äôt watch the videos and go through the notes for that week before class, you will miss out on this and potentially fall behind while others are getting on with their problem set work during classtime.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "module-faq.html#flipped-classroom-can-i-still-come-to-lecture-if-i-dont-watch-the-videos-and-go-through-the-notes-for-that-week-before-class",
    "href": "module-faq.html#flipped-classroom-can-i-still-come-to-lecture-if-i-dont-watch-the-videos-and-go-through-the-notes-for-that-week-before-class",
    "title": "FAQ",
    "section": "",
    "text": "Yes, of course you can and should! But‚Ä¶\nThe point of the flipped classroom is to maximise the time spent in class discussing and solving problems. If you don‚Äôt watch the videos and go through the notes for that week before class, you will miss out on this and potentially fall behind while others are getting on with their problem set work during classtime.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "module-faq.html#can-i-just-code-on-my-own-computer",
    "href": "module-faq.html#can-i-just-code-on-my-own-computer",
    "title": "FAQ",
    "section": "Can I just code on my own computer?",
    "text": "Can I just code on my own computer?\nThe short answer is, heck yeah!!, with caveats.\nInstalling and configuring programming and editing software on your own computer is sometimes complicated and irritating, even for very experienced people. Configuring all the tools we will be using in The Sandbox to be exactly the same on your own computer, operating system, etc., will require advanced skills. Still, you can attempt to do it if you want to; most of the tools are open source and free and will work for most tasks. Also, we are here to help support you if you want to try.",
    "crumbs": [
      "FAQ"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Programming Concepts",
    "section": "",
    "text": "Code must be written for people to read, and, only incidentally, for machines to execute.\n\nWelcome!\nThis module offers an engaging introduction and an overview to programming utilising Python. Designed to accommodate both novices and those with some programming background, it covers a spectrum of fundamental concepts of coding. Key topics include reading and writing code, testing, debugging with Python-specific features. This module provides essential underpinning skills for other computer science modules. The content will be highly relevant and adaptable, enhancing employability and professional development in technology-driven industries.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "module-extras.html",
    "href": "module-extras.html",
    "title": "Extra Meetings",
    "section": "",
    "text": "HADC! is a group of people (students, researchers, and staff) who are interested in statistics and data science. We typically meet regularly to discuss practical statistics and data science topics. Check out the HADC! website for the schedule."
  },
  {
    "objectID": "module-extras.html#harper-adams-data-club-hadc",
    "href": "module-extras.html#harper-adams-data-club-hadc",
    "title": "Extra Meetings",
    "section": "",
    "text": "HADC! is a group of people (students, researchers, and staff) who are interested in statistics and data science. We typically meet regularly to discuss practical statistics and data science topics. Check out the HADC! website for the schedule."
  },
  {
    "objectID": "module-extras.html#code-club",
    "href": "module-extras.html#code-club",
    "title": "Extra Meetings",
    "section": "Code Club",
    "text": "Code Club\nThe Code Club is an outreach-based initiative that aims to help school students learn programming and data science. Our first Code Club will be held regularly (keep your eyes open for announcements). This is an opportunity for you to help mentor younger students - it is fun, rewarding, and a great way to build your CV."
  },
  {
    "objectID": "module-links.html",
    "href": "module-links.html",
    "title": "Useful links",
    "section": "",
    "text": "üîó Reference manual for Python - Probably very useful‚Ä¶\nüîó üé§ HADC! & Code Club meetings\nüîó üè¢ Harper Adams VLE site for this module (HAU enrolled students only)",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "module-schedule.html",
    "href": "module-schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "üîó ‚ÄúThe Sandbox‚Äù coding environment (Github Codespace)\nThis schedule is an outline of the topics, with links to all content, and assignments. Note that this may be updated as the module progresses.\n\n\n\nWeek (HAU)\nTopics\nReadings\nNotes\n\n\n\n\n00 (16)\nü§ñ Gen AI Intro\nüìí\nüìï\n\n\n01 (17)\nü§ñ AI-assisted Code\nüìí\nüìï\n\n\n02 (18)\nü§ñ Design\nüìí\nüìï\n\n\n03 (19)\nü§ñ Code an App\nüìí\nüìï\n\n\n04 (20)\nü§ñ Base Code Gen\nüìí\nüìï\n\n\n05 (21)\nü§ñ Backend\nüìí\nüìï\n\n\n06 (22)\nü§ñ UI\nüìí\nüìï\n\n\n\nüí• Mid term Break\n\n\n\n\n07 (23)\nü§ñ Software Testing\nüìí\nüìï\n\n\n08 (24)\nü§ñ Prompt Engineering\nüìí\nüìï\n\n\n09 (25)\nü§ñ Vibe Coding\nüìí\nüìï\n\n\n10 (26)\nüëã Wrap up, Open lab\n\n\n\n\n11 (27)\nüöÄ Open lab\n\n\n\n\n12 (28)\nüöÄ Open lab",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "notes/notes_1.html",
    "href": "notes/notes_1.html",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "‚Äú‚Ä¶use your existing Python expertise alongside AI that understands your code context‚Ä¶‚Äù\n\n\nOverview\nWhy this matters for ‚ÄúReal World AI‚Äù\nGenerative AI for coders\nDeveloper tools landscape\nHow does generative AI work?\nWhat an LLM is (and isn‚Äôt)\nThe potential of LLMs\nGenerative AI vs code completion\nOther types of generative AI (quick map)\nProject workflow with AI assistance\nChoosing the right generative AI tools\nCommon failure modes (what students must learn)\nGo forth and code!\nA simple ‚ÄúAI as refactor assistant‚Äù workflow\nTurning a prompt into a structured specification\nA minimal test-first habit\nSumming Up\n\n\n\n\n‚ÄúSupercharge your coding with AI‚Äù ‚ÄúNo AI expertise is required.‚Äù -Some AI Bro\n\nIn this week we introduce generative AI as a practical toolset for real software development. The goal is not to turn you into an ML creator. The goal is to help you build code solutions, with less busywork, and fewer defects.\n\n\n\nA working definition of generative AI in the context of programming work.\nA map of the developer tool landscape (general chat models vs IDE assistants vs specialist tools).\nA mental model for how LLMs work at a useful, developer-friendly level.\nA realistic view of benefits and failure modes (hallucinations, shallow tests, brittle outputs).\n\n\n\n\n\nThe week frames AI coding tools as an ‚Äúextra pair of hands‚Äù for experienced developers. The promise is not magic ‚Äî it‚Äôs leverage:\n\nfaster scaffolding\nfaster iteration\nearlier bug discovery\nimproved test coverage\n\n\n‚Äú‚Ä¶reduced implementation time by approximately 30%, while improving code quality‚Ä¶‚Äù\n\nThe course lens: real world means we care about:\n\npractical constraints\nmaintainability\nsecurity\ncorrectness\nteam workflows\n\n\n\n\n\n\nGenerative AI refers to models that produce new content (text, code, images) based on learned patterns. For coding, the relevant output types are:\n\ncode snippets (functions, classes, scripts)\nrefactors (changing structure without changing behaviour)\ntests (unit tests, fuzz tests, property-based tests)\ndocumentation (docstrings, READMEs, API docs)\n\n\n\n\nThis week highlights several practical use cases. We‚Äôll treat these as work modes you can switch between.\n\n\nThis is the ‚Äúpair programmer‚Äù mode. You describe what you want, and the tool produces starter code.\nKey reality check:\n\ngenerated code is rarely perfect\nit is often plausible\nyou must still review and test\n\n\n‚Äú‚Ä¶anticipates patterns, and generates implementation details‚Ä¶‚Äù\n\n\n\n\nTools can propose patches quickly ‚Äî especially for:\n\ncommon exceptions\nmisuse of APIs\noff-by-one errors\nmissing edge-case handling\n\nBut: AI doesn‚Äôt understand your logic. It matches patterns. If your bug is domain-specific, AI help may be weaker.\n\n\n\nAI is good at turning code into:\n\ndocstrings\nusage examples\n‚Äúhow it works‚Äù summaries\n\nThe risk is confident incorrectness. Docs can become wrong faster than code. So you should treat AI-generated docs as a draft, not truth.\n\n\n\nAI can help with:\n\nextracting helper functions\nrenaming variables for clarity\nreorganising modules\nperformance suggestions\n\nBut optimisation suggestions can be dangerous if they change semantics. Always lock behaviour first with tests.\n\n\n\nAI is useful for producing lots of tests quickly. This is a big deal because tests are often the bottleneck.\nHowever:\n\nAI tests can be shallow\nAI may test implementation details instead of behaviour\n\nSo we‚Äôll keep a rule of thumb:\n\nTest behaviour, not the code‚Äôs current shape.\n\n\n\n\n\n\nThis week separates tools into categories. You should be able to explain what each category is good for.\n\n\nThese include ChatGPT / Claude-style assistants. They are good for:\n\nexplaining concepts\ndrafting code in isolation\nbrainstorming designs\ngenerating documentation\n\nBut they can struggle with:\n\nlarge codebases\nproject-wide refactors\nkeeping context consistent\n\n\n\n\nThese integrate into editors (VS Code, JetBrains, etc.). They can:\n\nautocomplete in-place\nunderstand surrounding code context\nsuggest local refactors\n\n\n\n\nSome tools focus on a specific workflow:\n\ncode search + fixes\nsecurity checks\ntest generation\n\nThis week calls out tools like Cursor, Blackbox AI, and Tabnine later. Your skill as a developer is choosing the right tool for the task.\n\n‚Äú‚Ä¶practical guide to using AI tools to supercharge your coding‚Ä¶‚Äù\n\n\n\n\n\nThis is the part where people often get intimidated. This week‚Äôs key message is:\n\nyou don‚Äôt need the math\nyou do need the mental model\n\nWe‚Äôll focus on what helps you predict failure modes.\n\n\nThink of a large language model (LLM) as:\n\na probability engine over sequences of tokens\ntrained to predict ‚Äúwhat comes next‚Äù\n\nIt does not ‚Äúknow‚Äù facts the way humans do. It generates outputs that are statistically likely given its training.\n\n\n\nThis week describes internal steps like:\n\ninput embedding (tokens ‚Üí vectors)\ntransformer layers (attention)\noutput decoding (vectors ‚Üí tokens)\n\nWe‚Äôll capture this with a diagram.\n\n\n\n\n\nflowchart LR\n  A[Input prompt] --&gt; B[Tokenisation]\n  B --&gt; C[Embeddings]\n  C --&gt; D[Transformer layers&lt;br/&gt;attention + feedforward]\n  D --&gt; E[Next-token probabilities]\n  E --&gt; F[Sampling / decoding]\n  F --&gt; G[Output text/code]\n\n\n\n\n\n\n\n‚ÄúAn LLM is a deep learning architecture based on the Transformer model‚Ä¶‚Äù\n\n\n\n\nAttention is what allows the model to:\n\n‚Äúfocus‚Äù on relevant parts of the prompt\nrelate tokens across distance\nkeep patterns consistent (sometimes!)\n\nWhen you see a model forget instructions, it‚Äôs often because:\n\nthe prompt was too long\nthe instruction got buried\nthe model weighted other tokens as more relevant\n\n\n\n\n\n\n\nAn LLM is a model trained on huge amounts of text and code. It learns patterns like:\n\nhow functions are structured\nhow docs are written\nhow bugs are commonly fixed\nhow tests are typically expressed\n\n\n\n\nIt is not:\n\na compiler\na proof system\na deterministic system\na guaranteed factual source\n\nThis week hints at the most important practical issue:\n\noutputs can be fluent and wrong\n\n\n‚Äú‚Ä¶determines whether ‚Äòthis is most likely to be correct and functional source code‚Äô‚Äù\n\n\n\n\n\nThis is where this week is optimistic. The potential is that AI tools can shift your role upward:\n\nfrom typing code ‚Üí designing systems\nfrom debugging syntax ‚Üí validating behaviour\nfrom writing boilerplate ‚Üí reviewing structure\n\n\n\nIt usually means:\n\nfaster first drafts\nearlier working prototypes\nmore time spent on architecture\n\nBut only if you adopt a workflow that includes:\n\nreview\ntests\niteration\n\n\n\n\n\nMany students confuse these. This week draws a distinction:\n\n\n\nrule-based or simple statistical\nlocal suggestions\nlimited context\n\n\n\n\n\ncan produce novel combinations\ncan respond to higher-level instructions\ncan create multi-file scaffolds\n\n\n‚ÄúUnlike traditional code completion‚Ä¶ generative AI creates‚Ä¶‚Äù\n\nThe key point:\nGenerative AI is a collaborator. Code completion is a convenience feature.\n\n\n\n\nEven though this week is coding-focused, generative AI includes:\n\nimage generation\naudio generation\nvideo generation\n\nWhy you should care:\n\nreal products combine modalities\ndocumentation and UI work often needs imagery\n\n\n\n\nThis is the most ‚Äúreal world‚Äù section. It breaks software development into stages where AI can help.\n\n\nWhere AI helps:\n\nbrainstorming features\ngenerating user stories\ndrafting requirements\n\nRisk:\n\n‚Äúfeature creep‚Äù (AI happily invents scope)\n\n\n\n\nWhere AI helps:\n\nscaffolding projects\ndrafting modules\nwriting repetitive code\n\nRisk:\n\ninconsistent design decisions\nhidden complexity\n\n\n\n\nWhere AI helps:\n\nspotting style issues\nsuggesting simplifications\nidentifying missing error handling\n\nRisk:\n\nfalse confidence (the model sounds sure)\n\n\n\n\nWhere AI helps:\n\ngenerating test cases\nsuggesting likely failure points\nexplaining stack traces\n\nRisk:\n\nshallow tests that don‚Äôt reflect real requirements\n\n\n\n\nWhere AI helps:\n\nexplaining APIs\nimproving README clarity\ngenerating examples\n\nRisk:\n\ndocs drift from reality\n\n\n‚ÄúThis cycle creates a powerful symbiotic relationship‚Ä¶‚Äù\n\n\n\n\n\nThis week emphasises selection criteria. In real teams, you choose tools based on:\n\nintegration (does it fit your IDE/workflow?)\ncontext (does it see your project?)\nprivacy (can you send code externally?)\nquality (accuracy + helpfulness)\n\n\n\n\nDoes it understand multi-file projects?\nDoes it support test generation?\nDoes it help you refactor safely?\nDoes it provide citations or traceability?\n\n\n\n\n\nThis week implies several failure modes. We make them explicit because they will recur all term.\n\n\nThe model invents:\n\nnonexistent functions\nwrong APIs\nfake ‚Äúfacts‚Äù\n\nMitigation:\n\nverify against docs\nrun the code\nwrite tests\n\n\n\n\nSometimes your prompt ‚Äúaccidentally‚Äù forces a weird design. Mitigation:\n\nask for alternatives\nrequest tradeoffs\niterate\n\n\n\n\nAI-generated tests often:\n\nonly test ‚Äúhappy path‚Äù\nmirror implementation details\n\nMitigation:\n\nuse boundary cases\ntest behaviour\nadd negative tests\n\n\n\n\n\nThis final section is motivational. This week‚Äôs message is:\n\ndon‚Äôt fear the tools\ntreat them as assistants\nkeep your engineering discipline\n\n\n\n\n# A tiny refactor candidate\n\ndef total_cost(prices, tax_rate):\n    # NOTE: intentionally minimal\n    return sum(prices) * (1 + tax_rate)\n\nprint(total_cost([10, 20, 30], 0.2))\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass FeatureSpec:\n    name: str\n    user_story: str\n    acceptance_criteria: list[str]\n\nspec = FeatureSpec(\n    name=\"Export report as CSV\",\n    user_story=\"As a user, I want to export my report so I can analyse it in Excel.\",\n    acceptance_criteria=[\n        \"Export button downloads a .csv file\",\n        \"CSV includes headers\",\n        \"Handles empty datasets gracefully\",\n    ],\n)\n\nprint(spec)\n\n\n\n# Simple behaviour test (no frameworks yet)\n\ndef add(a, b):\n    return a + b\n\nassert add(2, 3) == 5\nassert add(-1, 1) == 0\nprint(\"All tests passed\")\n\n\n\n\nGenerative AI can improve coding productivity, but it must be combined with testing and review.\nThe developer tool landscape includes chat models, IDE assistants, and specialised coding tools.\nLLMs are probabilistic systems (fluent but not guaranteed correct).\nReal-world workflows use AI across ideation, implementation, review, testing, and documentation.",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#overview",
    "href": "notes/notes_1.html#overview",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "‚ÄúSupercharge your coding with AI‚Äù ‚ÄúNo AI expertise is required.‚Äù -Some AI Bro\n\nIn this week we introduce generative AI as a practical toolset for real software development. The goal is not to turn you into an ML creator. The goal is to help you build code solutions, with less busywork, and fewer defects.\n\n\n\nA working definition of generative AI in the context of programming work.\nA map of the developer tool landscape (general chat models vs IDE assistants vs specialist tools).\nA mental model for how LLMs work at a useful, developer-friendly level.\nA realistic view of benefits and failure modes (hallucinations, shallow tests, brittle outputs).",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#why-this-matters-for-real-world-ai",
    "href": "notes/notes_1.html#why-this-matters-for-real-world-ai",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "The week frames AI coding tools as an ‚Äúextra pair of hands‚Äù for experienced developers. The promise is not magic ‚Äî it‚Äôs leverage:\n\nfaster scaffolding\nfaster iteration\nearlier bug discovery\nimproved test coverage\n\n\n‚Äú‚Ä¶reduced implementation time by approximately 30%, while improving code quality‚Ä¶‚Äù\n\nThe course lens: real world means we care about:\n\npractical constraints\nmaintainability\nsecurity\ncorrectness\nteam workflows",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#generative-ai-for-coders",
    "href": "notes/notes_1.html#generative-ai-for-coders",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "Generative AI refers to models that produce new content (text, code, images) based on learned patterns. For coding, the relevant output types are:\n\ncode snippets (functions, classes, scripts)\nrefactors (changing structure without changing behaviour)\ntests (unit tests, fuzz tests, property-based tests)\ndocumentation (docstrings, READMEs, API docs)\n\n\n\n\nThis week highlights several practical use cases. We‚Äôll treat these as work modes you can switch between.\n\n\nThis is the ‚Äúpair programmer‚Äù mode. You describe what you want, and the tool produces starter code.\nKey reality check:\n\ngenerated code is rarely perfect\nit is often plausible\nyou must still review and test\n\n\n‚Äú‚Ä¶anticipates patterns, and generates implementation details‚Ä¶‚Äù\n\n\n\n\nTools can propose patches quickly ‚Äî especially for:\n\ncommon exceptions\nmisuse of APIs\noff-by-one errors\nmissing edge-case handling\n\nBut: AI doesn‚Äôt understand your logic. It matches patterns. If your bug is domain-specific, AI help may be weaker.\n\n\n\nAI is good at turning code into:\n\ndocstrings\nusage examples\n‚Äúhow it works‚Äù summaries\n\nThe risk is confident incorrectness. Docs can become wrong faster than code. So you should treat AI-generated docs as a draft, not truth.\n\n\n\nAI can help with:\n\nextracting helper functions\nrenaming variables for clarity\nreorganising modules\nperformance suggestions\n\nBut optimisation suggestions can be dangerous if they change semantics. Always lock behaviour first with tests.\n\n\n\nAI is useful for producing lots of tests quickly. This is a big deal because tests are often the bottleneck.\nHowever:\n\nAI tests can be shallow\nAI may test implementation details instead of behaviour\n\nSo we‚Äôll keep a rule of thumb:\n\nTest behaviour, not the code‚Äôs current shape.",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#developer-tools-landscape",
    "href": "notes/notes_1.html#developer-tools-landscape",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This week separates tools into categories. You should be able to explain what each category is good for.\n\n\nThese include ChatGPT / Claude-style assistants. They are good for:\n\nexplaining concepts\ndrafting code in isolation\nbrainstorming designs\ngenerating documentation\n\nBut they can struggle with:\n\nlarge codebases\nproject-wide refactors\nkeeping context consistent\n\n\n\n\nThese integrate into editors (VS Code, JetBrains, etc.). They can:\n\nautocomplete in-place\nunderstand surrounding code context\nsuggest local refactors\n\n\n\n\nSome tools focus on a specific workflow:\n\ncode search + fixes\nsecurity checks\ntest generation\n\nThis week calls out tools like Cursor, Blackbox AI, and Tabnine later. Your skill as a developer is choosing the right tool for the task.\n\n‚Äú‚Ä¶practical guide to using AI tools to supercharge your coding‚Ä¶‚Äù",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#how-does-generative-ai-work",
    "href": "notes/notes_1.html#how-does-generative-ai-work",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This is the part where people often get intimidated. This week‚Äôs key message is:\n\nyou don‚Äôt need the math\nyou do need the mental model\n\nWe‚Äôll focus on what helps you predict failure modes.\n\n\nThink of a large language model (LLM) as:\n\na probability engine over sequences of tokens\ntrained to predict ‚Äúwhat comes next‚Äù\n\nIt does not ‚Äúknow‚Äù facts the way humans do. It generates outputs that are statistically likely given its training.\n\n\n\nThis week describes internal steps like:\n\ninput embedding (tokens ‚Üí vectors)\ntransformer layers (attention)\noutput decoding (vectors ‚Üí tokens)\n\nWe‚Äôll capture this with a diagram.\n\n\n\n\n\nflowchart LR\n  A[Input prompt] --&gt; B[Tokenisation]\n  B --&gt; C[Embeddings]\n  C --&gt; D[Transformer layers&lt;br/&gt;attention + feedforward]\n  D --&gt; E[Next-token probabilities]\n  E --&gt; F[Sampling / decoding]\n  F --&gt; G[Output text/code]\n\n\n\n\n\n\n\n‚ÄúAn LLM is a deep learning architecture based on the Transformer model‚Ä¶‚Äù\n\n\n\n\nAttention is what allows the model to:\n\n‚Äúfocus‚Äù on relevant parts of the prompt\nrelate tokens across distance\nkeep patterns consistent (sometimes!)\n\nWhen you see a model forget instructions, it‚Äôs often because:\n\nthe prompt was too long\nthe instruction got buried\nthe model weighted other tokens as more relevant",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#what-an-llm-is-and-isnt",
    "href": "notes/notes_1.html#what-an-llm-is-and-isnt",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "An LLM is a model trained on huge amounts of text and code. It learns patterns like:\n\nhow functions are structured\nhow docs are written\nhow bugs are commonly fixed\nhow tests are typically expressed\n\n\n\n\nIt is not:\n\na compiler\na proof system\na deterministic system\na guaranteed factual source\n\nThis week hints at the most important practical issue:\n\noutputs can be fluent and wrong\n\n\n‚Äú‚Ä¶determines whether ‚Äòthis is most likely to be correct and functional source code‚Äô‚Äù",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#the-potential-of-llms",
    "href": "notes/notes_1.html#the-potential-of-llms",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This is where this week is optimistic. The potential is that AI tools can shift your role upward:\n\nfrom typing code ‚Üí designing systems\nfrom debugging syntax ‚Üí validating behaviour\nfrom writing boilerplate ‚Üí reviewing structure\n\n\n\nIt usually means:\n\nfaster first drafts\nearlier working prototypes\nmore time spent on architecture\n\nBut only if you adopt a workflow that includes:\n\nreview\ntests\niteration",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#generative-ai-vs-code-completion",
    "href": "notes/notes_1.html#generative-ai-vs-code-completion",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "Many students confuse these. This week draws a distinction:\n\n\n\nrule-based or simple statistical\nlocal suggestions\nlimited context\n\n\n\n\n\ncan produce novel combinations\ncan respond to higher-level instructions\ncan create multi-file scaffolds\n\n\n‚ÄúUnlike traditional code completion‚Ä¶ generative AI creates‚Ä¶‚Äù\n\nThe key point:\nGenerative AI is a collaborator. Code completion is a convenience feature.",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#other-types-of-generative-ai-quick-map",
    "href": "notes/notes_1.html#other-types-of-generative-ai-quick-map",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "Even though this week is coding-focused, generative AI includes:\n\nimage generation\naudio generation\nvideo generation\n\nWhy you should care:\n\nreal products combine modalities\ndocumentation and UI work often needs imagery",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#project-workflow-with-ai-assistance",
    "href": "notes/notes_1.html#project-workflow-with-ai-assistance",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This is the most ‚Äúreal world‚Äù section. It breaks software development into stages where AI can help.\n\n\nWhere AI helps:\n\nbrainstorming features\ngenerating user stories\ndrafting requirements\n\nRisk:\n\n‚Äúfeature creep‚Äù (AI happily invents scope)\n\n\n\n\nWhere AI helps:\n\nscaffolding projects\ndrafting modules\nwriting repetitive code\n\nRisk:\n\ninconsistent design decisions\nhidden complexity\n\n\n\n\nWhere AI helps:\n\nspotting style issues\nsuggesting simplifications\nidentifying missing error handling\n\nRisk:\n\nfalse confidence (the model sounds sure)\n\n\n\n\nWhere AI helps:\n\ngenerating test cases\nsuggesting likely failure points\nexplaining stack traces\n\nRisk:\n\nshallow tests that don‚Äôt reflect real requirements\n\n\n\n\nWhere AI helps:\n\nexplaining APIs\nimproving README clarity\ngenerating examples\n\nRisk:\n\ndocs drift from reality\n\n\n‚ÄúThis cycle creates a powerful symbiotic relationship‚Ä¶‚Äù",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#choosing-the-right-generative-ai-tools",
    "href": "notes/notes_1.html#choosing-the-right-generative-ai-tools",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This week emphasises selection criteria. In real teams, you choose tools based on:\n\nintegration (does it fit your IDE/workflow?)\ncontext (does it see your project?)\nprivacy (can you send code externally?)\nquality (accuracy + helpfulness)\n\n\n\n\nDoes it understand multi-file projects?\nDoes it support test generation?\nDoes it help you refactor safely?\nDoes it provide citations or traceability?",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#common-failure-modes-what-students-must-learn",
    "href": "notes/notes_1.html#common-failure-modes-what-students-must-learn",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This week implies several failure modes. We make them explicit because they will recur all term.\n\n\nThe model invents:\n\nnonexistent functions\nwrong APIs\nfake ‚Äúfacts‚Äù\n\nMitigation:\n\nverify against docs\nrun the code\nwrite tests\n\n\n\n\nSometimes your prompt ‚Äúaccidentally‚Äù forces a weird design. Mitigation:\n\nask for alternatives\nrequest tradeoffs\niterate\n\n\n\n\nAI-generated tests often:\n\nonly test ‚Äúhappy path‚Äù\nmirror implementation details\n\nMitigation:\n\nuse boundary cases\ntest behaviour\nadd negative tests",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#go-forth-and-code",
    "href": "notes/notes_1.html#go-forth-and-code",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "This final section is motivational. This week‚Äôs message is:\n\ndon‚Äôt fear the tools\ntreat them as assistants\nkeep your engineering discipline",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#a-simple-ai-as-refactor-assistant-workflow",
    "href": "notes/notes_1.html#a-simple-ai-as-refactor-assistant-workflow",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "# A tiny refactor candidate\n\ndef total_cost(prices, tax_rate):\n    # NOTE: intentionally minimal\n    return sum(prices) * (1 + tax_rate)\n\nprint(total_cost([10, 20, 30], 0.2))",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#turning-a-prompt-into-a-structured-specification",
    "href": "notes/notes_1.html#turning-a-prompt-into-a-structured-specification",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "from dataclasses import dataclass\n\n@dataclass\nclass FeatureSpec:\n    name: str\n    user_story: str\n    acceptance_criteria: list[str]\n\nspec = FeatureSpec(\n    name=\"Export report as CSV\",\n    user_story=\"As a user, I want to export my report so I can analyse it in Excel.\",\n    acceptance_criteria=[\n        \"Export button downloads a .csv file\",\n        \"CSV includes headers\",\n        \"Handles empty datasets gracefully\",\n    ],\n)\n\nprint(spec)",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#a-minimal-test-first-habit",
    "href": "notes/notes_1.html#a-minimal-test-first-habit",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "# Simple behaviour test (no frameworks yet)\n\ndef add(a, b):\n    return a + b\n\nassert add(2, 3) == 5\nassert add(-1, 1) == 0\nprint(\"All tests passed\")",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_1.html#summing-up",
    "href": "notes/notes_1.html#summing-up",
    "title": "Week 01 ‚Äî Introducing generative AI",
    "section": "",
    "text": "Generative AI can improve coding productivity, but it must be combined with testing and review.\nThe developer tool landscape includes chat models, IDE assistants, and specialised coding tools.\nLLMs are probabilistic systems (fluent but not guaranteed correct).\nReal-world workflows use AI across ideation, implementation, review, testing, and documentation.",
    "crumbs": [
      "Notes",
      "Notes 01 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html",
    "href": "notes/notes_2.html",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "‚ÄúImagine having an intelligent assistant that helps you with coding.‚Äù\n\n\nWeek focus\nWhat is GitHub Copilot?\nHow GitHub Copilot works\nInteracting with GitHub Copilot\nCommon patterns\nContext is everything\nWhat is NLP?\nA simple Python project\nImages from Week 02\nSumming Up\nVerification report (Week 02)\n\n\n\nThis week is your practical starting point: using an AI coding assistant while you write real code. This week uses GitHub Copilot as the main example and builds up three big ideas:\n\nAn assistant is only useful if you can communicate intent clearly.\nContext is the difference between ‚Äúwow‚Äù and ‚Äúwhy on earth did it do that‚Äù.\nSmall, repeatable patterns beat ‚Äúmagic prompts‚Äù.\n\nWe‚Äôll work through:\n\nWhat Copilot is and how it behaves in an IDE\nThe main interaction modes (completion vs chat)\nCommon prompting patterns for code\nWhy context matters (and how to control it)\nA mini Python project (word frequency) that‚Äôs perfect for practicing AI-assisted workflow\n\n\n\n\nExplain what GitHub Copilot does (and what it does not do)\nUse code completion and chat-style assistance appropriately\nCreate stronger prompts inside the code editor using comments and structure\nControl context by isolating files, adding examples, and writing better docstrings\nBuild a small Python script that is correct, testable, and easy to extend\n\n\n\n\n\n\n‚ÄúDo you remember the ‚Äúrubber duck‚Äù debugging method?‚Äù\n\nThis week frames Copilot as a smarter rubber duck: a helper you can ‚Äútalk to‚Äù while you‚Äôre coding. The difference is that Copilot can respond with suggestions and complete code.\n\n\nCopilot is not a human developer. It does not ‚Äúunderstand‚Äù your goal the way a teammate does.\nInstead, it behaves like:\n\na fast autocomplete system that predicts what code might come next\na probabilistic generator that fills in plausible patterns\na tool that is highly sensitive to what is currently visible in your editor\n\nIn practical terms:\n\nCopilot is great at boilerplate, repetitive patterns, standard library usage\nCopilot is weaker at novel requirements, hidden constraints, domain-specific rules\nCopilot can be confidently wrong (especially when you give it vague context)\n\n\nMicro-quote (Ch2): ‚ÄúIt‚Äôs like having your own pair programmer.‚Äù\n\n\n\n\n\nCopilot can generate code that runs, but still fails your real requirements.\nCopilot can generate code that looks professional but has subtle errors.\nCopilot can ‚Äúinvent‚Äù APIs (especially for libraries you‚Äôre not actually using).\n\nA useful way to say it:\nCopilot is a productivity amplifier, not a correctness guarantee.\n\n\n\nIn this week, Copilot is described as integrated into popular tools.\n\n‚ÄúGitHub Copilot is available for Visual Studio, Visual Studio Code ‚Ä¶‚Äù\n\nIn practice you‚Äôll encounter Copilot mainly in:\n\nVS Code\nJetBrains IDEs\nVisual Studio\n\nThe UX matters because the tool ‚Äúfeels‚Äù different depending on editor:\n\ncompletions can appear inline (ghost text)\nchat can appear in a sidebar\nsuggestions can appear as full blocks\n\n\n\n\n\n\n‚ÄúGitHub Copilot is trained on public source code.‚Äù\n\nAt a high level Copilot is trained on lots of code and learns patterns like:\n\n‚Äúfunctions often have docstrings‚Äù\n‚Äúfor-loops often follow this syntax‚Äù\n‚Äúcommon web frameworks have typical shapes‚Äù\n\nThat‚Äôs enough to make it extremely useful.\n\n\nThink of Copilot like this:\n\nYou write code + comments in your editor.\nCopilot reads a window of text around your cursor (context).\nIt predicts likely next tokens (characters/words/code fragments).\nIt offers suggestions, sometimes multiple.\n\n\n\n\n\n\nflowchart LR\n  A[Your current file context] --&gt; B[Model predicts next tokens]\n  B --&gt; C[Inline completion suggestion]\n  B --&gt; D[Chat response / code block]\n  C --&gt; E[You accept / reject / modify]\n  D --&gt; E\n  E --&gt; A\n\n\n\n\n\n\n\n\n\n1) Small changes change the output.\n\nA better function name can massively improve suggestions.\nA docstring can ‚Äúanchor‚Äù Copilot to the right intent.\nA wrong import can cause Copilot to hallucinate an API.\n\n2) Copilot is context-bound.\nIf your editor context is poor (no clear requirements, no types, no examples), output quality drops.\n\n\n\n\nThis week highlights multiple interaction modes. You should deliberately choose the mode that fits the task.\n\n\nBest for:\n\nboilerplate\nrepetitive patterns\nstandard data manipulation\nwriting ‚Äúthe next few lines‚Äù\n\nWhat it feels like:\n\nghost text appears\nyou accept it (tab / enter)\nor you keep typing to steer it\n\n\n\nIf you already know what you want, use completion.\nIf you are unsure or exploring, use chat.\n\n\n\n\nBest for:\n\nasking ‚Äúwhy is this failing?‚Äù\ngetting refactors\ngenerating tests\nexplaining code\nexploring design options\n\nBut: chat can drift into confident nonsense unless you anchor it.\nA useful discipline:\n\nask for assumptions explicitly\ndemand test cases\nrequire it to cite file names / functions\n\n\n\n\nOne of the strongest techniques is to prompt inside the code:\n\nwrite a docstring describing inputs and outputs\nadd examples\nthen let Copilot fill in the function body\n\nHere‚Äôs a pattern that works well:\ndef normalize_text(s: str) -&gt; str:\n    \"\"\"Normalize text for counting words.\n\n    Requirements:\n    - Lowercase\n    - Remove punctuation\n    - Collapse whitespace\n\n    Examples:\n    &gt;&gt;&gt; normalize_text(\"Hello, World!\")\n    'hello world'\n\n    \"\"\"\n    # Copilot-friendly: clear name, clear docstring, example-driven\n    raise NotImplementedError\nEven if Copilot isn‚Äôt available, this is good engineering: it communicates intent to humans too.\n\n\n\n\nThis is where this week becomes highly practical. The goal is to build a toolbox of repeatable patterns rather than ‚Äúasking AI to code the whole thing‚Äù.\n\n\nInstead of asking for a complete program, define the shape:\n\nfunction stubs\nclear names\ndocstrings\nTODO comments\n\nThen ask Copilot to fill the middle.\nWhy it works:\n\ncontext becomes stable\nyou reduce the chance of architectural drift\nyou force the assistant into your structure\n\n\n\n\nIf you leave room for multiple solutions, Copilot will pick one randomly.\nSo constrain:\n\nlibraries (standard library only)\ncomplexity (O(n) where possible)\ninterface requirements\n\nExample constraint prompt:\n# Implement count_words(text: str) -&gt; dict[str,int]\n# Constraints: standard library only, ignore punctuation, lowercase\n# Return: dictionary mapping word -&gt; count\n\n\n\nA shortcut to quality is to require tests early.\nExample:\ndef count_words(text: str) -&gt; dict[str, int]:\n    \"\"\"Count words in a string.\"\"\"\n    ...\n\ndef test_count_words():\n    assert count_words(\"a a b\") == {\"a\": 2, \"b\": 1}\nEven if the assistant writes the test, you decide what correctness means.\n\n\n\nHave the assistant explain its approach before it writes code.\nThis reduces:\n\nsilent bad assumptions\nhidden complexity\nmagic one-liners that students can‚Äôt read\n\n\n\n\n\nThis is one of the most important ideas in the early weeks.\nCopilot is sensitive to:\n\nthe file you‚Äôre in\nthe function signature\nnearby imports\nvariable names\ncomments and docstrings\n\nIf you want consistent help, you must engineer the context.\n\n\nCopilot learns from what you have open. So if the wrong file is open, suggestions can be biased.\nPractical tactics:\n\nkeep only relevant files open\nput shared utilities in a clear module\nwrite better names (don‚Äôt use x, tmp, stuff)\n\n\n\n\nIf your code is messy:\n\ninconsistent naming\nunclear responsibilities\nno docstrings\nno tests\n\nCopilot will often reinforce the mess.\nIf your code is clean:\n\ntyped functions\npredictable structure\ngood abstractions\n\nCopilot will often ‚Äúlock on‚Äù and become extremely productive.\n\n\n\n\nThis week includes a section on Natural Language Processing (NLP) to support the mini project.\nFor this course, you don‚Äôt need a full NLP theory module yet, but you do need the practical idea:\n\nNLP is about treating language as data.\n\nThis matters because:\n\nprompts are language\ncode is language-like\nthe project uses text analysis techniques\n\n\n\nFor word frequency, the NLP-ish pipeline is often:\n\nNormalize: lowercase, remove punctuation\nTokenize: split text into words\nCount: build frequency counts\nSort / rank: extract most common terms\nInterpret: what do the counts suggest?\n\nIn real world AI coding:\n\nsteps 1‚Äì4 are easy to automate\nstep 5 is where humans add meaning\n\n\n\n\n\nThis week includes a mini Python project to practice working with Copilot. It analyses word frequency in a text.\nWe‚Äôll encounter:\n\nfile reading\nstrings\ndictionaries\nsorting\nbasic program structure\n\nAnd it scales easily:\n\nadd stopwords\nsupport multiple files\nplot results\ncompute TF-IDF later\n\n\n\nA minimal setup:\n\nPython 3.11+\nVS Code\na virtual environment\n\npython -m venv .venv\nsource .venv/bin/activate\npython -V\n\n\n\nWe‚Äôll build the program in small, testable pieces.\n\n\nimport re\n\ndef normalize_text(text: str) -&gt; str:\n    \"\"\"Lowercase, remove punctuation, collapse whitespace.\"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\nWhat Copilot often gets wrong here:\n\naccidentally removing apostrophes in a way that merges words\nleaving multiple spaces\nforgetting to handle newlines\n\n\n\n\ndef tokenize(text: str) -&gt; list[str]:\n    \"\"\"Split normalized text into tokens.\"\"\"\n    if not text:\n        return []\n    return text.split(\" \")\nWhy we keep this separate:\n\nit‚Äôs easy to test\nyou might later replace it with a smarter tokenizer\n\n\n\n\nfrom collections import Counter\n\ndef word_frequencies(tokens: list[str]) -&gt; dict[str, int]:\n    \"\"\"Return frequency counts for tokens.\"\"\"\n    return dict(Counter(tokens))\nCopilot-friendly note:\n\nCounter is standard library and reliable\navoids manual dictionary counting bugs\n\n\n\n\ndef top_n(freq: dict[str, int], n: int = 10) -&gt; list[tuple[str, int]]:\n    \"\"\"Return top N words sorted by descending frequency.\"\"\"\n    return sorted(freq.items(), key=lambda kv: kv[1], reverse=True)[:n]\n\n\n\nfrom pathlib import Path\n\ndef analyze_file(path: str, n: int = 10) -&gt; list[tuple[str, int]]:\n    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n    norm = normalize_text(text)\n    tokens = tokenize(norm)\n    freq = word_frequencies(tokens)\n    return top_n(freq, n=n)\n\nif __name__ == \"__main__\":\n    results = analyze_file(\"sample.txt\", n=20)\n    for word, count in results:\n        print(f\"{word}\\t{count}\")\nThis is ‚Äúboring‚Äù code ‚Äî and that‚Äôs the point.\nIt‚Äôs ideal for Copilot because:\n\nthe patterns are common\nthe structure is predictable\nthe errors are easy to spot\n\n\n\n\n\nOnce you have correctness, you can explore performance.\nA simple benchmark pattern:\nimport time\n\ndef time_it(fn, *args, repeats: int = 1000):\n    start = time.perf_counter()\n    for _ in range(repeats):\n        fn(*args)\n    end = time.perf_counter()\n    return end - start\nStudents should learn the ordering:\n\ncorrect\nreadable\nonly then faster\n\n\n\n\n\n\n\n\nThe assistant is only as good as the context you give it.\n\n\n\n\n\nDesign the skeleton\nGenerate small chunks\nRun tests\nRefactor\nRepeat",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#week-focus",
    "href": "notes/notes_2.html#week-focus",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This week is your practical starting point: using an AI coding assistant while you write real code. This week uses GitHub Copilot as the main example and builds up three big ideas:\n\nAn assistant is only useful if you can communicate intent clearly.\nContext is the difference between ‚Äúwow‚Äù and ‚Äúwhy on earth did it do that‚Äù.\nSmall, repeatable patterns beat ‚Äúmagic prompts‚Äù.\n\nWe‚Äôll work through:\n\nWhat Copilot is and how it behaves in an IDE\nThe main interaction modes (completion vs chat)\nCommon prompting patterns for code\nWhy context matters (and how to control it)\nA mini Python project (word frequency) that‚Äôs perfect for practicing AI-assisted workflow\n\n\n\n\nExplain what GitHub Copilot does (and what it does not do)\nUse code completion and chat-style assistance appropriately\nCreate stronger prompts inside the code editor using comments and structure\nControl context by isolating files, adding examples, and writing better docstrings\nBuild a small Python script that is correct, testable, and easy to extend",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#what-is-github-copilot",
    "href": "notes/notes_2.html#what-is-github-copilot",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "‚ÄúDo you remember the ‚Äúrubber duck‚Äù debugging method?‚Äù\n\nThis week frames Copilot as a smarter rubber duck: a helper you can ‚Äútalk to‚Äù while you‚Äôre coding. The difference is that Copilot can respond with suggestions and complete code.\n\n\nCopilot is not a human developer. It does not ‚Äúunderstand‚Äù your goal the way a teammate does.\nInstead, it behaves like:\n\na fast autocomplete system that predicts what code might come next\na probabilistic generator that fills in plausible patterns\na tool that is highly sensitive to what is currently visible in your editor\n\nIn practical terms:\n\nCopilot is great at boilerplate, repetitive patterns, standard library usage\nCopilot is weaker at novel requirements, hidden constraints, domain-specific rules\nCopilot can be confidently wrong (especially when you give it vague context)\n\n\nMicro-quote (Ch2): ‚ÄúIt‚Äôs like having your own pair programmer.‚Äù\n\n\n\n\n\nCopilot can generate code that runs, but still fails your real requirements.\nCopilot can generate code that looks professional but has subtle errors.\nCopilot can ‚Äúinvent‚Äù APIs (especially for libraries you‚Äôre not actually using).\n\nA useful way to say it:\nCopilot is a productivity amplifier, not a correctness guarantee.\n\n\n\nIn this week, Copilot is described as integrated into popular tools.\n\n‚ÄúGitHub Copilot is available for Visual Studio, Visual Studio Code ‚Ä¶‚Äù\n\nIn practice you‚Äôll encounter Copilot mainly in:\n\nVS Code\nJetBrains IDEs\nVisual Studio\n\nThe UX matters because the tool ‚Äúfeels‚Äù different depending on editor:\n\ncompletions can appear inline (ghost text)\nchat can appear in a sidebar\nsuggestions can appear as full blocks",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#how-github-copilot-works",
    "href": "notes/notes_2.html#how-github-copilot-works",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "‚ÄúGitHub Copilot is trained on public source code.‚Äù\n\nAt a high level Copilot is trained on lots of code and learns patterns like:\n\n‚Äúfunctions often have docstrings‚Äù\n‚Äúfor-loops often follow this syntax‚Äù\n‚Äúcommon web frameworks have typical shapes‚Äù\n\nThat‚Äôs enough to make it extremely useful.\n\n\nThink of Copilot like this:\n\nYou write code + comments in your editor.\nCopilot reads a window of text around your cursor (context).\nIt predicts likely next tokens (characters/words/code fragments).\nIt offers suggestions, sometimes multiple.\n\n\n\n\n\n\nflowchart LR\n  A[Your current file context] --&gt; B[Model predicts next tokens]\n  B --&gt; C[Inline completion suggestion]\n  B --&gt; D[Chat response / code block]\n  C --&gt; E[You accept / reject / modify]\n  D --&gt; E\n  E --&gt; A\n\n\n\n\n\n\n\n\n\n1) Small changes change the output.\n\nA better function name can massively improve suggestions.\nA docstring can ‚Äúanchor‚Äù Copilot to the right intent.\nA wrong import can cause Copilot to hallucinate an API.\n\n2) Copilot is context-bound.\nIf your editor context is poor (no clear requirements, no types, no examples), output quality drops.",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#interacting-with-github-copilot",
    "href": "notes/notes_2.html#interacting-with-github-copilot",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This week highlights multiple interaction modes. You should deliberately choose the mode that fits the task.\n\n\nBest for:\n\nboilerplate\nrepetitive patterns\nstandard data manipulation\nwriting ‚Äúthe next few lines‚Äù\n\nWhat it feels like:\n\nghost text appears\nyou accept it (tab / enter)\nor you keep typing to steer it\n\n\n\nIf you already know what you want, use completion.\nIf you are unsure or exploring, use chat.\n\n\n\n\nBest for:\n\nasking ‚Äúwhy is this failing?‚Äù\ngetting refactors\ngenerating tests\nexplaining code\nexploring design options\n\nBut: chat can drift into confident nonsense unless you anchor it.\nA useful discipline:\n\nask for assumptions explicitly\ndemand test cases\nrequire it to cite file names / functions\n\n\n\n\nOne of the strongest techniques is to prompt inside the code:\n\nwrite a docstring describing inputs and outputs\nadd examples\nthen let Copilot fill in the function body\n\nHere‚Äôs a pattern that works well:\ndef normalize_text(s: str) -&gt; str:\n    \"\"\"Normalize text for counting words.\n\n    Requirements:\n    - Lowercase\n    - Remove punctuation\n    - Collapse whitespace\n\n    Examples:\n    &gt;&gt;&gt; normalize_text(\"Hello, World!\")\n    'hello world'\n\n    \"\"\"\n    # Copilot-friendly: clear name, clear docstring, example-driven\n    raise NotImplementedError\nEven if Copilot isn‚Äôt available, this is good engineering: it communicates intent to humans too.",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#common-patterns",
    "href": "notes/notes_2.html#common-patterns",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This is where this week becomes highly practical. The goal is to build a toolbox of repeatable patterns rather than ‚Äúasking AI to code the whole thing‚Äù.\n\n\nInstead of asking for a complete program, define the shape:\n\nfunction stubs\nclear names\ndocstrings\nTODO comments\n\nThen ask Copilot to fill the middle.\nWhy it works:\n\ncontext becomes stable\nyou reduce the chance of architectural drift\nyou force the assistant into your structure\n\n\n\n\nIf you leave room for multiple solutions, Copilot will pick one randomly.\nSo constrain:\n\nlibraries (standard library only)\ncomplexity (O(n) where possible)\ninterface requirements\n\nExample constraint prompt:\n# Implement count_words(text: str) -&gt; dict[str,int]\n# Constraints: standard library only, ignore punctuation, lowercase\n# Return: dictionary mapping word -&gt; count\n\n\n\nA shortcut to quality is to require tests early.\nExample:\ndef count_words(text: str) -&gt; dict[str, int]:\n    \"\"\"Count words in a string.\"\"\"\n    ...\n\ndef test_count_words():\n    assert count_words(\"a a b\") == {\"a\": 2, \"b\": 1}\nEven if the assistant writes the test, you decide what correctness means.\n\n\n\nHave the assistant explain its approach before it writes code.\nThis reduces:\n\nsilent bad assumptions\nhidden complexity\nmagic one-liners that students can‚Äôt read",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#context-is-everything",
    "href": "notes/notes_2.html#context-is-everything",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This is one of the most important ideas in the early weeks.\nCopilot is sensitive to:\n\nthe file you‚Äôre in\nthe function signature\nnearby imports\nvariable names\ncomments and docstrings\n\nIf you want consistent help, you must engineer the context.\n\n\nCopilot learns from what you have open. So if the wrong file is open, suggestions can be biased.\nPractical tactics:\n\nkeep only relevant files open\nput shared utilities in a clear module\nwrite better names (don‚Äôt use x, tmp, stuff)\n\n\n\n\nIf your code is messy:\n\ninconsistent naming\nunclear responsibilities\nno docstrings\nno tests\n\nCopilot will often reinforce the mess.\nIf your code is clean:\n\ntyped functions\npredictable structure\ngood abstractions\n\nCopilot will often ‚Äúlock on‚Äù and become extremely productive.",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#what-is-nlp",
    "href": "notes/notes_2.html#what-is-nlp",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This week includes a section on Natural Language Processing (NLP) to support the mini project.\nFor this course, you don‚Äôt need a full NLP theory module yet, but you do need the practical idea:\n\nNLP is about treating language as data.\n\nThis matters because:\n\nprompts are language\ncode is language-like\nthe project uses text analysis techniques\n\n\n\nFor word frequency, the NLP-ish pipeline is often:\n\nNormalize: lowercase, remove punctuation\nTokenize: split text into words\nCount: build frequency counts\nSort / rank: extract most common terms\nInterpret: what do the counts suggest?\n\nIn real world AI coding:\n\nsteps 1‚Äì4 are easy to automate\nstep 5 is where humans add meaning",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#a-simple-python-project",
    "href": "notes/notes_2.html#a-simple-python-project",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "This week includes a mini Python project to practice working with Copilot. It analyses word frequency in a text.\nWe‚Äôll encounter:\n\nfile reading\nstrings\ndictionaries\nsorting\nbasic program structure\n\nAnd it scales easily:\n\nadd stopwords\nsupport multiple files\nplot results\ncompute TF-IDF later\n\n\n\nA minimal setup:\n\nPython 3.11+\nVS Code\na virtual environment\n\npython -m venv .venv\nsource .venv/bin/activate\npython -V\n\n\n\nWe‚Äôll build the program in small, testable pieces.\n\n\nimport re\n\ndef normalize_text(text: str) -&gt; str:\n    \"\"\"Lowercase, remove punctuation, collapse whitespace.\"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\nWhat Copilot often gets wrong here:\n\naccidentally removing apostrophes in a way that merges words\nleaving multiple spaces\nforgetting to handle newlines\n\n\n\n\ndef tokenize(text: str) -&gt; list[str]:\n    \"\"\"Split normalized text into tokens.\"\"\"\n    if not text:\n        return []\n    return text.split(\" \")\nWhy we keep this separate:\n\nit‚Äôs easy to test\nyou might later replace it with a smarter tokenizer\n\n\n\n\nfrom collections import Counter\n\ndef word_frequencies(tokens: list[str]) -&gt; dict[str, int]:\n    \"\"\"Return frequency counts for tokens.\"\"\"\n    return dict(Counter(tokens))\nCopilot-friendly note:\n\nCounter is standard library and reliable\navoids manual dictionary counting bugs\n\n\n\n\ndef top_n(freq: dict[str, int], n: int = 10) -&gt; list[tuple[str, int]]:\n    \"\"\"Return top N words sorted by descending frequency.\"\"\"\n    return sorted(freq.items(), key=lambda kv: kv[1], reverse=True)[:n]\n\n\n\nfrom pathlib import Path\n\ndef analyze_file(path: str, n: int = 10) -&gt; list[tuple[str, int]]:\n    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n    norm = normalize_text(text)\n    tokens = tokenize(norm)\n    freq = word_frequencies(tokens)\n    return top_n(freq, n=n)\n\nif __name__ == \"__main__\":\n    results = analyze_file(\"sample.txt\", n=20)\n    for word, count in results:\n        print(f\"{word}\\t{count}\")\nThis is ‚Äúboring‚Äù code ‚Äî and that‚Äôs the point.\nIt‚Äôs ideal for Copilot because:\n\nthe patterns are common\nthe structure is predictable\nthe errors are easy to spot\n\n\n\n\n\nOnce you have correctness, you can explore performance.\nA simple benchmark pattern:\nimport time\n\ndef time_it(fn, *args, repeats: int = 1000):\n    start = time.perf_counter()\n    for _ in range(repeats):\n        fn(*args)\n    end = time.perf_counter()\n    return end - start\nStudents should learn the ordering:\n\ncorrect\nreadable\nonly then faster",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_2.html#summing-up",
    "href": "notes/notes_2.html#summing-up",
    "title": "Week 02: First steps with AI-assisted coding",
    "section": "",
    "text": "The assistant is only as good as the context you give it.\n\n\n\n\n\nDesign the skeleton\nGenerate small chunks\nRun tests\nRefactor\nRepeat",
    "crumbs": [
      "Notes",
      "Notes 02 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html",
    "href": "notes/notes_4.html",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "‚ÄúHow can generative AI speed up the design process?‚Äù\n\n\nWeek focus\nFrom design to code: extracting requirements\nSetting up your Python environment\nLaying out the application skeleton\nYour first Flask app: routes, requests, responses\nTemplates and pages: keeping logic out of HTML\nConfiguration, dependencies, and reproducibility\nRun, observe, iterate\nCommon failure modes\nSumming Up\nAppendix: check your understanding\nDeep dive: mapping requirements to routes\nDeep dive: keeping state (for now)\nDeep dive: debugging discipline\nA worked walkthrough: turning an idea into a plan\nSetting up your Python environment (deeper notes)\nLaying out the application skeleton (deeper notes)\nFlask request lifecycle (what actually happens)\nTemplates (expanded)\nUsing AI tools without losing control\nChecklist\n\n\n\n\nThis week focuses on Coding the first version of our application.\nThis week is deliberately practical: it moves from an idea/design document into a runnable web app skeleton.\nKey theme: you don‚Äôt need the perfect design before you start coding ‚Äî but you do need a clear first version.\nWe‚Äôre practising how to translate text descriptions into code artefacts: folders, files, functions, and routes.\nThis week includes visual callouts/figures; we‚Äôll reference the key ones as we go.\n\n\n\n\n\nThis week starts by treating your design notes as an input to development. The goal is to turn vague ideas into requirements you can implement.\nA requirement in this context is a short, testable statement of behaviour: what the user can do, or what the system must store/compute/display.\nA useful trick: rewrite each requirement as a user story: As a user, I want ‚Ä¶ so that ‚Ä¶.\nRequirements are not only ‚Äòfeatures‚Äô. They also include constraints like performance, security, and maintainability.\nWhen you work with AI coding tools, requirements become even more important: they are the anchor that keeps generated code aligned with your intent.\nPractical decomposition pattern used in this week:\nStart with the smallest end-to-end ‚Äòthin slice‚Äô (a page that loads; a form that submits; a result that displays).\nList the minimal data objects you need (e.g., a question bank, callsigns, categories, logs).\nIdentify the ‚Äòedges‚Äô: inputs (forms/API), outputs (HTML pages), and storage (files/DB).\nOnly then decide folder structure and module boundaries.\n\n\n\n\n\nPython projects should isolate libraries per project using a virtual environment (often abbreviated venv).\nIsolation matters because different projects need different versions of the same library (a common source of ‚Äòworks on my machine‚Äô bugs).\nWe care about this early because it teaches reproducibility and reduces setup friction later. ### Create and activate a venv\nCreate the environment in your project folder (name is conventional: .venv).\nActivate it before installing packages so installs go into the venv, not your global Python.\n\npython -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n# .venv\\Scripts\\activate   # Windows (PowerShell)\npython -m pip install --upgrade pip\n\n\n\nThis week uses Flask as the minimal web framework: it‚Äôs small enough to understand without magic.\nTreat dependencies as part of your project: record them in requirements.txt so another machine can reproduce the same environment.\n\npip install flask\npip freeze &gt; requirements.txt\n\n\n\n\n\nBefore writing ‚Äòreal logic‚Äô, you create a project structure: folders for app code, templates, static assets, and tests.\nA structure is a communication tool: it tells future you (and teammates) where things belong.\nThis week‚Äôs key move is creating stubs: placeholder functions/routes that define the shape of the app before it is complete. ### A minimal, conventional Flask layout\n\nmyapp/\n  app.py\n  templates/\n    index.html\n  static/\n    style.css\n  requirements.txt\n  README.md\n\nYou can evolve this into a package layout (myapp/__init__.py) later. Start simple.\nKeep templates/ for HTML templates and static/ for CSS/JS/images, matching Flask conventions.\n\n\n\n\n\nA Flask app is basically a mapping from URLs to Python functions.\nEach mapping is a route. When a browser requests /, Flask calls the function and returns a response.\nYour job is to control: input ‚Üí processing ‚Üí output. ### The smallest runnable Flask app\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    return \"Hello! Week 4 app is running.\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ndebug=True turns on debug mode: auto-reload + a helpful error page. Great for learning; disable in production.\nWhen something breaks, read the traceback top-to-bottom: it tells you where and often why. ### Running the app\n\npython app.py\n# Then open the printed URL (usually http://127.0.0.1:5000/)\n\n\n\nA browser makes an HTTP request (method + path + headers + optional body).\nYour route function returns a response (status code + headers + body).\nEven when you ‚Äòjust return a string‚Äô, Flask wraps it into a full HTTP response. ## Templates and pages: keeping logic out of HTML {#templates-and-pages-keeping-logic-out-of-html}\nOnce you move beyond plain strings, you‚Äôll render HTML templates.\nTemplates let you keep Python logic in Python and presentation in HTML.\nFlask uses Jinja2 templates; you inject values and loops into a mostly-normal HTML page. ### Example: render a template\n\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    return render_template(\"index.html\", title=\"Real World AI\")\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;{{ title }}&lt;/h1&gt;\n    &lt;p&gt;Your Flask app is rendering templates.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\nJinja placeholders like { title } are replaced at render time.\nWhen you later add forms, you‚Äôll pass user-submitted values into templates to show results.\n\n\n\n\n\n\nTreat every external library as a dependency you must track.\nrequirements.txt is the simplest approach: it records exact versions so installs are consistent.\nA clean workflow: create venv ‚Üí install deps ‚Üí freeze ‚Üí commit both code and requirements.\nIf a teammate can‚Äôt run your app, 80% of the time it‚Äôs: wrong venv, wrong Python, or missing deps. ### Recreating the environment from scratch\n\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n\n\n\n\nThis week‚Äôs development rhythm is iterative: make a small change, run the app, observe, and repeat.\nThis is not just ‚Äòcoding style‚Äô ‚Äî it‚Äôs risk management. Small steps make bugs easier to locate.\nWhen using AI assistants, keep iterations even smaller: generate a tiny piece, test it, then proceed. ### A tiny iteration loop (conceptual)\n\n\n\n\n\n\nflowchart TD\n  A[Write or generate small change] --&gt; B[Run unit of work]\n  B --&gt; C{Did it work?}\n  C -- Yes --&gt; D[Commit / move to next small change]\n  C -- No --&gt; E[Read error, reduce scope, fix]\n  E --&gt; A\n\n\n\n\n\n\n\n\n\n\nVenv not activated: pip install went to global Python; Flask isn‚Äôt found when running.\nWrong working directory: you run python app.py from a different folder and Flask can‚Äôt find templates/.\nPort already in use: another process is using 5000; you‚Äôll see an ‚ÄòAddress already in use‚Äô message.\nTemplate errors: misspelt variable names in Jinja; look at the stack trace and the highlighted line.\nImport cycles when you split files too early: keep it simple until you understand module imports. ## Summing Up {#summing-up}\nWeek 04 is about moving from idea to a runnable first version.\nYou practised translating requirements into a project skeleton and basic routes.\nYou set up a virtual environment, installed Flask, and captured dependencies for reproducibility.\nYou learned the core web mental model: request ‚Üí route ‚Üí response, then grew it into templates.\nNext weeks build on this baseline by generating more code with AI tools and expanding the app‚Äôs features.\n\n\n\n\n\nExplain the difference between a feature request and an implementation detail.\nWhat does a virtual environment protect you from? Give two concrete examples.\nWhy is requirements.txt useful even for a tiny project?\nWhat is a route? How is it different from a function that is never called by Flask?\nWhat is the advantage of templates compared to building HTML with string concatenation?\nDescribe the iteration loop you should follow when using an AI assistant to generate code.\n\n\n\n\n\nA strong starting point is to list user actions, then map each to a URL.\nExample: ‚ÄòStart a practice session‚Äô might map to /start, while ‚ÄòSubmit an answer‚Äô maps to /answer.\nThink about which actions change state (POST) versus which only read state (GET).\nEven if you ignore REST purity in week 4, using GET/POST correctly prevents common bugs.\n\n\n\n\n\nEarly versions often avoid databases. You can store small state in memory while learning.\nBut remember: in-memory state resets when the server restarts.\nFor learning, this is fine ‚Äî it encourages you to separate ‚Äòcore logic‚Äô from ‚Äòstorage‚Äô.\nLater weeks can swap storage without rewriting everything if your code is modular.\n\n\n\n\n\nWhen an error happens, copy the traceback and identify the first line in your codebase.\nAdd a print/log statement above that line to inspect variables.\nChange one thing at a time; rerun; confirm the effect.\nAvoid random guessing ‚Äî it‚Äôs slower than systematic diagnosis.\n\n\n\n\n\nThis week‚Äôs core move is: design artefacts ‚Üí requirements ‚Üí code skeleton.\nA useful mental model is to treat every paragraph of your design document as one of:\n\na feature (something the user can do)\na constraint (a limit: performance, privacy, usability)\nan assumption (something you are betting is true)\nan open question (something you must resolve)\n\nIf you can‚Äôt label a paragraph, it‚Äôs often ‚Äúhand-wavy‚Äù and needs rewriting.\nA simple requirement format (good enough for first year) is:\n\nAs a &lt;type of user&gt;\nI want &lt;capability&gt;\nSo that &lt;benefit&gt;\n\nThen add acceptance criteria:\n\n‚ÄúGiven ‚Ä¶ when ‚Ä¶ then ‚Ä¶‚Äù statements (these become tests later).\n\n\n\n\n\nSuppose your design doc says the app should help someone prepare for an exam.\nTurn that into requirements:\n\nAs a learner, I want to select a topic so I can practise questions.\nAs a learner, I want immediate feedback so I can correct misconceptions.\nAs a learner, I want progress tracking so I can focus on weak areas.\n\nNotice how this immediately suggests data structures and routes.\n\n# A tiny starting point: represent requirements as data\nrequirements = [\n    {\"as\": \"learner\", \"want\": \"select a topic\", \"so_that\": \"practise questions\"},\n    {\"as\": \"learner\", \"want\": \"immediate feedback\", \"so_that\": \"correct misconceptions\"},\n    {\"as\": \"learner\", \"want\": \"progress tracking\", \"so_that\": \"focus on weak areas\"},\n]\n\n# Later, you might render this in a template or export to JSON.\nimport json\nprint(json.dumps(requirements, indent=2))\n\nThis is intentionally ‚Äúlow-tech‚Äù ‚Äî the goal is to reduce ambiguity.\n\n\n\n\n\n\nWeek 04 emphasises a repeatable environment because web apps are dependency-heavy.\nTwo common mistakes by beginners:\n\ninstalling packages ‚Äúglobally‚Äù and then not remembering what you installed\nforgetting to activate the virtual environment and installing into the wrong place\n\nA virtual environment is just a folder containing:\n\na Python interpreter\nsite-packages (third-party libraries)\nscripts for activation\n\n\n\n\npython -m venv .venv\n\n# macOS / Linux\nsource .venv/bin/activate\n\n# Windows (PowerShell)\n# .venv\\Scripts\\Activate.ps1\n\nIf activation worked, your shell prompt usually changes (e.g., shows (.venv)).\nIf you get ‚Äúpermission‚Äù errors on Windows PowerShell, you may need to adjust the execution policy.\n\n\n\n\npython -m pip install --upgrade pip\npip install flask\n\npip freeze &gt; requirements.txt\ncat requirements.txt\n\nrequirements.txt is a ‚Äúsnapshot‚Äù of your environment.\nLater chapters build on this to make the project easier to share and deploy.\n\n\n\n\n\n\nThe chapter‚Äôs approach is basically: create stubs first, then fill them in.\nA stub is a small piece of code that compiles/runs but doesn‚Äôt do much yet.\n\n\n\n\nThe chapter‚Äôs screenshots illustrate a clean starting structure.\nA common layout is:\n\nmyapp/\n  app.py\n  templates/\n    index.html\n  static/\n    style.css\n  requirements.txt\n  README.md\n\nWhy separate templates/ and static/?\n\ntemplates are rendered by Flask (HTML with placeholders)\nstatic files are served directly (CSS, images, JS)\n\n\n\n\n\n\n\nStudents often think ‚ÄúFlask runs my code top-to-bottom each time‚Äù. Not quite.\nIn a Flask server:\n\nPython starts once\nFlask registers routes (URL patterns)\nthen waits for requests\n\nWhen a request arrives, Flask:\n\nmatches the URL to a route\ncalls the function for that route\nreturns the response\n\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    # This runs when the browser requests / (not when the file is imported)\n    return \"Hello from Week 4\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ndebug=True is a development convenience:\n\nauto-reloads when files change\nshows a helpful debug page on errors\n\nYou must disable debug mode for production.\n\n\n\n&lt;!-- templates/index.html --&gt;\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;{{ heading }}&lt;/h1&gt;\n    &lt;p&gt;Today we are building the first version.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef index():\n    return render_template(\"index.html\", title=\"Week 4\", heading=\"Hello, Flask\")\n\nKey idea: render_template fills placeholders like { title }.\n\n\n\n\n\n\nChapter 4 is where the temptation to ‚Äúlet the AI write the whole app‚Äù becomes strong.\nThe safe workflow is:\n\nyou define the requirements and the ‚Äúshape‚Äù of the app\nyou ask the AI for small deltas (a route, a template, a helper function)\nyou run the code and inspect the results\n\nA good prompt pattern is:\n\ngive the current file structure\nspecify the exact change\nrequire that the output is a patch/diff\n\n\nYou are helping me with a Flask app.\nCurrent structure:\n- app.py\n- templates/index.html\nTask: add a new route GET /about that renders templates/about.html.\nConstraints:\n- keep app.py minimal\n- do not add new dependencies\nReturn: the full updated app.py and new about.html\n\nThis kind of prompt reduces ‚ÄúAI drift‚Äù (the model inventing extra features).\n\n\n\n\n\nBefore you start coding:\n\nyou can explain the user goal in one sentence\nyou can list 3‚Äì5 requirements\nyou know what data your app needs to store (even if it‚Äôs just in memory)\n\nWhen you set up the project:\n\nyou created .venv/\nyou can activate it\npython -m pip -V points inside .venv/\n\nWhen you run Flask:\n\nyou can visit http://127.0.0.1:5000/\nyou can change a file and see it reload\nyou can read a stack trace when it breaks\n\n\n\n\n\n\nThis week is where your project becomes ‚Äúreal‚Äù: an app skeleton you can run.\nThe highest-value skills:\n\nturning a vague design idea into concrete requirements\ncreating a reproducible Python environment\nbuilding stubs first, then iterating\nunderstanding the request/response cycle\nusing AI tools in small, testable steps",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#week-focus",
    "href": "notes/notes_4.html#week-focus",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "This week focuses on Coding the first version of our application.\nThis week is deliberately practical: it moves from an idea/design document into a runnable web app skeleton.\nKey theme: you don‚Äôt need the perfect design before you start coding ‚Äî but you do need a clear first version.\nWe‚Äôre practising how to translate text descriptions into code artefacts: folders, files, functions, and routes.\nThis week includes visual callouts/figures; we‚Äôll reference the key ones as we go.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#from-design-to-code-extracting-requirements",
    "href": "notes/notes_4.html#from-design-to-code-extracting-requirements",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "This week starts by treating your design notes as an input to development. The goal is to turn vague ideas into requirements you can implement.\nA requirement in this context is a short, testable statement of behaviour: what the user can do, or what the system must store/compute/display.\nA useful trick: rewrite each requirement as a user story: As a user, I want ‚Ä¶ so that ‚Ä¶.\nRequirements are not only ‚Äòfeatures‚Äô. They also include constraints like performance, security, and maintainability.\nWhen you work with AI coding tools, requirements become even more important: they are the anchor that keeps generated code aligned with your intent.\nPractical decomposition pattern used in this week:\nStart with the smallest end-to-end ‚Äòthin slice‚Äô (a page that loads; a form that submits; a result that displays).\nList the minimal data objects you need (e.g., a question bank, callsigns, categories, logs).\nIdentify the ‚Äòedges‚Äô: inputs (forms/API), outputs (HTML pages), and storage (files/DB).\nOnly then decide folder structure and module boundaries.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#setting-up-your-python-environment",
    "href": "notes/notes_4.html#setting-up-your-python-environment",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Python projects should isolate libraries per project using a virtual environment (often abbreviated venv).\nIsolation matters because different projects need different versions of the same library (a common source of ‚Äòworks on my machine‚Äô bugs).\nWe care about this early because it teaches reproducibility and reduces setup friction later. ### Create and activate a venv\nCreate the environment in your project folder (name is conventional: .venv).\nActivate it before installing packages so installs go into the venv, not your global Python.\n\npython -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n# .venv\\Scripts\\activate   # Windows (PowerShell)\npython -m pip install --upgrade pip\n\n\n\nThis week uses Flask as the minimal web framework: it‚Äôs small enough to understand without magic.\nTreat dependencies as part of your project: record them in requirements.txt so another machine can reproduce the same environment.\n\npip install flask\npip freeze &gt; requirements.txt",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#laying-out-the-application-skeleton",
    "href": "notes/notes_4.html#laying-out-the-application-skeleton",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Before writing ‚Äòreal logic‚Äô, you create a project structure: folders for app code, templates, static assets, and tests.\nA structure is a communication tool: it tells future you (and teammates) where things belong.\nThis week‚Äôs key move is creating stubs: placeholder functions/routes that define the shape of the app before it is complete. ### A minimal, conventional Flask layout\n\nmyapp/\n  app.py\n  templates/\n    index.html\n  static/\n    style.css\n  requirements.txt\n  README.md\n\nYou can evolve this into a package layout (myapp/__init__.py) later. Start simple.\nKeep templates/ for HTML templates and static/ for CSS/JS/images, matching Flask conventions.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#your-first-flask-app-routes-requests-responses",
    "href": "notes/notes_4.html#your-first-flask-app-routes-requests-responses",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "A Flask app is basically a mapping from URLs to Python functions.\nEach mapping is a route. When a browser requests /, Flask calls the function and returns a response.\nYour job is to control: input ‚Üí processing ‚Üí output. ### The smallest runnable Flask app\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    return \"Hello! Week 4 app is running.\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ndebug=True turns on debug mode: auto-reload + a helpful error page. Great for learning; disable in production.\nWhen something breaks, read the traceback top-to-bottom: it tells you where and often why. ### Running the app\n\npython app.py\n# Then open the printed URL (usually http://127.0.0.1:5000/)\n\n\n\nA browser makes an HTTP request (method + path + headers + optional body).\nYour route function returns a response (status code + headers + body).\nEven when you ‚Äòjust return a string‚Äô, Flask wraps it into a full HTTP response. ## Templates and pages: keeping logic out of HTML {#templates-and-pages-keeping-logic-out-of-html}\nOnce you move beyond plain strings, you‚Äôll render HTML templates.\nTemplates let you keep Python logic in Python and presentation in HTML.\nFlask uses Jinja2 templates; you inject values and loops into a mostly-normal HTML page. ### Example: render a template\n\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    return render_template(\"index.html\", title=\"Real World AI\")\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;{{ title }}&lt;/h1&gt;\n    &lt;p&gt;Your Flask app is rendering templates.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\nJinja placeholders like { title } are replaced at render time.\nWhen you later add forms, you‚Äôll pass user-submitted values into templates to show results.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#configuration-dependencies-and-reproducibility",
    "href": "notes/notes_4.html#configuration-dependencies-and-reproducibility",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Treat every external library as a dependency you must track.\nrequirements.txt is the simplest approach: it records exact versions so installs are consistent.\nA clean workflow: create venv ‚Üí install deps ‚Üí freeze ‚Üí commit both code and requirements.\nIf a teammate can‚Äôt run your app, 80% of the time it‚Äôs: wrong venv, wrong Python, or missing deps. ### Recreating the environment from scratch\n\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#run-observe-iterate",
    "href": "notes/notes_4.html#run-observe-iterate",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "This week‚Äôs development rhythm is iterative: make a small change, run the app, observe, and repeat.\nThis is not just ‚Äòcoding style‚Äô ‚Äî it‚Äôs risk management. Small steps make bugs easier to locate.\nWhen using AI assistants, keep iterations even smaller: generate a tiny piece, test it, then proceed. ### A tiny iteration loop (conceptual)\n\n\n\n\n\n\nflowchart TD\n  A[Write or generate small change] --&gt; B[Run unit of work]\n  B --&gt; C{Did it work?}\n  C -- Yes --&gt; D[Commit / move to next small change]\n  C -- No --&gt; E[Read error, reduce scope, fix]\n  E --&gt; A",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#common-failure-modes",
    "href": "notes/notes_4.html#common-failure-modes",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Venv not activated: pip install went to global Python; Flask isn‚Äôt found when running.\nWrong working directory: you run python app.py from a different folder and Flask can‚Äôt find templates/.\nPort already in use: another process is using 5000; you‚Äôll see an ‚ÄòAddress already in use‚Äô message.\nTemplate errors: misspelt variable names in Jinja; look at the stack trace and the highlighted line.\nImport cycles when you split files too early: keep it simple until you understand module imports. ## Summing Up {#summing-up}\nWeek 04 is about moving from idea to a runnable first version.\nYou practised translating requirements into a project skeleton and basic routes.\nYou set up a virtual environment, installed Flask, and captured dependencies for reproducibility.\nYou learned the core web mental model: request ‚Üí route ‚Üí response, then grew it into templates.\nNext weeks build on this baseline by generating more code with AI tools and expanding the app‚Äôs features.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#appendix-check-your-understanding",
    "href": "notes/notes_4.html#appendix-check-your-understanding",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Explain the difference between a feature request and an implementation detail.\nWhat does a virtual environment protect you from? Give two concrete examples.\nWhy is requirements.txt useful even for a tiny project?\nWhat is a route? How is it different from a function that is never called by Flask?\nWhat is the advantage of templates compared to building HTML with string concatenation?\nDescribe the iteration loop you should follow when using an AI assistant to generate code.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#deep-dive-mapping-requirements-to-routes",
    "href": "notes/notes_4.html#deep-dive-mapping-requirements-to-routes",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "A strong starting point is to list user actions, then map each to a URL.\nExample: ‚ÄòStart a practice session‚Äô might map to /start, while ‚ÄòSubmit an answer‚Äô maps to /answer.\nThink about which actions change state (POST) versus which only read state (GET).\nEven if you ignore REST purity in week 4, using GET/POST correctly prevents common bugs.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#deep-dive-keeping-state-for-now",
    "href": "notes/notes_4.html#deep-dive-keeping-state-for-now",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Early versions often avoid databases. You can store small state in memory while learning.\nBut remember: in-memory state resets when the server restarts.\nFor learning, this is fine ‚Äî it encourages you to separate ‚Äòcore logic‚Äô from ‚Äòstorage‚Äô.\nLater weeks can swap storage without rewriting everything if your code is modular.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#deep-dive-debugging-discipline",
    "href": "notes/notes_4.html#deep-dive-debugging-discipline",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "When an error happens, copy the traceback and identify the first line in your codebase.\nAdd a print/log statement above that line to inspect variables.\nChange one thing at a time; rerun; confirm the effect.\nAvoid random guessing ‚Äî it‚Äôs slower than systematic diagnosis.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#a-worked-walkthrough-turning-the-week-into-a-plan",
    "href": "notes/notes_4.html#a-worked-walkthrough-turning-the-week-into-a-plan",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "This week‚Äôs core move is: design artefacts ‚Üí requirements ‚Üí code skeleton.\nA useful mental model is to treat every paragraph of your design document as one of:\n\na feature (something the user can do)\na constraint (a limit: performance, privacy, usability)\nan assumption (something you are betting is true)\nan open question (something you must resolve)\n\nIf you can‚Äôt label a paragraph, it‚Äôs often ‚Äúhand-wavy‚Äù and needs rewriting.\nA simple requirement format (good enough for first year) is:\n\nAs a &lt;type of user&gt;\nI want &lt;capability&gt;\nSo that &lt;benefit&gt;\n\nThen add acceptance criteria:\n\n‚ÄúGiven ‚Ä¶ when ‚Ä¶ then ‚Ä¶‚Äù statements (these become tests later).\n\n\n\n\n\nSuppose your design doc says the app should help someone prepare for an exam.\nTurn that into requirements:\n\nAs a learner, I want to select a topic so I can practise questions.\nAs a learner, I want immediate feedback so I can correct misconceptions.\nAs a learner, I want progress tracking so I can focus on weak areas.\n\nNotice how this immediately suggests data structures and routes.\n\n# A tiny starting point: represent requirements as data\nrequirements = [\n    {\"as\": \"learner\", \"want\": \"select a topic\", \"so_that\": \"practise questions\"},\n    {\"as\": \"learner\", \"want\": \"immediate feedback\", \"so_that\": \"correct misconceptions\"},\n    {\"as\": \"learner\", \"want\": \"progress tracking\", \"so_that\": \"focus on weak areas\"},\n]\n\n# Later, you might render this in a template or export to JSON.\nimport json\nprint(json.dumps(requirements, indent=2))\n\nThis is intentionally ‚Äúlow-tech‚Äù ‚Äî the goal is to reduce ambiguity.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#setting-up-your-python-environment-deeper-notes",
    "href": "notes/notes_4.html#setting-up-your-python-environment-deeper-notes",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Week 04 emphasises a repeatable environment because web apps are dependency-heavy.\nTwo common mistakes by beginners:\n\ninstalling packages ‚Äúglobally‚Äù and then not remembering what you installed\nforgetting to activate the virtual environment and installing into the wrong place\n\nA virtual environment is just a folder containing:\n\na Python interpreter\nsite-packages (third-party libraries)\nscripts for activation\n\n\n\n\npython -m venv .venv\n\n# macOS / Linux\nsource .venv/bin/activate\n\n# Windows (PowerShell)\n# .venv\\Scripts\\Activate.ps1\n\nIf activation worked, your shell prompt usually changes (e.g., shows (.venv)).\nIf you get ‚Äúpermission‚Äù errors on Windows PowerShell, you may need to adjust the execution policy.\n\n\n\n\npython -m pip install --upgrade pip\npip install flask\n\npip freeze &gt; requirements.txt\ncat requirements.txt\n\nrequirements.txt is a ‚Äúsnapshot‚Äù of your environment.\nLater chapters build on this to make the project easier to share and deploy.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#laying-out-the-application-skeleton-deeper-notes",
    "href": "notes/notes_4.html#laying-out-the-application-skeleton-deeper-notes",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "The chapter‚Äôs approach is basically: create stubs first, then fill them in.\nA stub is a small piece of code that compiles/runs but doesn‚Äôt do much yet.\n\n\n\n\nThe chapter‚Äôs screenshots illustrate a clean starting structure.\nA common layout is:\n\nmyapp/\n  app.py\n  templates/\n    index.html\n  static/\n    style.css\n  requirements.txt\n  README.md\n\nWhy separate templates/ and static/?\n\ntemplates are rendered by Flask (HTML with placeholders)\nstatic files are served directly (CSS, images, JS)",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#flask-request-lifecycle-what-actually-happens",
    "href": "notes/notes_4.html#flask-request-lifecycle-what-actually-happens",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Students often think ‚ÄúFlask runs my code top-to-bottom each time‚Äù. Not quite.\nIn a Flask server:\n\nPython starts once\nFlask registers routes (URL patterns)\nthen waits for requests\n\nWhen a request arrives, Flask:\n\nmatches the URL to a route\ncalls the function for that route\nreturns the response\n\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef home():\n    # This runs when the browser requests / (not when the file is imported)\n    return \"Hello from Week 4\"\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\ndebug=True is a development convenience:\n\nauto-reloads when files change\nshows a helpful debug page on errors\n\nYou must disable debug mode for production.\n\n\n\n&lt;!-- templates/index.html --&gt;\n&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;{{ heading }}&lt;/h1&gt;\n    &lt;p&gt;Today we are building the first version.&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.get(\"/\")\ndef index():\n    return render_template(\"index.html\", title=\"Week 4\", heading=\"Hello, Flask\")\n\nKey idea: render_template fills placeholders like { title }.",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#using-ai-tools-without-losing-control",
    "href": "notes/notes_4.html#using-ai-tools-without-losing-control",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Chapter 4 is where the temptation to ‚Äúlet the AI write the whole app‚Äù becomes strong.\nThe safe workflow is:\n\nyou define the requirements and the ‚Äúshape‚Äù of the app\nyou ask the AI for small deltas (a route, a template, a helper function)\nyou run the code and inspect the results\n\nA good prompt pattern is:\n\ngive the current file structure\nspecify the exact change\nrequire that the output is a patch/diff\n\n\nYou are helping me with a Flask app.\nCurrent structure:\n- app.py\n- templates/index.html\nTask: add a new route GET /about that renders templates/about.html.\nConstraints:\n- keep app.py minimal\n- do not add new dependencies\nReturn: the full updated app.py and new about.html\n\nThis kind of prompt reduces ‚ÄúAI drift‚Äù (the model inventing extra features).",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#checklist",
    "href": "notes/notes_4.html#checklist",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "Before you start coding:\n\nyou can explain the user goal in one sentence\nyou can list 3‚Äì5 requirements\nyou know what data your app needs to store (even if it‚Äôs just in memory)\n\nWhen you set up the project:\n\nyou created .venv/\nyou can activate it\npython -m pip -V points inside .venv/\n\nWhen you run Flask:\n\nyou can visit http://127.0.0.1:5000/\nyou can change a file and see it reload\nyou can read a stack trace when it breaks",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_4.html#summing-up",
    "href": "notes/notes_4.html#summing-up",
    "title": "Week 04 ‚Äî Coding the first version of our application",
    "section": "",
    "text": "This week is where your project becomes ‚Äúreal‚Äù: an app skeleton you can run.\nThe highest-value skills:\n\nturning a vague design idea into concrete requirements\ncreating a reproducible Python environment\nbuilding stubs first, then iterating\nunderstanding the request/response cycle\nusing AI tools in small, testable steps",
    "crumbs": [
      "Notes",
      "Notes 04 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html",
    "href": "notes/notes_6.html",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "",
    "text": "This week focuses on Generating a software backend.\nWe are continuing the HAM radio practice-test app. In Week 05 we got a working scaffold (database + basic page). In Week 06 the focus shifts from ‚Äúit runs‚Äù to ‚Äúit behaves like a real application‚Äù: it must keep track of a test-taker over multiple clicks, handle restarts, and recover cleanly from bugs.\n\nWhat this week covers (in plain language)\nRecap of the app architecture so far\nWhy backend work matters here\nSymptom-driven debugging\nWhy this is a good learning bug\nWhat Tabnine is good at\nWhat Tabnine is bad at (unless you guide it)\nA minimal but clean Flask app structure\nCode example: enabling sessions and setting a secret key\nCode example: selecting questions once per session\nCode example: route that shows the current question\nCode example: recording an answer (POST)\nMake failures reproducible\nCode example: quick smoke test with requests\nCommon error classes in this week\nSumming Up\nChecks for understanding\n\n\n\n\nTurning a static ‚Äúdump of questions‚Äù into an interactive workflow.\nCreating and maintaining persistent sessions so each student sees a consistent test flow.\nImplementing backend features in Flask with database integration.\nUsing Tabnine to:\n\ngenerate code (scaffolds and functions)\ndebug and fix errors\nrefactor toward cleaner modules\n\nCrafting prompts that produce useful help instead of vague output.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#what-this-week-covers-in-plain-language",
    "href": "notes/notes_6.html#what-this-week-covers-in-plain-language",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "",
    "text": "Turning a static ‚Äúdump of questions‚Äù into an interactive workflow.\nCreating and maintaining persistent sessions so each student sees a consistent test flow.\nImplementing backend features in Flask with database integration.\nUsing Tabnine to:\n\ngenerate code (scaffolds and functions)\ndebug and fix errors\nrefactor toward cleaner modules\n\nCrafting prompts that produce useful help instead of vague output.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#recap-of-the-app-architecture-so-far",
    "href": "notes/notes_6.html#recap-of-the-app-architecture-so-far",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Recap of the app architecture so far",
    "text": "Recap of the app architecture so far\nBy now you have:\n\nA SQLite database file (for example data/questions.db).\nModel code that opens a connection and fetches question data.\nA Flask app entry point that defines routes and renders templates.\n\nThe problem: the app works once, then fails on reload / restart / second session.\nThat is a classic ‚Äústate + lifecycle‚Äù bug.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#why-backend-work-matters-here",
    "href": "notes/notes_6.html#why-backend-work-matters-here",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Why backend work matters here",
    "text": "Why backend work matters here\nA lot of beginner web apps fail because they confuse:\n\na request (one browser hit)\na session (a user across many requests)\nan application run (the server process)\n\nWhen you build a ‚Äútest‚Äù experience, you are building a mini state machine:\n\nStart test ‚Üí choose 35 questions ‚Üí show question 1 ‚Üí record answer ‚Üí show question 2 ‚Üí ‚Ä¶ ‚Üí show results.\n\nIf you don‚Äôt store state, every click resets you.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#symptom-driven-debugging",
    "href": "notes/notes_6.html#symptom-driven-debugging",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Symptom-driven debugging",
    "text": "Symptom-driven debugging\nWhen an app works once and fails on the second run, the cause is often one of:\n\nstate is not reset correctly\nfile handles are not closed\ndatabase locks persist\ncached objects survive between sessions\nassumptions about directory paths are wrong\n\nThis week shows a concrete example: a SQLite error that appears when loading again.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#why-this-is-a-good-learning-bug",
    "href": "notes/notes_6.html#why-this-is-a-good-learning-bug",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Why this is a good learning bug",
    "text": "Why this is a good learning bug\nIt forces you to think in layers:\n\nWhat is the user action (reload, restart, new browser)?\nWhat is the server lifecycle (process reused, dev server reload)?\nWhat is the data layer lifecycle (connection, cursor, transaction)?",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#what-tabnine-is-good-at",
    "href": "notes/notes_6.html#what-tabnine-is-good-at",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "What Tabnine is good at",
    "text": "What Tabnine is good at\n\nproducing ‚Äústarter‚Äù implementations quickly\nsuggesting fixes when you show a stack trace\nrefactoring repetitive code into functions/classes\ngenerating tests and edge cases (if you ask clearly)",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#what-tabnine-is-bad-at-unless-you-guide-it",
    "href": "notes/notes_6.html#what-tabnine-is-bad-at-unless-you-guide-it",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "What Tabnine is bad at (unless you guide it)",
    "text": "What Tabnine is bad at (unless you guide it)\n\nunderstanding your full codebase without context\nmaking architectural decisions for you\nreliably preserving your project conventions\n\n\nPractical prompting pattern\nWhen you ask for help, include:\n\nthe goal in one sentence\nthe current behavior\nthe error message / stack trace\nthe relevant file(s) and function(s)\nwhat you already tried\n\nExample prompt structure (adapt for Tabnine or any assistant):\n\nI have a Flask app that selects 35 questions from SQLite. It works on first load but fails on reload with this error: ‚Ä¶ Here is my DatabaseConnection context manager and the route handler. Please diagnose the root cause and suggest a minimal fix.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#a-minimal-but-clean-flask-app-structure",
    "href": "notes/notes_6.html#a-minimal-but-clean-flask-app-structure",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "A minimal but clean Flask app structure",
    "text": "A minimal but clean Flask app structure\nA simple, testable structure often looks like:\n\napp/__init__.py (create_app)\napp/routes.py (blueprints)\napp/models/ (db + query logic)\napp/templates/ (Jinja2 HTML)\ntests/ (pytest)\n\nThis week keeps things lightweight but pushes toward separation:\n\nroutes should be thin\ndatabase code should be isolated\n‚Äúsession state‚Äù should be explicit",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#code-example-enabling-sessions-and-setting-a-secret-key",
    "href": "notes/notes_6.html#code-example-enabling-sessions-and-setting-a-secret-key",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Code example: enabling sessions and setting a secret key",
    "text": "Code example: enabling sessions and setting a secret key\nfrom flask import Flask\n\ndef create_app():\n    app = Flask(__name__)\n    # In production: load from environment variable\n    app.config[\"SECRET_KEY\"] = \"dev-only-change-me\"\n    return app\n\nWhy SECRET_KEY matters\nFlask signs session cookies. If the key changes between runs, the server can‚Äôt validate old cookies. That can look like ‚Äúrandom‚Äù session failures.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#code-example-selecting-questions-once-per-session",
    "href": "notes/notes_6.html#code-example-selecting-questions-once-per-session",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Code example: selecting questions once per session",
    "text": "Code example: selecting questions once per session\nimport random\nfrom flask import session\n\ndef init_test_session(question_ids, n=35):\n    # Store a stable random selection for this user\n    chosen = random.sample(question_ids, n)\n    session[\"question_ids\"] = chosen\n    session[\"current_index\"] = 0\n    session[\"answers\"] = {}\n\nPitfall\nIf you re-sample on every request, the test becomes inconsistent.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#code-example-route-that-shows-the-current-question",
    "href": "notes/notes_6.html#code-example-route-that-shows-the-current-question",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Code example: route that shows the current question",
    "text": "Code example: route that shows the current question\nfrom flask import render_template, session, redirect, url_for\n\n@app.route(\"/question\")\ndef show_question():\n    ids = session.get(\"question_ids\")\n    if not ids:\n        return redirect(url_for(\"start_test\"))\n\n    idx = session.get(\"current_index\", 0)\n    qid = ids[idx]\n    question = Questions(cursor).fetch_by_id(qid)\n\n    return render_template(\"question.html\", question=question, index=idx+1, total=len(ids))",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#code-example-recording-an-answer-post",
    "href": "notes/notes_6.html#code-example-recording-an-answer-post",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Code example: recording an answer (POST)",
    "text": "Code example: recording an answer (POST)\nfrom flask import request\n\n@app.post(\"/answer\")\ndef record_answer():\n    qid = request.form[\"question_id\"]\n    choice = request.form[\"choice\"]\n\n    answers = session.get(\"answers\", {})\n    answers[qid] = choice\n    session[\"answers\"] = answers\n\n    session[\"current_index\"] = session.get(\"current_index\", 0) + 1\n    return redirect(url_for(\"show_question\"))\n\nWhy we reassign session[\"answers\"]\nWith cookie-based sessions, Flask detects modifications when you set a key. Mutating a nested dict may not be detected consistently unless you reassign.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#make-failures-reproducible",
    "href": "notes/notes_6.html#make-failures-reproducible",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Make failures reproducible",
    "text": "Make failures reproducible\nGood debugging is about making ‚Äúsometimes‚Äù become ‚Äúalways‚Äù.\nTry:\n\nrestart server, load app, answer 1 question, reload\nopen private window (new session), repeat\ndelete cookies, repeat",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#code-example-quick-smoke-test-with-requests",
    "href": "notes/notes_6.html#code-example-quick-smoke-test-with-requests",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Code example: quick smoke test with requests",
    "text": "Code example: quick smoke test with requests\nimport requests\n\nbase = \"http://127.0.0.1:5000\"\nwith requests.Session() as s:\n    r = s.get(f\"{base}/start\")\n    r.raise_for_status()\n    r = s.get(f\"{base}/question\")\n    print(r.status_code, len(r.text))",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#common-error-classes-in-this-week",
    "href": "notes/notes_6.html#common-error-classes-in-this-week",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Common error classes in this week",
    "text": "Common error classes in this week\n\nSQLite ‚Äúdatabase is locked‚Äù\nfile path errors (relative vs absolute)\nsession cookie invalidation\nKeyError from missing session keys",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#summing-up",
    "href": "notes/notes_6.html#summing-up",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Summing Up",
    "text": "Summing Up\n\nChapter 6 is about turning a basic Flask + SQLite app into an interactive experience.\nThe technical core is sessions: how to persist a user‚Äôs progress across requests.\nThe practical workflow skill is using Tabnine effectively: provide context, ask for minimal diffs, and validate with tests.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_6.html#checks-for-understanding",
    "href": "notes/notes_6.html#checks-for-understanding",
    "title": "Week 06 ‚Äî Generating a software backend",
    "section": "Checks for understanding",
    "text": "Checks for understanding\n\nExplain the difference between a request, a session, and a server process.\nList 5 pieces of state you need for a practice test workflow.\nDescribe one reason a SQLite app works once but fails on reload.\nGive an example of a good debugging prompt for Tabnine.",
    "crumbs": [
      "Notes",
      "Notes 06 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html",
    "href": "notes/notes_8.html",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Why testing matters in AI-assisted coding\nKey terms you must be able to use\nAI-assisted test generation workflow\nunittest vs pytest and when to use each\nIn-memory databases for isolated tests\nPrompting patterns for high-quality tests\nEvaluating and fixing AI-generated tests\nCommon failure modes and how to debug\nUsing generated tests in CI\nSumming Up\n\n\n\nWeek 08 focuses on using generative AI to produce tests, but this week is really about trust.\n\nA test suite is your ‚Äúcontract‚Äù with future you: it describes what must stay true as code changes.\nAI can write tests quickly, but speed is useless if the tests are brittle, shallow, or wrong.\n‚ÄúGenerative AI tools are changing this boring, repetitive process.‚Äù\n‚ÄúThey can automatically create detailed test suites, spot edge cases, and generate boilerplate code.‚Äù\n\nIn AI-assisted workflows, tests do two jobs at once: 1) catch regressions, and 2) detect when the AI has hallucinated an API, edge case, or assumption.\n\nA bad test suite can make things worse by giving you false confidence.\n\n\n\n\n\ntest oracle ‚Äî what counts as correct output (and what your test is really asserting).\nfixture ‚Äî a repeatable setup/teardown wrapper that makes tests readable and reliable.\nmock ‚Äî a controlled stand‚Äëin that isolates your code from external systems.\nin‚Äëmemory database ‚Äî a temporary database (often SQLite) used for fast, isolated tests.\nregression test ‚Äî a test that locks in behaviour so future changes don‚Äôt break it.\nproperty‚Äëbased testing ‚Äî testing broad invariants by generating many inputs automatically.\n\n\n\n\n\nStart by writing a clear spec (even if it is only a paragraph). AI cannot test what you cannot describe.\nDecide what level you are testing: unit (small), integration (multiple parts), or end-to-end (system).\nGive the AI: the function signature, expected behaviour, edge cases, and any invariants.\nAsk for tests and a short explanation of why each test exists.\n‚ÄúThis week shows how GitHub Copilot, Tabnine, and Blackbox AI can enhance your testing workflow.‚Äù\nRun the tests immediately. If they fail, treat that as feedback about either the code or the prompt.\nRefactor the generated tests: rename, remove duplicates, add missing assertions, and simplify setup.\n\n\n\n\n\nBoth frameworks can express the same logic; the difference is ergonomics and ecosystem conventions.\nunittest is batteries-included and class-based; pytest is function-first and heavily fixture-driven.\nIf a project already uses one, prefer consistency over personal preference.\nAI tends to produce verbose unittest boilerplate unless you explicitly ask for pytest.\n‚ÄúThey can help you build strong, maintain¬≠ able test suites quickly, while upholding quality.‚Äù\nAsk the AI to follow your project‚Äôs test naming and folder conventions.\nDo not accept tests that assert implementation details (e.g., exact SQL query strings) unless required.\n\n\n\n\n\nAn in-memory DB makes tests fast and isolated: every test starts from a clean slate.\nThis week demonstrates creating temporary databases and seeding minimal data for reproducibility.\n‚Äú211 Why use generative AI for testing?‚Äù\nKey idea: tests should not depend on a developer‚Äôs local environment (paths, ports, or existing data).\nWhen you use SQLite in-memory, ensure your app‚Äôs DB layer can accept a connection string override.\n\n\n\n\n\nUse prompts that are specific about assertions, not just ‚Äúwrite tests‚Äù.\nAlways list edge cases you care about (empty inputs, None/null, boundary values, malformed data).\nIf you want parameterised tests, say so explicitly.\nIf you want mocks, name the dependencies to mock and the behaviour they should simulate.\n‚ÄúHigh-quality tests are vital for reliable software.‚Äù\nAsk for one test per behaviour: big ‚Äúkitchen sink‚Äù tests are hard to debug.\nAsk the AI to include comments that link each test back to a requirement line.\n\n\n\n\n\nTreat AI-generated tests as a draft. Your job is to make them true and useful.\nCheck: does the test actually fail when the code is wrong? (mutation testing mindset).\nCheck for false positives: tests that pass even if you delete the core logic.\nCheck for brittle fixtures: random data, timestamps, network calls, global state.\n‚ÄúThey catch bugs early and ensure new features don‚Äôt break existing code.‚Äù\nPrefer explicit assertions over printing/logging output.\n\n\n\n\n\nHallucinated imports: AI may invent packages or functions that do not exist in your environment.\nOver-mocking: if everything is mocked, you are not testing meaningful behaviour.\nTesting the wrong thing: asserting internal variable values rather than outputs or side effects.\nNondeterminism: tests that depend on ordering, random seeds, time, or network.\n‚ÄúThey also document how the code should behave.‚Äù\nFix strategy: reduce the test to the minimal reproducer, then rebuild structure with fixtures.\n\n\n\n\n\nA CI pipeline turns ‚Äúworks on my machine‚Äù into ‚Äúworks for everyone‚Äù.\nAI-generated tests are only valuable if they run reliably in a clean environment.\nMake dependencies explicit (requirements file / lockfile).\nPin versions when your tests rely on specific behaviour.\nKeep secrets out of tests; use environment variables and test doubles.\n\n\n\nimport pytest\n\ndef add(a, b):\n    return a + b\n\ndef test_add_basic():\n    assert add(2, 3) == 5\n\n@pytest.mark.parametrize('a,b,expected', [(0,0,0), (-1,1,0), (2,-5,-3)])\ndef test_add_parametrized(a,b,expected):\n    assert add(a,b) == expected\n\n\n\nimport unittest\n\ndef add(a, b):\n    return a + b\n\nclass TestAdd(unittest.TestCase):\n    def test_add_basic(self):\n        self.assertEqual(add(2,3), 5)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\nimport sqlite3\n\ndef make_db():\n    conn = sqlite3.connect(':memory:')\n    cur = conn.cursor()\n    cur.execute('CREATE TABLE users(id INTEGER PRIMARY KEY, name TEXT)')\n    cur.execute('INSERT INTO users(name) VALUES (?)', ('Ada',))\n    conn.commit()\n    return conn\n\ndef test_user_seed():\n    conn = make_db()\n    cur = conn.cursor()\n    cur.execute('SELECT COUNT(*) FROM users')\n    assert cur.fetchone()[0] == 1\n\n\n\nfrom unittest.mock import Mock\n\nclass Client:\n    def __init__(self, http_get):\n        self.http_get = http_get\n\n    def fetch_status(self, url):\n        resp = self.http_get(url)\n        return resp.status_code\n\ndef test_fetch_status_mocked():\n    fake_resp = Mock()\n    fake_resp.status_code = 200\n    http_get = Mock(return_value=fake_resp)\n\n    c = Client(http_get=http_get)\n    assert c.fetch_status('https://example.com') == 200\n    http_get.assert_called_once()\n\n\n\nPROMPT = '''\nYou are generating tests for Python code.\n\nContext:\n- Framework: pytest\n- File under test: app/service.py\n- Function signatures: &lt;paste here&gt;\n\nRequirements:\n1) Write one test per requirement.\n2) Include edge cases: empty input, None, boundary values.\n3) Use fixtures for setup; avoid global state.\n4) Explain each test in 1 line comment.\n\nOutput: a complete pytest test module.\n'''\nprint(PROMPT)\n\n\n\n# Demonstration: if you break the function, does a test fail?\n\ndef area(radius):\n    return 3.14159 * radius * radius\n\ndef test_area_positive():\n    assert area(2) == 12.56636\n\n# Try changing area() to return 0 and rerun the test.\n\n\n\n\n\nQuick checklist before you trust an AI-generated test suite:\nDoes each test name describe behaviour (not implementation)?\nDo the tests include at least one negative case?\nAre edge cases explicit, not implied?\nIs the database/file system/network isolated or mocked?\nWould the tests still pass if you replaced your function body with pass?\nDo failures produce actionable error messages?\nDo fixtures reduce duplication (but not hide important details)?\nAre time-dependent values controlled (freeze time or inject clocks)?\nAre random values seeded?\nAre you asserting the right thing (outputs/side effects) rather than intermediate variables?\nMini Q&A (the kinds of questions you should be able to answer):\nWhat is a ‚Äútest oracle‚Äù and why is it hard for AI?\nWhen is mocking harmful?\nWhy is an in-memory database useful but sometimes misleading?\nHow do you know a test is not ‚Äútoo shallow‚Äù?\nWhat prompt information tends to improve assertion quality?\n\n\n\n\n\nWeek 08 keeps returning to a simple idea: tests get better when the specification is clearer than the code.\nWhen you prompt a tool to generate tests, you are effectively writing a mini test plan.\n\n\n\n\nUse this structure (adapt the wording, keep the slots):\nSystem/role: ‚ÄúYou are a software tester writing pytest tests.‚Äù\nGoal: ‚ÄúGenerate tests for function X.‚Äù\nBehaviour spec: list rules as bullets.\nEdge cases: list edge cases explicitly.\nConstraints:\n- no network\n- no real filesystem writes\n- deterministic outcomes (freeze time or inject clock)\nOutput format: ‚ÄúReturn only test code in one file.‚Äù\nA useful trick: ask the model to first list test cases (names + what they check), then write the code.\nIf you do that, you can review the case list quickly before trusting the code.\n\n\n\n\n\nAsk for meaningful assertions rather than ‚Äújust coverage‚Äù.\nAsk for failure-mode tests:\ninvalid inputs\nboundary values\nempty collections\nmissing keys\nduplicates\nIf the code touches randomness, instruct: ‚Äúinject a seed or mock the RNG.‚Äù\n\n\n\n\n\nAsk for fixtures with clear names: user_factory, db_session, tmp_project.\nAsk the tool to explain (briefly) what each fixture is responsible for.\nIf the tool creates a fixture that does too much, split it.\n\n\n\n\n\n\nBelow is a deliberately small example to illustrate the workflow Week 08 argues for:\nstart with a vague prompt ‚Üí get weak tests ‚Üí tighten the prompt ‚Üí get stronger tests.\n\n\n\n#| echo: true\n#| eval: false\n\ndef normalise_score(x: float) -&gt; float:\n    \"\"\"Clamp to [0, 1] and round to 2dp.\"\"\"\n    if x != x:  # NaN\n        raise ValueError(\"NaN\")\n    if x &lt; 0:\n        return 0.0\n    if x &gt; 1:\n        return 1.0\n    return round(x, 2)\n\n\n\n\nWeak tests often:\nonly test the ‚Äúhappy path‚Äù,\nassert very little,\nmiss NaN / boundary cases.\n\n#| echo: true\n#| eval: false\n\nimport pytest\n\n\ndef test_normalise_score_basic():\n    assert normalise_score(0.2) == 0.2\n\n\n\n#| echo: true\n#| eval: false\n\nimport math\nimport pytest\n\n\ndef test_clamps_low_values_to_zero():\n    assert normalise_score(-0.0001) == 0.0\n\n\ndef test_clamps_high_values_to_one():\n    assert normalise_score(1.0001) == 1.0\n\n\ndef test_rounds_to_two_decimal_places():\n    assert normalise_score(0.126) == 0.13\n\n\ndef test_boundary_values_are_stable():\n    assert normalise_score(0.0) == 0.0\n    assert normalise_score(1.0) == 1.0\n\n\ndef test_nan_raises_value_error():\n    with pytest.raises(ValueError):\n        normalise_score(math.nan)\n\nThis is the kind of result you get when you prompt with explicit rules and edge cases.\n\n\n\n\n\n\nWeek 08 discusses isolated environments because test reliability depends on control.\nA database is a common source of hidden coupling:\nleftover rows from previous runs\nschema drift\ntests that require manual setup\n\n\n\n\nSQLite can run entirely in memory by using :memory:.\nBenefit: every test run starts from a clean slate.\nTradeoff: behaviour can differ from your production database (types, constraints, concurrency).\n\n#| echo: true\n#| eval: false\n\nimport sqlite3\n\n\ndef make_db():\n    conn = sqlite3.connect(\":memory:\")\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE items(id INTEGER PRIMARY KEY, name TEXT)\")\n    conn.commit()\n    return conn\n\n\ndef test_insert_and_query():\n    conn = make_db()\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO items(name) VALUES (?)\", (\"apple\",))\n    conn.commit()\n    cur.execute(\"SELECT name FROM items WHERE id=1\")\n    assert cur.fetchone()[0] == \"apple\"\n\nIf you‚Äôre using an ORM (SQLAlchemy), you can build a test session bound to an in-memory engine.\nPrompt tip: explicitly name the stack (SQLite + SQLAlchemy + pytest fixtures) so the model doesn‚Äôt invent libraries.\n\n\n\n\n\n\nCommon failure patterns (and how to spot them):\nAsserting implementation details: tests break when you refactor harmlessly.\nBrittle data: hard-coded timestamps, random ids, order-dependent lists.\nMocking the wrong thing: you end up testing the mock, not the behaviour.\nTesting the tool‚Äôs hallucination: the model invents functions/options that do not exist.\nDefensive habits:\nrun tests immediately\nread assertions carefully\ncheck that fixtures actually model the real domain\n\n\n\n\n\nAI can draft tests quickly, but good tests require judgement.\nYour workflow should be: specify behaviour ‚Üí generate tests ‚Üí run ‚Üí review assertions ‚Üí improve prompts ‚Üí repeat.\nThe fastest way to level up is to treat tests as communication: a clear story about what the code must do.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#why-testing-matters",
    "href": "notes/notes_8.html#why-testing-matters",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Week 08 focuses on using generative AI to produce tests, but this week is really about trust.\n\nA test suite is your ‚Äúcontract‚Äù with future you: it describes what must stay true as code changes.\nAI can write tests quickly, but speed is useless if the tests are brittle, shallow, or wrong.\n‚ÄúGenerative AI tools are changing this boring, repetitive process.‚Äù\n‚ÄúThey can automatically create detailed test suites, spot edge cases, and generate boilerplate code.‚Äù\n\nIn AI-assisted workflows, tests do two jobs at once: 1) catch regressions, and 2) detect when the AI has hallucinated an API, edge case, or assumption.\n\nA bad test suite can make things worse by giving you false confidence.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#key-terms",
    "href": "notes/notes_8.html#key-terms",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "test oracle ‚Äî what counts as correct output (and what your test is really asserting).\nfixture ‚Äî a repeatable setup/teardown wrapper that makes tests readable and reliable.\nmock ‚Äî a controlled stand‚Äëin that isolates your code from external systems.\nin‚Äëmemory database ‚Äî a temporary database (often SQLite) used for fast, isolated tests.\nregression test ‚Äî a test that locks in behaviour so future changes don‚Äôt break it.\nproperty‚Äëbased testing ‚Äî testing broad invariants by generating many inputs automatically.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#ai-workflow",
    "href": "notes/notes_8.html#ai-workflow",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Start by writing a clear spec (even if it is only a paragraph). AI cannot test what you cannot describe.\nDecide what level you are testing: unit (small), integration (multiple parts), or end-to-end (system).\nGive the AI: the function signature, expected behaviour, edge cases, and any invariants.\nAsk for tests and a short explanation of why each test exists.\n‚ÄúThis week shows how GitHub Copilot, Tabnine, and Blackbox AI can enhance your testing workflow.‚Äù\nRun the tests immediately. If they fail, treat that as feedback about either the code or the prompt.\nRefactor the generated tests: rename, remove duplicates, add missing assertions, and simplify setup.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#unittest-vs-pytest",
    "href": "notes/notes_8.html#unittest-vs-pytest",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Both frameworks can express the same logic; the difference is ergonomics and ecosystem conventions.\nunittest is batteries-included and class-based; pytest is function-first and heavily fixture-driven.\nIf a project already uses one, prefer consistency over personal preference.\nAI tends to produce verbose unittest boilerplate unless you explicitly ask for pytest.\n‚ÄúThey can help you build strong, maintain¬≠ able test suites quickly, while upholding quality.‚Äù\nAsk the AI to follow your project‚Äôs test naming and folder conventions.\nDo not accept tests that assert implementation details (e.g., exact SQL query strings) unless required.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#in-memory-db",
    "href": "notes/notes_8.html#in-memory-db",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "An in-memory DB makes tests fast and isolated: every test starts from a clean slate.\nThis week demonstrates creating temporary databases and seeding minimal data for reproducibility.\n‚Äú211 Why use generative AI for testing?‚Äù\nKey idea: tests should not depend on a developer‚Äôs local environment (paths, ports, or existing data).\nWhen you use SQLite in-memory, ensure your app‚Äôs DB layer can accept a connection string override.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#prompting-patterns",
    "href": "notes/notes_8.html#prompting-patterns",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Use prompts that are specific about assertions, not just ‚Äúwrite tests‚Äù.\nAlways list edge cases you care about (empty inputs, None/null, boundary values, malformed data).\nIf you want parameterised tests, say so explicitly.\nIf you want mocks, name the dependencies to mock and the behaviour they should simulate.\n‚ÄúHigh-quality tests are vital for reliable software.‚Äù\nAsk for one test per behaviour: big ‚Äúkitchen sink‚Äù tests are hard to debug.\nAsk the AI to include comments that link each test back to a requirement line.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#evaluating",
    "href": "notes/notes_8.html#evaluating",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Treat AI-generated tests as a draft. Your job is to make them true and useful.\nCheck: does the test actually fail when the code is wrong? (mutation testing mindset).\nCheck for false positives: tests that pass even if you delete the core logic.\nCheck for brittle fixtures: random data, timestamps, network calls, global state.\n‚ÄúThey catch bugs early and ensure new features don‚Äôt break existing code.‚Äù\nPrefer explicit assertions over printing/logging output.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#failure-modes",
    "href": "notes/notes_8.html#failure-modes",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Hallucinated imports: AI may invent packages or functions that do not exist in your environment.\nOver-mocking: if everything is mocked, you are not testing meaningful behaviour.\nTesting the wrong thing: asserting internal variable values rather than outputs or side effects.\nNondeterminism: tests that depend on ordering, random seeds, time, or network.\n‚ÄúThey also document how the code should behave.‚Äù\nFix strategy: reduce the test to the minimal reproducer, then rebuild structure with fixtures.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#ci",
    "href": "notes/notes_8.html#ci",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "A CI pipeline turns ‚Äúworks on my machine‚Äù into ‚Äúworks for everyone‚Äù.\nAI-generated tests are only valuable if they run reliably in a clean environment.\nMake dependencies explicit (requirements file / lockfile).\nPin versions when your tests rely on specific behaviour.\nKeep secrets out of tests; use environment variables and test doubles.\n\n\n\nimport pytest\n\ndef add(a, b):\n    return a + b\n\ndef test_add_basic():\n    assert add(2, 3) == 5\n\n@pytest.mark.parametrize('a,b,expected', [(0,0,0), (-1,1,0), (2,-5,-3)])\ndef test_add_parametrized(a,b,expected):\n    assert add(a,b) == expected\n\n\n\nimport unittest\n\ndef add(a, b):\n    return a + b\n\nclass TestAdd(unittest.TestCase):\n    def test_add_basic(self):\n        self.assertEqual(add(2,3), 5)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\nimport sqlite3\n\ndef make_db():\n    conn = sqlite3.connect(':memory:')\n    cur = conn.cursor()\n    cur.execute('CREATE TABLE users(id INTEGER PRIMARY KEY, name TEXT)')\n    cur.execute('INSERT INTO users(name) VALUES (?)', ('Ada',))\n    conn.commit()\n    return conn\n\ndef test_user_seed():\n    conn = make_db()\n    cur = conn.cursor()\n    cur.execute('SELECT COUNT(*) FROM users')\n    assert cur.fetchone()[0] == 1\n\n\n\nfrom unittest.mock import Mock\n\nclass Client:\n    def __init__(self, http_get):\n        self.http_get = http_get\n\n    def fetch_status(self, url):\n        resp = self.http_get(url)\n        return resp.status_code\n\ndef test_fetch_status_mocked():\n    fake_resp = Mock()\n    fake_resp.status_code = 200\n    http_get = Mock(return_value=fake_resp)\n\n    c = Client(http_get=http_get)\n    assert c.fetch_status('https://example.com') == 200\n    http_get.assert_called_once()\n\n\n\nPROMPT = '''\nYou are generating tests for Python code.\n\nContext:\n- Framework: pytest\n- File under test: app/service.py\n- Function signatures: &lt;paste here&gt;\n\nRequirements:\n1) Write one test per requirement.\n2) Include edge cases: empty input, None, boundary values.\n3) Use fixtures for setup; avoid global state.\n4) Explain each test in 1 line comment.\n\nOutput: a complete pytest test module.\n'''\nprint(PROMPT)\n\n\n\n# Demonstration: if you break the function, does a test fail?\n\ndef area(radius):\n    return 3.14159 * radius * radius\n\ndef test_area_positive():\n    assert area(2) == 12.56636\n\n# Try changing area() to return 0 and rerun the test.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#sanity",
    "href": "notes/notes_8.html#sanity",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Quick checklist before you trust an AI-generated test suite:\nDoes each test name describe behaviour (not implementation)?\nDo the tests include at least one negative case?\nAre edge cases explicit, not implied?\nIs the database/file system/network isolated or mocked?\nWould the tests still pass if you replaced your function body with pass?\nDo failures produce actionable error messages?\nDo fixtures reduce duplication (but not hide important details)?\nAre time-dependent values controlled (freeze time or inject clocks)?\nAre random values seeded?\nAre you asserting the right thing (outputs/side effects) rather than intermediate variables?\nMini Q&A (the kinds of questions you should be able to answer):\nWhat is a ‚Äútest oracle‚Äù and why is it hard for AI?\nWhen is mocking harmful?\nWhy is an in-memory database useful but sometimes misleading?\nHow do you know a test is not ‚Äútoo shallow‚Äù?\nWhat prompt information tends to improve assertion quality?",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#prompt-patterns-for-test-generation",
    "href": "notes/notes_8.html#prompt-patterns-for-test-generation",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Week 08 keeps returning to a simple idea: tests get better when the specification is clearer than the code.\nWhen you prompt a tool to generate tests, you are effectively writing a mini test plan.\n\n\n\n\nUse this structure (adapt the wording, keep the slots):\nSystem/role: ‚ÄúYou are a software tester writing pytest tests.‚Äù\nGoal: ‚ÄúGenerate tests for function X.‚Äù\nBehaviour spec: list rules as bullets.\nEdge cases: list edge cases explicitly.\nConstraints:\n- no network\n- no real filesystem writes\n- deterministic outcomes (freeze time or inject clock)\nOutput format: ‚ÄúReturn only test code in one file.‚Äù\nA useful trick: ask the model to first list test cases (names + what they check), then write the code.\nIf you do that, you can review the case list quickly before trusting the code.\n\n\n\n\n\nAsk for meaningful assertions rather than ‚Äújust coverage‚Äù.\nAsk for failure-mode tests:\ninvalid inputs\nboundary values\nempty collections\nmissing keys\nduplicates\nIf the code touches randomness, instruct: ‚Äúinject a seed or mock the RNG.‚Äù\n\n\n\n\n\nAsk for fixtures with clear names: user_factory, db_session, tmp_project.\nAsk the tool to explain (briefly) what each fixture is responsible for.\nIf the tool creates a fixture that does too much, split it.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#worked-example",
    "href": "notes/notes_8.html#worked-example",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Below is a deliberately small example to illustrate the workflow Week 08 argues for:\nstart with a vague prompt ‚Üí get weak tests ‚Üí tighten the prompt ‚Üí get stronger tests.\n\n\n\n#| echo: true\n#| eval: false\n\ndef normalise_score(x: float) -&gt; float:\n    \"\"\"Clamp to [0, 1] and round to 2dp.\"\"\"\n    if x != x:  # NaN\n        raise ValueError(\"NaN\")\n    if x &lt; 0:\n        return 0.0\n    if x &gt; 1:\n        return 1.0\n    return round(x, 2)\n\n\n\n\nWeak tests often:\nonly test the ‚Äúhappy path‚Äù,\nassert very little,\nmiss NaN / boundary cases.\n\n#| echo: true\n#| eval: false\n\nimport pytest\n\n\ndef test_normalise_score_basic():\n    assert normalise_score(0.2) == 0.2\n\n\n\n#| echo: true\n#| eval: false\n\nimport math\nimport pytest\n\n\ndef test_clamps_low_values_to_zero():\n    assert normalise_score(-0.0001) == 0.0\n\n\ndef test_clamps_high_values_to_one():\n    assert normalise_score(1.0001) == 1.0\n\n\ndef test_rounds_to_two_decimal_places():\n    assert normalise_score(0.126) == 0.13\n\n\ndef test_boundary_values_are_stable():\n    assert normalise_score(0.0) == 0.0\n    assert normalise_score(1.0) == 1.0\n\n\ndef test_nan_raises_value_error():\n    with pytest.raises(ValueError):\n        normalise_score(math.nan)\n\nThis is the kind of result you get when you prompt with explicit rules and edge cases.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#in-memory-databases",
    "href": "notes/notes_8.html#in-memory-databases",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Week 08 discusses isolated environments because test reliability depends on control.\nA database is a common source of hidden coupling:\nleftover rows from previous runs\nschema drift\ntests that require manual setup\n\n\n\n\nSQLite can run entirely in memory by using :memory:.\nBenefit: every test run starts from a clean slate.\nTradeoff: behaviour can differ from your production database (types, constraints, concurrency).\n\n#| echo: true\n#| eval: false\n\nimport sqlite3\n\n\ndef make_db():\n    conn = sqlite3.connect(\":memory:\")\n    cur = conn.cursor()\n    cur.execute(\"CREATE TABLE items(id INTEGER PRIMARY KEY, name TEXT)\")\n    conn.commit()\n    return conn\n\n\ndef test_insert_and_query():\n    conn = make_db()\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO items(name) VALUES (?)\", (\"apple\",))\n    conn.commit()\n    cur.execute(\"SELECT name FROM items WHERE id=1\")\n    assert cur.fetchone()[0] == \"apple\"\n\nIf you‚Äôre using an ORM (SQLAlchemy), you can build a test session bound to an in-memory engine.\nPrompt tip: explicitly name the stack (SQLite + SQLAlchemy + pytest fixtures) so the model doesn‚Äôt invent libraries.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#where-ai-tests-go-wrong",
    "href": "notes/notes_8.html#where-ai-tests-go-wrong",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "Common failure patterns (and how to spot them):\nAsserting implementation details: tests break when you refactor harmlessly.\nBrittle data: hard-coded timestamps, random ids, order-dependent lists.\nMocking the wrong thing: you end up testing the mock, not the behaviour.\nTesting the tool‚Äôs hallucination: the model invents functions/options that do not exist.\nDefensive habits:\nrun tests immediately\nread assertions carefully\ncheck that fixtures actually model the real domain",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  },
  {
    "objectID": "notes/notes_8.html#summing-up-week8",
    "href": "notes/notes_8.html#summing-up-week8",
    "title": "Week 08 ‚Äî Building effective tests with generative AI",
    "section": "",
    "text": "AI can draft tests quickly, but good tests require judgement.\nYour workflow should be: specify behaviour ‚Üí generate tests ‚Üí run ‚Üí review assertions ‚Üí improve prompts ‚Üí repeat.\nThe fastest way to level up is to treat tests as communication: a clear story about what the code must do.",
    "crumbs": [
      "Notes",
      "Notes 08 üìï"
    ]
  }
]