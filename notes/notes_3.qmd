# Week 03 — Designing Your First Prompt

> **Week focus:** turning vague intent into reliable, testable prompt instructions.


-   [Figures used in this week](#figures-used-in-this-week)
-   [Why prompting is a design skill](#why-prompting-is-a-design-skill)
-   [Prompt anatomy](#prompt-anatomy)
-   [From vague to specific](#from-vague-to-specific)
-   [Constraints, rubrics, and acceptance tests](#constraints-rubrics-and-acceptance-tests)
-   [Few-shot examples](#few-shot-examples)
-   [Output formats that don't break](#output-formats-that-dont-break)
-   [Iteration loop](#iteration-loop)
-   [Failure modes + debugging prompts](#failure-modes-debugging-prompts)
-   [Prompt patterns you can reuse](#prompt-patterns-you-can-reuse)
-   [Summing Up](#summing-up)
-   [Appendix A — Worked walkthrough: building a prompt step by step](#appendix-a-worked-walkthrough-building-a-prompt-step-by-step)

## Why prompting is a design skill {#why-prompting-is-a-design-skill}

Prompting is not “asking nicely”. In the real world, you use prompts to **specify behaviour** under constraints:

-   you want *repeatable* outputs
-   you want outputs that are *checkable*
-   you want the model to use the *right information*
-   you want it to behave sensibly when information is missing

A useful mindset: a prompt is closer to a **function signature** than a conversation.

> "A prompt is a program in natural language."

### The two jobs of a good prompt

1)  **Explain the task** (what success looks like)
2)  **Constrain the output** (how the response must be shaped)

## Prompt anatomy {#prompt-anatomy}

A practical prompt usually contains these parts (not always in this order):

-   **Role**: “You are a …” (useful when behaviour matters)
-   **Goal**: what to produce
-   **Context**: what the model should know or assume
-   **Inputs**: what you are providing (data, text, code, constraints)
-   **Rules**: explicit do/don’t constraints
-   **Output format**: JSON, Markdown, table, bullet list, etc
-   **Quality bar**: rubric / checklist / tests

### Keywords to look out for

-   **Role prompting**: framing the model’s perspective and responsibilities.
-   **Constraints**: explicit limits that prevent unwanted behaviour or formats.
-   **Few-shot prompting**: teaching by giving examples of desired input→output.
-   **Rubric**: a checklist the model should satisfy.
-   **Output schema**: a formal structure the output must follow (e.g., JSON).

### Example: minimal vs engineered prompt

Bad (vague):

``` text
Write a good explanation of SQL joins.
```

Better (engineered):

``` text
You are a first-year teaching assistant.
Explain SQL joins to beginners using:
- exactly 3 join types (INNER, LEFT, FULL)
- one simple table example
- one common mistake per join type
Output as markdown with headings and a final summary.
```

## From vague to specific {#from-vague-to-specific}

Many prompts fail because they skip **specification**. You can convert vague intent into a usable prompt by answering:

-   What is the *deliverable*? (an explanation / a plan / code / critique)
-   Who is the *audience*?
-   What *constraints* exist? (length, style, tools, time)
-   What does *success* look like? (rubric / checklist)
-   What should *never* happen? (anti-requirements)

> "Be specific about what you want and what you don’t want."

### A reusable rewrite pattern

Take the vague prompt:

``` text
Help me debug my Python.
```

Rewrite it into something testable:

``` text
You are a Python tutor. I will paste code and an error message.
1) Explain what the error means in plain language
2) Identify the exact line likely causing it
3) Provide a fixed version of the code
4) Add one print/debug statement that proves the fix works
Keep changes minimal.
```

## Constraints, rubrics, and acceptance tests {#constraints-rubrics-and-acceptance-tests}

If you want outputs you can trust, you need a **quality gate**. This can be:

-   A short rubric (“must include X, avoid Y”)
-   An acceptance test (unit tests, schema validation)
-   A format requirement (JSON schema, markdown headings)

> "Make the output easy to verify."

### Code: acceptance-test mindset

```python
# A tiny “acceptance test” for AI-written text outputs
def contains_required_phrases(text: str, required: list[str]) -> bool:
    return all(phrase.lower() in text.lower() for phrase in required)

sample = "This answer covers INNER JOIN, LEFT JOIN and FULL JOIN."
print(contains_required_phrases(sample, ["INNER JOIN", "LEFT JOIN", "FULL JOIN"]))
```

## Few-shot examples {#few-shot-examples}

**Few-shot prompting** is when you show the model examples of the behaviour you want. This is especially powerful when you want consistent style or formatting.

### When few-shot helps

-   When outputs must match a “house style”
-   When the task is ambiguous (many valid interpretations)
-   When you care about edge-cases

> "Examples reduce ambiguity."

### Example prompt with one example pair

``` text
Task: Convert short notes into a structured study card.
Format:
- Term
- Definition
- Example
- Common mistake

Example input:
"SQL LEFT JOIN keeps all rows from the left table"
Example output:
Term: LEFT JOIN
Definition: Returns all rows from left table plus matching right rows.
Example: ...
Common mistake: Confusing with INNER JOIN.

Now do the same for: "FULL JOIN keeps all rows from both"
```

## Output formats that don't break {#output-formats-that-dont-break}

In real workflows, AI output often becomes input to something else: - a script - a web app - a report - a database

That means formatting matters. If you don’t force structure, you’ll get “pretty” output that is unusable.

> "Ask for the output format you need."

### Code: strict JSON parsing

```python
import json

raw = """{
  "title": "Study card",
  "points": ["one", "two"]
}"""
data = json.loads(raw)
print(data["title"], data["points"])
```

## Iteration loop {#iteration-loop}

Prompting is iterative. A common loop is:

1)  Draft prompt
2)  Run it on a few cases
3)  Diagnose failure modes
4)  Add constraints / examples
5)  Re-run

> "Iterate on prompts like you iterate on code."

### Code: prompt versioning idea

```python
PROMPT_V1 = "Summarise this article."
PROMPT_V2 = "Summarise this article in 5 bullets for first-year students."
PROMPT_V3 = PROMPT_V2 + " Include one limitation and one question."

for i, p in enumerate([PROMPT_V1, PROMPT_V2, PROMPT_V3], start=1):
    print(f"v{i}: {p}")
```

## Failure modes + debugging prompts {#failure-modes-debugging-prompts}

Common failure modes in early prompting:

-   **Overly broad instructions** → generic output
-   **Missing constraints** → wrong format, wrong length
-   **Unclear audience** → tone mismatch
-   **No examples** → inconsistent style
-   **No rubric** → “sounds good” but incomplete

### Debug prompt template

``` text
You produced an answer that failed these requirements:
- (list failures)
Explain why it failed and produce a corrected version.
Then provide a checklist to prevent the same failure next time.
```

## Prompt patterns you can reuse {#prompt-patterns-you-can-reuse}

Below are reusable patterns you can copy into real work.

### Pattern A: “Do X, then verify”

``` text
Do the task. Then verify your output against this checklist:
- ...
If anything fails, fix and re-check.
```

### Pattern B: “Teach me, then quiz me”

``` text
Explain this concept simply. Then ask me 5 questions, increasing difficulty.
Give answers at the end.
```

### Pattern C: “Generate options + trade-offs”

``` text
Give 3 options. For each, include: benefits, drawbacks, best use-case.
Finish with a recommendation for a beginner.
```

## Summing Up {#summing-up}

-   Prompting is best treated as **specification** and **design**.
-   Good prompts include constraints, a format, and a quality gate.
-   Few-shot examples reduce ambiguity and increase consistency.
-   Iteration is normal: test, debug, refine.
-   Outputs should be structured so they can be verified and reused.

## Appendix A — Worked walkthrough: building a prompt step by step {#appendix-a-worked-walkthrough-building-a-prompt-step-by-step}

### Step 1: Start vague

-   We begin with an intent that a human understands but a model can misinterpret.
-   Example intent: *Make this explanation better.*
-   The phrase *better* is underspecified: better for who, in what way, with what constraints?
-   If you leave this vague, the model may change style, length, or even meaning.

### Step 2: Add audience + purpose

-   Decide who the output is for (audience).
-   For week 3, our audience is *first-year university students*.
-   Purpose might be: quick revision notes, lecture handout, or tutorial support.
-   Audience + purpose affect tone, vocabulary, and example choice.

### Step 3: Add constraints

-   Constraints prevent ‘creative drift’.
-   Common constraints: word count, formatting, required sections, no jargon.
-   Constraints also help the model prioritise: if space is limited, it must focus.

### Step 4: Add success criteria (rubric)

-   A rubric is a checklist your output should pass.
-   Rubrics are powerful because they are *explicit*, *testable*, and *reusable*.
-   Example rubric: must include 1 analogy, 1 code example, 3 bullet takeaways.

### Step 5: Add output format

-   If the output must feed another tool, specify a strict format.
-   Examples: JSON, Markdown headings, CSV, YAML front matter.
-   In Quarto notes, Markdown + headings is usually enough.

### Step 6: Add examples (few-shot)

-   If you care about style, show the model the style.
-   One example pair can dramatically increase consistency.
-   Too many examples can bloat the prompt, so start small.

### Step 7: Iterate with failure analysis

-   When output fails, don’t just re-run: diagnose.
-   Ask: which instruction was ignored? Was it ambiguous? Was it missing?
-   Then modify the prompt so the failure becomes impossible or unlikely.